{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from library.ipynb\n",
      "importing Jupyter notebook from job.ipynb\n",
      "importing Jupyter notebook from jobqueue.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from dataplane.ipynb\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "\n",
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet, TopKPooling, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sys import getsizeof\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/stacking/node10/model6\")\n",
    "\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(pathName)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "modelNum = 0\n",
    "availableJobNum = 0\n",
    "nodeNum = 0\n",
    "jobWaitingLength = 0\n",
    "adjacency = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda:0\n",
      "<class 'torch.device'>\n",
      "<built-in method cuda of Tensor object at 0x0000021E80C8E160>\n",
      "<class 'torch.device'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.get_device_name(device = 0))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "\n",
    "print(type(device))\n",
    "\n",
    "kkkkk = torch.Tensor(1)\n",
    "\n",
    "kkkkk = kkkkk.cuda()\n",
    "\n",
    "print(kkkkk.cuda)\n",
    "print(type(kkkkk.device))\n",
    "print(kkkkk.device == device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.0002 # 0.0003\n",
    "gamma           = 0.95\n",
    "entropy_weight  = 0.01 # 0.001\n",
    "val_loss_coef = 0.8\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.8\n",
    "eps_clip      = 0.2\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100\n",
    "hidden_feature_num = 0\n",
    "\n",
    "job_generate_rate = 0.003\n",
    "\n",
    "train_cycle = 300\n",
    "is_train = True\n",
    "train_quitient = 0\n",
    "\n",
    "replay_buffer_size = 10000\n",
    "history_learning_time = 6\n",
    "\n",
    "if not is_train:\n",
    "    train_quitient = train_cycle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os.path\n",
    "# import os\n",
    "# FIFO_FILENAME = './fifo-test'\n",
    "\n",
    "# if not os.path.exists(FIFO_FILENAME):\n",
    "#     os.mkfifo(FIFO_FILENAME)\n",
    "#     if os.path.exists(FIFO_FILENAME):\n",
    "#         fp_fifo = open(FIFO_FILENAME, \"rw\")\n",
    "\n",
    "# for i in range(128):\n",
    "#     fp_fifo.write(\"Hello,MakeCode\\n\")\n",
    "#     fp_fifo.write(\"\")\n",
    "\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\worker4\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"modelNum\":\"6\", \"availableJobNum\":\"5\", \"nodeNum\":\"10\", \"jobWaitingQueueLength\":\"15\", \"adjacencyList\":\"[[0,1,0,2,0,3,0,6,0,8,1,3,1,4,1,5,1,7,1,8,3,4,4,5,4,6,4,7,5,9,7,9,],[1,0,2,0,3,0,6,0,8,0,3,1,4,1,5,1,7,1,8,1,4,3,5,4,6,4,7,4,9,5,9,7,]]\", \"episode_length\":\"100\", \"job_generate_rate\":\"30\", \"node_capacity\":\"0.030000, 0.090000\"}\n",
      "네트워크 초기화 완료\n",
      "\n",
      "노드 개수 : 10\n",
      "네트워크 최대 job 개수 : 5\n",
      "job 대기 가능 개수 : 15\n",
      "최대 subtask 개수 : 6\n",
      "인접 리스트 : [[0, 1, 0, 2, 0, 3, 0, 6, 0, 8, 1, 3, 1, 4, 1, 5, 1, 7, 1, 8, 3, 4, 4, 5, 4, 6, 4, 7, 5, 9, 7, 9], [1, 0, 2, 0, 3, 0, 6, 0, 8, 0, 3, 1, 4, 1, 5, 1, 7, 1, 8, 1, 4, 3, 5, 4, 6, 4, 7, 4, 9, 5, 9, 7]]\n",
      "node_feature_num : 60\n",
      "queue_feature_num : 240\n",
      "episode_length : 100\n",
      "node_capacity : 0.030000, 0.090000\n",
      "entropy_weight : 0.01\n",
      "reward_weight : 0.16666666666666666\n",
      "job_generate_rate : 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "\n",
    "print(initial_message)\n",
    "\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "node_capacity = networkInfo['node_capacity']\n",
    "job_generate_rate = networkInfo['job_generate_rate']\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "\n",
    "reward_weight = 1/modelNum\n",
    "\n",
    "sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "info = f\"\"\"\n",
    "노드 개수 : {nodeNum}\n",
    "네트워크 최대 job 개수 : {availableJobNum}\n",
    "job 대기 가능 개수 : {jobWaitingLength}\n",
    "최대 subtask 개수 : {modelNum}\n",
    "인접 리스트 : {adjacency}\n",
    "node_feature_num : {node_feature_num}\n",
    "queue_feature_num : {queue_feature_num}\n",
    "episode_length : {episode_length}\n",
    "node_capacity : {node_capacity}\n",
    "entropy_weight : {entropy_weight}\n",
    "reward_weight : {reward_weight}\n",
    "job_generate_rate : {job_generate_rate}\n",
    "\"\"\"\n",
    "print(info)\n",
    "\n",
    "with open(f'{pathName}/info.txt', 'w') as f:\n",
    "    f.write(f'{info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = []\n",
    "        self.history = []\n",
    "\n",
    "        self.x_mean = 0\n",
    "        self.x2_mean = 0\n",
    "        self.sd = 0\n",
    "        self.reward_sample_num = 0\n",
    "\n",
    "        self.step = 0\n",
    "        #self.entropy_normalize_weight = 1/torch.log(torch.tensor(nodeNum))\n",
    "        self.entropy_normalize_weight = 1\n",
    "\n",
    "        \n",
    "        # print(f\"self.adjacency : \", self.adjacency.shape)\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * int(node_feature_num//2)), nn.ReLU())\n",
    "        self.pi_s_ecc1 = NNConv(node_feature_num, int(node_feature_num//2), self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_mlp2 = nn.Sequential(nn.Linear(1, int(node_feature_num//2) * int(node_feature_num//2)), nn.ReLU())\n",
    "        self.pi_s_ecc2 = NNConv(int(node_feature_num//2), int(node_feature_num//2), self.pi_mlp2, aggr='mean')\n",
    "\n",
    "        self.pi_graph_u_net1 = GraphUNet(int(node_feature_num//2), 50, int(node_feature_num//2), 3, 0.8)\n",
    "        self.pi_graph_u_net2 = GraphUNet(int(node_feature_num//2), 50, int(node_feature_num//2), 3, 0.8)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear((int(node_feature_num//2) + queue_feature_num) * 2, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidden_feature_num, hidden_feature_num), # 38부터 적용되는 코드. 38이전은 두 줄 지우고 실행하면됨.\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear((int(node_feature_num//2) + queue_feature_num) * 2, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num), # 38부터 적용되는 코드. 38이전은 두 줄 지우고 실행하면됨.\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        self.policy = nn.LSTM(input_size = hidden_feature_num, hidden_size = 100, num_layers = modelNum, batch_first=True)\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(100, nodeNum + 1) # nodeNum + voidAction\n",
    "\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(hidden_feature_num, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def gnn(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature_1, link_feature_1, adjacency_1 = data[0].x, data[0].edge_attr, data[0].edge_index\n",
    "        node_feature_2, link_feature_2, adjacency_2 = data[1].x, data[1].edge_attr, data[1].edge_index\n",
    "\n",
    "        job_waiting_feature_1 = job_waiting_feature[0]\n",
    "        job_waiting_feature_2 = job_waiting_feature[1]\n",
    "\n",
    "        if job_waiting_feature_2.device == 'cuda':\n",
    "            link_feature_1 = link_feature_1.cuda()\n",
    "            adjacency_1 = adjacency_1.cuda()\n",
    "\n",
    "            link_feature_2 = link_feature_2.cuda()\n",
    "            adjacency_2 = adjacency_2.cuda()\n",
    "\n",
    "        \n",
    "        # =========================================================================\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc1(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net1(node_feature_1, adjacency_1))\n",
    "\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc2(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net2(node_feature_1, adjacency_1))\n",
    "\n",
    "        data_num_1 = len(node_feature_1) // nodeNum\n",
    "        \n",
    "        readout_1 = global_mean_pool(node_feature_1, data[0].batch)\n",
    "\n",
    "        concat_1 = torch.cat([readout_1, job_waiting_feature_1], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc1(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net1(node_feature_2, adjacency_2))\n",
    "\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc2(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net2(node_feature_2, adjacency_2))\n",
    "\n",
    "        data_num_2 = len(node_feature_2) // nodeNum\n",
    "        \n",
    "        readout_2 = global_mean_pool(node_feature_2, data[0].batch)\n",
    "\n",
    "        concat_2 = torch.cat([readout_2, job_waiting_feature_2], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        concat = torch.cat([concat_1, concat_2], dim=1)\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        return feature_extract\n",
    "\n",
    "\n",
    "    def pi(self, state):\n",
    "\n",
    "        inp, h_c = state\n",
    "\n",
    "        output = self.policy(inp, h_c)\n",
    "\n",
    "\n",
    "        h_state = output[1][0]\n",
    "\n",
    "        output = self.pi_prob_fc(h_state)\n",
    "\n",
    "        # print(\"output\", output.shape)\n",
    "\n",
    "        prob = F.softmax(output, dim=1)\n",
    "\n",
    "        # print(\"prob\", prob.shape)\n",
    "        \n",
    "        # 아래는 엔트로피 구하는 과정\n",
    "        log_prob = F.log_softmax(output, dim=1)\n",
    "\n",
    "        # print(\"log_prob\", log_prob.shape)\n",
    "\n",
    "        en = - (log_prob * prob).sum(-1, keepdim=True)\n",
    "\n",
    "        # print(\"en\", en.shape)\n",
    "\n",
    "        entropy = torch.mean(en, dim=0)\n",
    "\n",
    "        # print(entropy.shape)\n",
    "        \n",
    "        return prob, entropy, output\n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state):\n",
    "\n",
    "        value = self.v_value_fc(state)\n",
    "\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "        \n",
    "    def return_link_dict(sel, ad):\n",
    "        result = {}\n",
    "        for i in range(len(ad[0])//2):\n",
    "            result[f'{ad[0][2*i]}{ad[0][2 *(i)+1]}'] = i\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def return_new_mean(self, mean, num, new_data):\n",
    "        result = (mean * num + new_data) / (num + 1)\n",
    "        return result\n",
    "        \n",
    "    def return_new_sd(self, square_mean, mean):\n",
    "        result = (square_mean - mean**2)**0.5\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def return_normalize_reward(self, reward):\n",
    "        self.x_mean = self.return_new_mean(self.x_mean, self.reward_sample_num, reward)\n",
    "        self.x2_mean = self.return_new_mean(self.x2_mean, self.reward_sample_num, reward**2)\n",
    "        self.sd = self.return_new_sd(self.x2_mean, self.x_mean)\n",
    "        \n",
    "        if self.sd == 0:\n",
    "                z_normalized = 0\n",
    "        else:  \n",
    "            z_normalized = (reward - self.x_mean) / self.sd\n",
    "\n",
    "        self.reward_sample_num += 1\n",
    "        \n",
    "        return z_normalized\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "\n",
    "    def make_batch_history(self):\n",
    "        sample_index = torch.randperm(len(self.history))[:batch_size]\n",
    "\n",
    "        if len(sample_index) == 0:\n",
    "            return -1\n",
    "\n",
    "        sampled_data = []\n",
    "        for sample_idx in sample_index:\n",
    "            sampled_data.append(self.history[sample_idx])\n",
    "\n",
    "        b_size = len(sample_index)\n",
    "\n",
    "        network_1_lst = [None] * b_size\n",
    "        network_2_lst = [None] * b_size\n",
    "        next_network_1_lst = [None] * b_size\n",
    "        next_network_2_lst = [None] * b_size\n",
    "        job_waiting_1_lst = [None] * b_size\n",
    "        job_waiting_2_lst = [None] * b_size\n",
    "        next_job_waiting_1_lst = [None] * b_size\n",
    "        next_job_waiting_2_lst = [None] * b_size\n",
    "        a_lst = [None] * b_size\n",
    "        r_lst = [None] * b_size\n",
    "        prob_a_lst = [None] * b_size\n",
    "        sojourn_time_lst = [None] * b_size\n",
    "        action_mask_lst = [None] * b_size\n",
    "        subtask_index_lst = [None] * b_size\n",
    "\n",
    "        for idx, transition in enumerate(sampled_data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, sojourn_time, action_mask, subtask_index = transition\n",
    "\n",
    "            r_lst[idx] = [r*reward_weight]\n",
    "\n",
    "            network_1_lst[idx] = network[0]\n",
    "            network_2_lst[idx] = network[1]\n",
    "\n",
    "            job_waiting_1_lst[idx] = job_waiting[0].tolist()\n",
    "            job_waiting_2_lst[idx] = job_waiting[1].tolist()\n",
    "\n",
    "            a_lst[idx] = [a]\n",
    "\n",
    "            next_network_1_lst[idx] = nxt_network[0]\n",
    "            next_network_2_lst[idx] = nxt_network[1]\n",
    "            \n",
    "            next_job_waiting_1_lst[idx] = nxt_job_waiting[0].tolist()\n",
    "            next_job_waiting_2_lst[idx] = nxt_job_waiting[1].tolist()\n",
    "\n",
    "            prob_a_lst[idx] = [prob_a]\n",
    "            sojourn_time_lst[idx] = [sojourn_time * 5]\n",
    "\n",
    "            action_mask_lst[idx] = action_mask\n",
    "            subtask_index_lst[idx] = [subtask_index]\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_1_loader = DataLoader(network_1_lst, batch_size=len(network_1_lst))\n",
    "        next_network_1_loader = DataLoader(next_network_1_lst, batch_size=len(network_1_lst))\n",
    "        network_1_batch = next(iter(network_1_loader))\n",
    "        next_network_1_batch = next(iter(next_network_1_loader))\n",
    "\n",
    "        network_2_loader = DataLoader(network_2_lst, batch_size=len(network_2_lst))\n",
    "        next_network_2_loader = DataLoader(next_network_2_lst, batch_size=len(network_2_lst))\n",
    "        network_2_batch = next(iter(network_2_loader))\n",
    "        next_network_2_batch = next(iter(next_network_2_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting_1 = torch.squeeze(torch.tensor(np.array(job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        job_waiting_2 = torch.squeeze(torch.tensor(np.array(job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "\n",
    "        next_job_waiting_1 = torch.squeeze(torch.tensor(np.array(next_job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        next_job_waiting_2 = torch.squeeze(torch.tensor(np.array(next_job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "        \n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "\n",
    "        subtask_index_lst = torch.tensor(subtask_index_lst, dtype=torch.long)\n",
    "        \n",
    "        # self.data = []\n",
    "        return [network_1_batch, network_2_batch], [job_waiting_1, job_waiting_2], a, r, [next_network_1_batch, next_network_2_batch], [next_job_waiting_1, next_job_waiting_2], prob_a, sojourn_time, action_mask_lst, subtask_index_lst, min(batch_size, idx + 1)\n",
    "\n",
    "        \n",
    "    def make_batch(self, isFirst):\n",
    "\n",
    "        if(isFirst):\n",
    "            self.data = self.data[::-1]\n",
    "\n",
    "        b_size = min(len(self.data), batch_size)\n",
    "\n",
    "        network_1_lst = [None] * b_size\n",
    "        network_2_lst = [None] * b_size\n",
    "        next_network_1_lst = [None] * b_size\n",
    "        next_network_2_lst = [None] * b_size\n",
    "        job_waiting_1_lst = [None] * b_size\n",
    "        job_waiting_2_lst = [None] * b_size\n",
    "        next_job_waiting_1_lst = [None] * b_size\n",
    "        next_job_waiting_2_lst = [None] * b_size\n",
    "        a_lst = [None] * b_size\n",
    "        r_lst = [None] * b_size\n",
    "        prob_a_lst = [None] * b_size\n",
    "        sojourn_time_lst = [None] * b_size\n",
    "        action_mask_lst = [None] * b_size\n",
    "        subtask_index_lst = [None] * b_size\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, sojourn_time, action_mask, subtask_index = transition\n",
    "\n",
    "            r_lst[idx] = [r*reward_weight]\n",
    "\n",
    "            network_1_lst[idx] = network[0]\n",
    "            network_2_lst[idx] = network[1]\n",
    "\n",
    "            job_waiting_1_lst[idx] = job_waiting[0].tolist()\n",
    "            job_waiting_2_lst[idx] = job_waiting[1].tolist()\n",
    "\n",
    "            a_lst[idx] = [a]\n",
    "\n",
    "            next_network_1_lst[idx] = nxt_network[0]\n",
    "            next_network_2_lst[idx] = nxt_network[1]\n",
    "            \n",
    "            next_job_waiting_1_lst[idx] = nxt_job_waiting[0].tolist()\n",
    "            next_job_waiting_2_lst[idx] = nxt_job_waiting[1].tolist()\n",
    "\n",
    "            prob_a_lst[idx] = [prob_a]\n",
    "            sojourn_time_lst[idx] = [sojourn_time * 5]\n",
    "\n",
    "            action_mask_lst[idx] = action_mask\n",
    "            subtask_index_lst[idx] = [subtask_index]\n",
    "\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_1_loader = DataLoader(network_1_lst, batch_size=len(network_1_lst))\n",
    "        next_network_1_loader = DataLoader(next_network_1_lst, batch_size=len(network_1_lst))\n",
    "        network_1_batch = next(iter(network_1_loader))\n",
    "        next_network_1_batch = next(iter(next_network_1_loader))\n",
    "\n",
    "        network_2_loader = DataLoader(network_2_lst, batch_size=len(network_2_lst))\n",
    "        next_network_2_loader = DataLoader(next_network_2_lst, batch_size=len(network_2_lst))\n",
    "        network_2_batch = next(iter(network_2_loader))\n",
    "        next_network_2_batch = next(iter(next_network_2_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting_1 = torch.squeeze(torch.tensor(np.array(job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        job_waiting_2 = torch.squeeze(torch.tensor(np.array(job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "\n",
    "        next_job_waiting_1 = torch.squeeze(torch.tensor(np.array(next_job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        next_job_waiting_2 = torch.squeeze(torch.tensor(np.array(next_job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "        \n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        subtask_index_lst = torch.tensor(subtask_index_lst, dtype=torch.long)\n",
    "        \n",
    "        # self.data = []\n",
    "        return [network_1_batch, network_2_batch], [job_waiting_1, job_waiting_2], a, r, [next_network_1_batch, next_network_2_batch], [next_job_waiting_1, next_job_waiting_2], prob_a, sojourn_time, action_mask_lst, subtask_index_lst, min(batch_size, idx + 1)\n",
    "    \n",
    "    def train_net(self):\n",
    "        self = self.cuda()\n",
    "        \n",
    "        pre_advantage = 0.0\n",
    "        first = True\n",
    "        while len(self.data) > 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, sojourn_time, action_mask, subtask_index, N_size = self.make_batch(first)\n",
    "\n",
    "            if first:\n",
    "                first = False\n",
    "            \n",
    "            network_batch_1 = network_batch[0].clone().cuda()\n",
    "            network_batch_2 = network_batch[1].clone().cuda()\n",
    "\n",
    "            network_batch = [network_batch_1, network_batch_2]\n",
    "\n",
    "            job_waiting_1 = job_waiting[0].clone().cuda()\n",
    "            job_waiting_2 = job_waiting[1].clone().cuda()\n",
    "\n",
    "            job_waiting = [job_waiting_1, job_waiting_2]\n",
    "\n",
    "            a = a.clone().cuda()\n",
    "            r = r.clone().cuda()\n",
    "\n",
    "            next_network_batch_1 = next_network_batch[0].clone().cuda()\n",
    "            next_network_batch_2 = next_network_batch[1].clone().cuda()\n",
    "\n",
    "            next_network_batch = [next_network_batch_1, next_network_batch_2]\n",
    "\n",
    "            next_job_waiting_1 = next_job_waiting[0].clone().cuda()\n",
    "            next_job_waiting_2 = next_job_waiting[1].clone().cuda()\n",
    "\n",
    "            next_job_waiting = [next_job_waiting_1, next_job_waiting_2]\n",
    "\n",
    "            prob_a = prob_a.clone().cuda()\n",
    "            sojourn_time = sojourn_time.clone().cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).clone().cuda()\n",
    "            action_mask = action_mask.clone().cuda()\n",
    "            subtask_index = subtask_index.clone().cuda()\n",
    "\n",
    "            for i in range(2):\n",
    "                \n",
    "                # void\n",
    "                next_state = self.gnn([next_network_batch, next_job_waiting])\n",
    "                cur_state = self.gnn([network_batch, job_waiting])\n",
    "\n",
    "                next_v = self.v(next_state)\n",
    "                cur_v = self.v(cur_state)\n",
    "\n",
    "                td_target = r + gamma_gpu * next_v\n",
    "                delta = td_target - cur_v\n",
    "                \n",
    "                delta = delta.detach().to('cpu').numpy()\n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = gamma_gpu * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                # advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "\n",
    "                pre_advantage = advantage\n",
    "\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(cur_v , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                cur_state = self.gnn([network_batch, job_waiting])\n",
    "                new_state = cur_state.view(N_size, 1, -1)\n",
    "                new_state = new_state.repeat(1, modelNum, 1)\n",
    "\n",
    "                pi, current_entropy, outputs = self.pi([new_state, (torch.zeros(modelNum, N_size, 100).cuda(), torch.zeros(modelNum, N_size, 100).cuda())])\n",
    "\n",
    "\n",
    "                current_entropy = self.entropy_normalize_weight * current_entropy\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                #outputs = outputs.clone().detach()\n",
    "\n",
    "                indices = subtask_index.view(-1)\n",
    "                indexed_outputs = outputs[indices, torch.arange(N_size), :]\n",
    "                indexed_outputs = indexed_outputs * action_mask\n",
    "\n",
    "                exp_sum = torch.sum(torch.exp(indexed_outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "                exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "                indexed_outputs_a = indexed_outputs.gather(1,a)\n",
    "\n",
    "                pi_a = torch.exp(indexed_outputs_a) / exp_sum\n",
    "\n",
    "                # pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.min(torch.tensor(88), torch.log(pi_a + 1e-9) - torch.log(prob_a + 1e-9)))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * advantage_lst\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage_lst\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train_net_history(self):\n",
    "\n",
    "        self = self.cuda()\n",
    "        # pre_advantage = 0.0\n",
    "        if len(self.history) <= 0:\n",
    "            return\n",
    "\n",
    "        for i in range(history_learning_time):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            temp_return = self.make_batch_history()\n",
    "\n",
    "            if temp_return == -1:\n",
    "                return\n",
    "\n",
    "            network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, sojourn_time, action_mask, subtask_index, N_size = temp_return\n",
    "\n",
    "            if len(network_batch) == 0:\n",
    "                return\n",
    "            \n",
    "            network_batch_1 = network_batch[0].clone().cuda()\n",
    "            network_batch_2 = network_batch[1].clone().cuda()\n",
    "\n",
    "            network_batch = [network_batch_1, network_batch_2]\n",
    "\n",
    "            job_waiting_1 = job_waiting[0].clone().cuda()\n",
    "            job_waiting_2 = job_waiting[1].clone().cuda()\n",
    "\n",
    "            job_waiting = [job_waiting_1, job_waiting_2]\n",
    "\n",
    "            a = a.clone().cuda()\n",
    "            r = r.clone().cuda()\n",
    "\n",
    "            next_network_batch_1 = next_network_batch[0].clone().cuda()\n",
    "            next_network_batch_2 = next_network_batch[1].clone().cuda()\n",
    "\n",
    "            next_network_batch = [next_network_batch_1, next_network_batch_2]\n",
    "\n",
    "            next_job_waiting_1 = next_job_waiting[0].clone().cuda()\n",
    "            next_job_waiting_2 = next_job_waiting[1].clone().cuda()\n",
    "\n",
    "            next_job_waiting = [next_job_waiting_1, next_job_waiting_2]\n",
    "\n",
    "            prob_a = prob_a.clone().cuda()\n",
    "            sojourn_time = sojourn_time.clone().cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).clone().cuda()\n",
    "            action_mask = action_mask.clone().cuda()\n",
    "            subtask_index = subtask_index.clone().cuda()\n",
    "\n",
    "            next_state = self.gnn([next_network_batch, next_job_waiting])\n",
    "            cur_state = self.gnn([network_batch, job_waiting])\n",
    "\n",
    "            next_v = self.v(next_state)\n",
    "            cur_v = self.v(cur_state)\n",
    "\n",
    "            td_target = r + gamma_gpu * next_v\n",
    "            v_loss = val_loss_coef * F.smooth_l1_loss(cur_v , td_target.detach())\n",
    "            #print(\"v_loss\", v_loss)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            v_loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            td_target = r + gamma_gpu * next_v\n",
    "            delta = td_target - cur_v\n",
    "\n",
    "            cur_state = self.gnn([network_batch, job_waiting])\n",
    "            new_state = cur_state.view(N_size, 1, -1)\n",
    "            new_state = new_state.repeat(1, modelNum, 1)\n",
    "\n",
    "            pi, current_entropy, outputs = self.pi([new_state, (torch.zeros(modelNum, N_size, 100).cuda(), torch.zeros(modelNum, N_size, 100).cuda())])\n",
    "\n",
    "\n",
    "            current_entropy = self.entropy_normalize_weight * current_entropy\n",
    "\n",
    "            # true가 valid action\n",
    "\n",
    "            #outputs = outputs.clone().detach()\n",
    "\n",
    "            indices = subtask_index.view(-1)\n",
    "            indexed_outputs = outputs[indices, torch.arange(N_size), :]\n",
    "            indexed_outputs = indexed_outputs * action_mask\n",
    "\n",
    "            exp_sum = torch.sum(torch.exp(indexed_outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "            exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "            indexed_outputs_a = indexed_outputs.gather(1,a)\n",
    "\n",
    "            pi_a = torch.exp(indexed_outputs_a) / exp_sum\n",
    "\n",
    "            # pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.min(torch.tensor(88), torch.log(pi_a + 1e-9) - torch.log(prob_a + 1e-9)))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * delta.detach()\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * delta.detach()\n",
    "            pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            pi_loss.mean().backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-13 11:11:00 AM] training....\n",
      "[2023-01-13 11:11:05 AM] training complete\n",
      "[2023-01-13 11:11:05 AM] training replay buffer....\n",
      "[2023-01-13 11:11:10 AM] training complete\n",
      "[2023-01-13 11:11:12 AM] training....\n",
      "[2023-01-13 11:11:15 AM] training complete\n",
      "[2023-01-13 11:11:15 AM] training replay buffer....\n",
      "[2023-01-13 11:11:20 AM] training complete\n",
      "[2023-01-13 11:11:22 AM] training....\n",
      "[2023-01-13 11:11:24 AM] training complete\n",
      "[2023-01-13 11:11:24 AM] training replay buffer....\n",
      "[2023-01-13 11:11:29 AM] training complete\n",
      "[2023-01-13 11:11:30 AM] training....\n",
      "[2023-01-13 11:11:32 AM] training complete\n",
      "[2023-01-13 11:11:32 AM] training replay buffer....\n",
      "[2023-01-13 11:11:38 AM] training complete\n",
      "[2023-01-13 11:11:39 AM] training....\n",
      "[2023-01-13 11:11:43 AM] training complete\n",
      "[2023-01-13 11:11:43 AM] training replay buffer....\n",
      "[2023-01-13 11:11:47 AM] training complete\n",
      "[2023-01-13 11:11:49 AM] training....\n",
      "[2023-01-13 11:11:52 AM] training complete\n",
      "[2023-01-13 11:11:52 AM] training replay buffer....\n",
      "[2023-01-13 11:11:57 AM] training complete\n",
      "[2023-01-13 11:11:59 AM] training....\n",
      "[2023-01-13 11:12:02 AM] training complete\n",
      "[2023-01-13 11:12:02 AM] training replay buffer....\n",
      "[2023-01-13 11:12:07 AM] training complete\n",
      "[2023-01-13 11:12:09 AM] training....\n",
      "[2023-01-13 11:12:12 AM] training complete\n",
      "[2023-01-13 11:12:12 AM] training replay buffer....\n",
      "[2023-01-13 11:12:16 AM] training complete\n",
      "[2023-01-13 11:12:18 AM] training....\n",
      "[2023-01-13 11:12:20 AM] training complete\n",
      "[2023-01-13 11:12:20 AM] training replay buffer....\n",
      "[2023-01-13 11:12:24 AM] training complete\n",
      "[2023-01-13 11:12:26 AM] training....\n",
      "[2023-01-13 11:12:29 AM] training complete\n",
      "[2023-01-13 11:12:29 AM] training replay buffer....\n",
      "[2023-01-13 11:12:34 AM] training complete\n",
      "[2023-01-13 11:12:36 AM] training....\n",
      "[2023-01-13 11:12:37 AM] training complete\n",
      "[2023-01-13 11:12:37 AM] training replay buffer....\n",
      "[2023-01-13 11:12:43 AM] training complete\n",
      "[2023-01-13 11:12:44 AM] training....\n",
      "[2023-01-13 11:12:47 AM] training complete\n",
      "[2023-01-13 11:12:47 AM] training replay buffer....\n",
      "[2023-01-13 11:12:52 AM] training complete\n",
      "[2023-01-13 11:12:53 AM] training....\n",
      "[2023-01-13 11:12:55 AM] training complete\n",
      "[2023-01-13 11:12:55 AM] training replay buffer....\n",
      "[2023-01-13 11:13:00 AM] training complete\n",
      "[2023-01-13 11:13:02 AM] training....\n",
      "[2023-01-13 11:13:05 AM] training complete\n",
      "[2023-01-13 11:13:05 AM] training replay buffer....\n",
      "[2023-01-13 11:13:10 AM] training complete\n",
      "[2023-01-13 11:13:12 AM] training....\n",
      "[2023-01-13 11:13:13 AM] training complete\n",
      "[2023-01-13 11:13:13 AM] training replay buffer....\n",
      "[2023-01-13 11:13:18 AM] training complete\n",
      "[2023-01-13 11:13:19 AM] training....\n",
      "[2023-01-13 11:13:21 AM] training complete\n",
      "[2023-01-13 11:13:21 AM] training replay buffer....\n",
      "[2023-01-13 11:13:26 AM] training complete\n",
      "[2023-01-13 11:13:28 AM] training....\n",
      "[2023-01-13 11:13:31 AM] training complete\n",
      "[2023-01-13 11:13:31 AM] training replay buffer....\n",
      "[2023-01-13 11:13:36 AM] training complete\n",
      "[2023-01-13 11:13:38 AM] training....\n",
      "[2023-01-13 11:13:41 AM] training complete\n",
      "[2023-01-13 11:13:41 AM] training replay buffer....\n",
      "[2023-01-13 11:13:46 AM] training complete\n",
      "[2023-01-13 11:13:48 AM] training....\n",
      "[2023-01-13 11:13:51 AM] training complete\n",
      "[2023-01-13 11:13:51 AM] training replay buffer....\n",
      "[2023-01-13 11:13:56 AM] training complete\n",
      "[2023-01-13 11:13:57 AM] training....\n",
      "[2023-01-13 11:14:01 AM] training complete\n",
      "[2023-01-13 11:14:01 AM] training replay buffer....\n",
      "[2023-01-13 11:14:05 AM] training complete\n",
      "[2023-01-13 11:14:07 AM] training....\n",
      "[2023-01-13 11:14:11 AM] training complete\n",
      "[2023-01-13 11:14:11 AM] training replay buffer....\n",
      "[2023-01-13 11:14:16 AM] training complete\n",
      "[2023-01-13 11:14:17 AM] training....\n",
      "[2023-01-13 11:14:20 AM] training complete\n",
      "[2023-01-13 11:14:20 AM] training replay buffer....\n",
      "[2023-01-13 11:14:25 AM] training complete\n",
      "[2023-01-13 11:14:27 AM] training....\n",
      "[2023-01-13 11:14:31 AM] training complete\n",
      "[2023-01-13 11:14:31 AM] training replay buffer....\n",
      "[2023-01-13 11:14:35 AM] training complete\n",
      "[2023-01-13 11:14:37 AM] training....\n",
      "[2023-01-13 11:14:40 AM] training complete\n",
      "[2023-01-13 11:14:40 AM] training replay buffer....\n",
      "[2023-01-13 11:14:45 AM] training complete\n",
      "[2023-01-13 11:14:46 AM] training....\n",
      "[2023-01-13 11:14:48 AM] training complete\n",
      "[2023-01-13 11:14:48 AM] training replay buffer....\n",
      "[2023-01-13 11:14:53 AM] training complete\n",
      "[2023-01-13 11:14:55 AM] training....\n",
      "[2023-01-13 11:14:58 AM] training complete\n",
      "[2023-01-13 11:14:58 AM] training replay buffer....\n",
      "[2023-01-13 11:15:02 AM] training complete\n",
      "[2023-01-13 11:15:04 AM] training....\n",
      "[2023-01-13 11:15:07 AM] training complete\n",
      "[2023-01-13 11:15:07 AM] training replay buffer....\n",
      "[2023-01-13 11:15:12 AM] training complete\n",
      "[2023-01-13 11:15:14 AM] training....\n",
      "[2023-01-13 11:15:17 AM] training complete\n",
      "[2023-01-13 11:15:17 AM] training replay buffer....\n",
      "[2023-01-13 11:15:22 AM] training complete\n",
      "[2023-01-13 11:15:24 AM] training....\n",
      "[2023-01-13 11:15:27 AM] training complete\n",
      "[2023-01-13 11:15:27 AM] training replay buffer....\n",
      "[2023-01-13 11:15:32 AM] training complete\n",
      "[2023-01-13 11:15:33 AM] training....\n",
      "[2023-01-13 11:15:36 AM] training complete\n",
      "[2023-01-13 11:15:36 AM] training replay buffer....\n",
      "[2023-01-13 11:15:41 AM] training complete\n",
      "[2023-01-13 11:15:43 AM] training....\n",
      "[2023-01-13 11:15:46 AM] training complete\n",
      "[2023-01-13 11:15:46 AM] training replay buffer....\n",
      "[2023-01-13 11:15:51 AM] training complete\n",
      "[2023-01-13 11:15:52 AM] training....\n",
      "[2023-01-13 11:15:54 AM] training complete\n",
      "[2023-01-13 11:15:54 AM] training replay buffer....\n",
      "[2023-01-13 11:15:59 AM] training complete\n",
      "[2023-01-13 11:16:01 AM] training....\n",
      "[2023-01-13 11:16:04 AM] training complete\n",
      "[2023-01-13 11:16:04 AM] training replay buffer....\n",
      "[2023-01-13 11:16:08 AM] training complete\n",
      "[2023-01-13 11:16:10 AM] training....\n",
      "[2023-01-13 11:16:11 AM] training complete\n",
      "[2023-01-13 11:16:11 AM] training replay buffer....\n",
      "[2023-01-13 11:16:17 AM] training complete\n",
      "[2023-01-13 11:16:18 AM] training....\n",
      "[2023-01-13 11:16:21 AM] training complete\n",
      "[2023-01-13 11:16:21 AM] training replay buffer....\n",
      "[2023-01-13 11:16:26 AM] training complete\n",
      "[2023-01-13 11:16:27 AM] training....\n",
      "[2023-01-13 11:16:31 AM] training complete\n",
      "[2023-01-13 11:16:31 AM] training replay buffer....\n",
      "[2023-01-13 11:16:35 AM] training complete\n",
      "[2023-01-13 11:16:37 AM] training....\n",
      "[2023-01-13 11:16:40 AM] training complete\n",
      "[2023-01-13 11:16:40 AM] training replay buffer....\n",
      "[2023-01-13 11:16:45 AM] training complete\n",
      "[2023-01-13 11:16:46 AM] training....\n",
      "[2023-01-13 11:16:48 AM] training complete\n",
      "[2023-01-13 11:16:48 AM] training replay buffer....\n",
      "[2023-01-13 11:16:53 AM] training complete\n",
      "[2023-01-13 11:16:55 AM] training....\n",
      "[2023-01-13 11:16:59 AM] training complete\n",
      "[2023-01-13 11:16:59 AM] training replay buffer....\n",
      "[2023-01-13 11:17:03 AM] training complete\n",
      "[2023-01-13 11:17:05 AM] training....\n",
      "[2023-01-13 11:17:08 AM] training complete\n",
      "[2023-01-13 11:17:08 AM] training replay buffer....\n",
      "[2023-01-13 11:17:13 AM] training complete\n",
      "[2023-01-13 11:17:14 AM] training....\n",
      "[2023-01-13 11:17:17 AM] training complete\n",
      "[2023-01-13 11:17:17 AM] training replay buffer....\n",
      "[2023-01-13 11:17:23 AM] training complete\n",
      "[2023-01-13 11:17:24 AM] training....\n",
      "[2023-01-13 11:17:25 AM] training complete\n",
      "[2023-01-13 11:17:25 AM] training replay buffer....\n",
      "[2023-01-13 11:17:30 AM] training complete\n",
      "[2023-01-13 11:17:32 AM] training....\n",
      "[2023-01-13 11:17:35 AM] training complete\n",
      "[2023-01-13 11:17:35 AM] training replay buffer....\n",
      "[2023-01-13 11:17:40 AM] training complete\n",
      "[2023-01-13 11:17:42 AM] training....\n",
      "[2023-01-13 11:17:45 AM] training complete\n",
      "[2023-01-13 11:17:45 AM] training replay buffer....\n",
      "[2023-01-13 11:17:49 AM] training complete\n",
      "[2023-01-13 11:17:51 AM] training....\n",
      "[2023-01-13 11:17:54 AM] training complete\n",
      "[2023-01-13 11:17:54 AM] training replay buffer....\n",
      "[2023-01-13 11:17:59 AM] training complete\n",
      "[2023-01-13 11:18:01 AM] training....\n",
      "[2023-01-13 11:18:05 AM] training complete\n",
      "[2023-01-13 11:18:05 AM] training replay buffer....\n",
      "[2023-01-13 11:18:10 AM] training complete\n",
      "[2023-01-13 11:18:12 AM] training....\n",
      "[2023-01-13 11:18:15 AM] training complete\n",
      "[2023-01-13 11:18:15 AM] training replay buffer....\n",
      "[2023-01-13 11:18:20 AM] training complete\n",
      "[2023-01-13 11:18:22 AM] training....\n",
      "[2023-01-13 11:18:25 AM] training complete\n",
      "[2023-01-13 11:18:25 AM] training replay buffer....\n",
      "[2023-01-13 11:18:30 AM] training complete\n",
      "[2023-01-13 11:18:32 AM] training....\n",
      "[2023-01-13 11:18:34 AM] training complete\n",
      "[2023-01-13 11:18:34 AM] training replay buffer....\n",
      "[2023-01-13 11:18:39 AM] training complete\n",
      "[2023-01-13 11:18:40 AM] training....\n",
      "[2023-01-13 11:18:41 AM] training complete\n",
      "[2023-01-13 11:18:41 AM] training replay buffer....\n",
      "[2023-01-13 11:18:46 AM] training complete\n",
      "[2023-01-13 11:18:48 AM] training....\n",
      "[2023-01-13 11:18:51 AM] training complete\n",
      "[2023-01-13 11:18:51 AM] training replay buffer....\n",
      "[2023-01-13 11:18:56 AM] training complete\n",
      "[2023-01-13 11:18:57 AM] training....\n",
      "[2023-01-13 11:19:00 AM] training complete\n",
      "[2023-01-13 11:19:00 AM] training replay buffer....\n",
      "[2023-01-13 11:19:05 AM] training complete\n",
      "[2023-01-13 11:19:07 AM] training....\n",
      "[2023-01-13 11:19:11 AM] training complete\n",
      "[2023-01-13 11:19:11 AM] training replay buffer....\n",
      "[2023-01-13 11:19:15 AM] training complete\n",
      "[2023-01-13 11:19:17 AM] training....\n",
      "[2023-01-13 11:19:20 AM] training complete\n",
      "[2023-01-13 11:19:20 AM] training replay buffer....\n",
      "[2023-01-13 11:19:25 AM] training complete\n",
      "[2023-01-13 11:19:26 AM] training....\n",
      "[2023-01-13 11:19:28 AM] training complete\n",
      "[2023-01-13 11:19:28 AM] training replay buffer....\n",
      "[2023-01-13 11:19:33 AM] training complete\n",
      "[2023-01-13 11:19:35 AM] training....\n",
      "[2023-01-13 11:19:38 AM] training complete\n",
      "[2023-01-13 11:19:38 AM] training replay buffer....\n",
      "[2023-01-13 11:19:43 AM] training complete\n",
      "[2023-01-13 11:19:44 AM] training....\n",
      "[2023-01-13 11:19:48 AM] training complete\n",
      "[2023-01-13 11:19:48 AM] training replay buffer....\n",
      "[2023-01-13 11:19:53 AM] training complete\n",
      "[2023-01-13 11:19:55 AM] training....\n",
      "[2023-01-13 11:19:58 AM] training complete\n",
      "[2023-01-13 11:19:58 AM] training replay buffer....\n",
      "[2023-01-13 11:20:02 AM] training complete\n",
      "[2023-01-13 11:20:04 AM] training....\n",
      "[2023-01-13 11:20:07 AM] training complete\n",
      "[2023-01-13 11:20:07 AM] training replay buffer....\n",
      "[2023-01-13 11:20:12 AM] training complete\n",
      "[2023-01-13 11:20:14 AM] training....\n",
      "[2023-01-13 11:20:15 AM] training complete\n",
      "[2023-01-13 11:20:15 AM] training replay buffer....\n",
      "[2023-01-13 11:20:20 AM] training complete\n",
      "[2023-01-13 11:20:21 AM] training....\n",
      "[2023-01-13 11:20:24 AM] training complete\n",
      "[2023-01-13 11:20:24 AM] training replay buffer....\n",
      "[2023-01-13 11:20:29 AM] training complete\n",
      "[2023-01-13 11:20:31 AM] training....\n",
      "[2023-01-13 11:20:34 AM] training complete\n",
      "[2023-01-13 11:20:34 AM] training replay buffer....\n",
      "[2023-01-13 11:20:39 AM] training complete\n",
      "[2023-01-13 11:20:41 AM] training....\n",
      "[2023-01-13 11:20:43 AM] training complete\n",
      "[2023-01-13 11:20:43 AM] training replay buffer....\n",
      "[2023-01-13 11:20:47 AM] training complete\n",
      "[2023-01-13 11:20:49 AM] training....\n",
      "[2023-01-13 11:20:50 AM] training complete\n",
      "[2023-01-13 11:20:50 AM] training replay buffer....\n",
      "[2023-01-13 11:20:55 AM] training complete\n",
      "[2023-01-13 11:20:57 AM] training....\n",
      "[2023-01-13 11:21:01 AM] training complete\n",
      "[2023-01-13 11:21:01 AM] training replay buffer....\n",
      "[2023-01-13 11:21:05 AM] training complete\n",
      "[2023-01-13 11:21:07 AM] training....\n",
      "[2023-01-13 11:21:10 AM] training complete\n",
      "[2023-01-13 11:21:10 AM] training replay buffer....\n",
      "[2023-01-13 11:21:15 AM] training complete\n",
      "[2023-01-13 11:21:17 AM] training....\n",
      "[2023-01-13 11:21:19 AM] training complete\n",
      "[2023-01-13 11:21:19 AM] training replay buffer....\n",
      "[2023-01-13 11:21:24 AM] training complete\n",
      "[2023-01-13 11:21:26 AM] training....\n",
      "[2023-01-13 11:21:29 AM] training complete\n",
      "[2023-01-13 11:21:29 AM] training replay buffer....\n",
      "[2023-01-13 11:21:34 AM] training complete\n",
      "[2023-01-13 11:21:35 AM] training....\n",
      "[2023-01-13 11:21:37 AM] training complete\n",
      "[2023-01-13 11:21:37 AM] training replay buffer....\n",
      "[2023-01-13 11:21:42 AM] training complete\n",
      "[2023-01-13 11:21:43 AM] training....\n",
      "[2023-01-13 11:21:46 AM] training complete\n",
      "[2023-01-13 11:21:46 AM] training replay buffer....\n",
      "[2023-01-13 11:21:51 AM] training complete\n",
      "[2023-01-13 11:21:53 AM] training....\n",
      "[2023-01-13 11:21:56 AM] training complete\n",
      "[2023-01-13 11:21:56 AM] training replay buffer....\n",
      "[2023-01-13 11:22:01 AM] training complete\n",
      "[2023-01-13 11:22:03 AM] training....\n",
      "[2023-01-13 11:22:07 AM] training complete\n",
      "[2023-01-13 11:22:07 AM] training replay buffer....\n",
      "[2023-01-13 11:22:12 AM] training complete\n",
      "[2023-01-13 11:22:13 AM] training....\n",
      "[2023-01-13 11:22:16 AM] training complete\n",
      "[2023-01-13 11:22:16 AM] training replay buffer....\n",
      "[2023-01-13 11:22:21 AM] training complete\n",
      "[2023-01-13 11:22:23 AM] training....\n",
      "[2023-01-13 11:22:26 AM] training complete\n",
      "[2023-01-13 11:22:26 AM] training replay buffer....\n",
      "[2023-01-13 11:22:31 AM] training complete\n",
      "[2023-01-13 11:22:32 AM] training....\n",
      "[2023-01-13 11:22:35 AM] training complete\n",
      "[2023-01-13 11:22:35 AM] training replay buffer....\n",
      "[2023-01-13 11:22:40 AM] training complete\n",
      "[2023-01-13 11:22:42 AM] training....\n",
      "[2023-01-13 11:22:45 AM] training complete\n",
      "[2023-01-13 11:22:45 AM] training replay buffer....\n",
      "[2023-01-13 11:22:50 AM] training complete\n",
      "[2023-01-13 11:22:52 AM] training....\n",
      "[2023-01-13 11:22:55 AM] training complete\n",
      "[2023-01-13 11:22:55 AM] training replay buffer....\n",
      "[2023-01-13 11:23:00 AM] training complete\n",
      "[2023-01-13 11:23:02 AM] training....\n",
      "[2023-01-13 11:23:05 AM] training complete\n",
      "[2023-01-13 11:23:05 AM] training replay buffer....\n",
      "[2023-01-13 11:23:10 AM] training complete\n",
      "[2023-01-13 11:23:11 AM] training....\n",
      "[2023-01-13 11:23:15 AM] training complete\n",
      "[2023-01-13 11:23:15 AM] training replay buffer....\n",
      "[2023-01-13 11:23:19 AM] training complete\n",
      "[2023-01-13 11:23:21 AM] training....\n",
      "[2023-01-13 11:23:25 AM] training complete\n",
      "[2023-01-13 11:23:25 AM] training replay buffer....\n",
      "[2023-01-13 11:23:29 AM] training complete\n",
      "[2023-01-13 11:23:31 AM] training....\n",
      "[2023-01-13 11:23:35 AM] training complete\n",
      "[2023-01-13 11:23:35 AM] training replay buffer....\n",
      "[2023-01-13 11:23:40 AM] training complete\n",
      "[2023-01-13 11:23:41 AM] training....\n",
      "[2023-01-13 11:23:44 AM] training complete\n",
      "[2023-01-13 11:23:44 AM] training replay buffer....\n",
      "[2023-01-13 11:23:49 AM] training complete\n",
      "[2023-01-13 11:23:51 AM] training....\n",
      "[2023-01-13 11:23:54 AM] training complete\n",
      "[2023-01-13 11:23:54 AM] training replay buffer....\n",
      "[2023-01-13 11:23:59 AM] training complete\n",
      "[2023-01-13 11:24:01 AM] training....\n",
      "[2023-01-13 11:24:04 AM] training complete\n",
      "[2023-01-13 11:24:04 AM] training replay buffer....\n",
      "[2023-01-13 11:24:09 AM] training complete\n",
      "[2023-01-13 11:24:10 AM] training....\n",
      "[2023-01-13 11:24:12 AM] training complete\n",
      "[2023-01-13 11:24:12 AM] training replay buffer....\n",
      "[2023-01-13 11:24:17 AM] training complete\n",
      "[2023-01-13 11:24:19 AM] training....\n",
      "[2023-01-13 11:24:22 AM] training complete\n",
      "[2023-01-13 11:24:22 AM] training replay buffer....\n",
      "[2023-01-13 11:24:27 AM] training complete\n",
      "[2023-01-13 11:24:28 AM] training....\n",
      "[2023-01-13 11:24:32 AM] training complete\n",
      "[2023-01-13 11:24:32 AM] training replay buffer....\n",
      "[2023-01-13 11:24:36 AM] training complete\n",
      "[2023-01-13 11:24:38 AM] training....\n",
      "[2023-01-13 11:24:41 AM] training complete\n",
      "[2023-01-13 11:24:41 AM] training replay buffer....\n",
      "[2023-01-13 11:24:46 AM] training complete\n",
      "[2023-01-13 11:24:48 AM] training....\n",
      "[2023-01-13 11:24:51 AM] training complete\n",
      "[2023-01-13 11:24:51 AM] training replay buffer....\n",
      "[2023-01-13 11:24:56 AM] training complete\n",
      "[2023-01-13 11:24:58 AM] training....\n",
      "[2023-01-13 11:25:02 AM] training complete\n",
      "[2023-01-13 11:25:02 AM] training replay buffer....\n",
      "[2023-01-13 11:25:07 AM] training complete\n",
      "[2023-01-13 11:25:08 AM] training....\n",
      "[2023-01-13 11:25:11 AM] training complete\n",
      "[2023-01-13 11:25:11 AM] training replay buffer....\n",
      "[2023-01-13 11:25:16 AM] training complete\n",
      "[2023-01-13 11:25:18 AM] training....\n",
      "[2023-01-13 11:25:21 AM] training complete\n",
      "[2023-01-13 11:25:21 AM] training replay buffer....\n",
      "[2023-01-13 11:25:27 AM] training complete\n",
      "[2023-01-13 11:25:28 AM] training....\n",
      "[2023-01-13 11:25:30 AM] training complete\n",
      "[2023-01-13 11:25:30 AM] training replay buffer....\n",
      "[2023-01-13 11:25:34 AM] training complete\n",
      "[2023-01-13 11:25:36 AM] training....\n",
      "[2023-01-13 11:25:37 AM] training complete\n",
      "[2023-01-13 11:25:37 AM] training replay buffer....\n",
      "[2023-01-13 11:25:42 AM] training complete\n",
      "[2023-01-13 11:25:44 AM] training....\n",
      "[2023-01-13 11:25:47 AM] training complete\n",
      "[2023-01-13 11:25:47 AM] training replay buffer....\n",
      "[2023-01-13 11:25:52 AM] training complete\n",
      "[2023-01-13 11:25:53 AM] training....\n",
      "[2023-01-13 11:25:55 AM] training complete\n",
      "[2023-01-13 11:25:55 AM] training replay buffer....\n",
      "[2023-01-13 11:26:00 AM] training complete\n",
      "[2023-01-13 11:26:01 AM] training....\n",
      "[2023-01-13 11:26:05 AM] training complete\n",
      "[2023-01-13 11:26:05 AM] training replay buffer....\n",
      "[2023-01-13 11:26:11 AM] training complete\n",
      "[2023-01-13 11:26:12 AM] training....\n",
      "[2023-01-13 11:26:15 AM] training complete\n",
      "[2023-01-13 11:26:15 AM] training replay buffer....\n",
      "[2023-01-13 11:26:20 AM] training complete\n",
      "[2023-01-13 11:26:22 AM] training....\n",
      "[2023-01-13 11:26:25 AM] training complete\n",
      "[2023-01-13 11:26:25 AM] training replay buffer....\n",
      "[2023-01-13 11:26:30 AM] training complete\n",
      "[2023-01-13 11:26:31 AM] training....\n",
      "[2023-01-13 11:26:35 AM] training complete\n",
      "[2023-01-13 11:26:35 AM] training replay buffer....\n",
      "[2023-01-13 11:26:39 AM] training complete\n",
      "[2023-01-13 11:26:41 AM] training....\n",
      "[2023-01-13 11:26:44 AM] training complete\n",
      "[2023-01-13 11:26:44 AM] training replay buffer....\n",
      "[2023-01-13 11:26:48 AM] training complete\n",
      "[2023-01-13 11:26:50 AM] training....\n",
      "[2023-01-13 11:26:53 AM] training complete\n",
      "[2023-01-13 11:26:53 AM] training replay buffer....\n",
      "[2023-01-13 11:26:58 AM] training complete\n",
      "[2023-01-13 11:27:00 AM] training....\n",
      "[2023-01-13 11:27:03 AM] training complete\n",
      "[2023-01-13 11:27:03 AM] training replay buffer....\n",
      "[2023-01-13 11:27:08 AM] training complete\n",
      "[2023-01-13 11:27:09 AM] training....\n",
      "[2023-01-13 11:27:11 AM] training complete\n",
      "[2023-01-13 11:27:11 AM] training replay buffer....\n",
      "[2023-01-13 11:27:17 AM] training complete\n",
      "[2023-01-13 11:27:18 AM] training....\n",
      "[2023-01-13 11:27:22 AM] training complete\n",
      "[2023-01-13 11:27:22 AM] training replay buffer....\n",
      "[2023-01-13 11:27:26 AM] training complete\n",
      "[2023-01-13 11:27:28 AM] training....\n",
      "[2023-01-13 11:27:31 AM] training complete\n",
      "[2023-01-13 11:27:31 AM] training replay buffer....\n",
      "[2023-01-13 11:27:37 AM] training complete\n",
      "[2023-01-13 11:27:38 AM] training....\n",
      "[2023-01-13 11:27:41 AM] training complete\n",
      "[2023-01-13 11:27:41 AM] training replay buffer....\n",
      "[2023-01-13 11:27:46 AM] training complete\n",
      "[2023-01-13 11:27:48 AM] training....\n",
      "[2023-01-13 11:27:51 AM] training complete\n",
      "[2023-01-13 11:27:51 AM] training replay buffer....\n",
      "[2023-01-13 11:27:56 AM] training complete\n",
      "[2023-01-13 11:27:57 AM] training....\n",
      "[2023-01-13 11:27:59 AM] training complete\n",
      "[2023-01-13 11:27:59 AM] training replay buffer....\n",
      "[2023-01-13 11:28:04 AM] training complete\n",
      "[2023-01-13 11:28:05 AM] training....\n",
      "[2023-01-13 11:28:09 AM] training complete\n",
      "[2023-01-13 11:28:09 AM] training replay buffer....\n",
      "[2023-01-13 11:28:13 AM] training complete\n",
      "[2023-01-13 11:28:15 AM] training....\n",
      "[2023-01-13 11:28:17 AM] training complete\n",
      "[2023-01-13 11:28:17 AM] training replay buffer....\n",
      "[2023-01-13 11:28:22 AM] training complete\n",
      "[2023-01-13 11:28:24 AM] training....\n",
      "[2023-01-13 11:28:27 AM] training complete\n",
      "[2023-01-13 11:28:27 AM] training replay buffer....\n",
      "[2023-01-13 11:28:32 AM] training complete\n",
      "[2023-01-13 11:28:33 AM] training....\n",
      "[2023-01-13 11:28:36 AM] training complete\n",
      "[2023-01-13 11:28:36 AM] training replay buffer....\n",
      "[2023-01-13 11:28:41 AM] training complete\n",
      "[2023-01-13 11:28:43 AM] training....\n",
      "[2023-01-13 11:28:46 AM] training complete\n",
      "[2023-01-13 11:28:46 AM] training replay buffer....\n",
      "[2023-01-13 11:28:51 AM] training complete\n",
      "[2023-01-13 11:28:52 AM] training....\n",
      "[2023-01-13 11:28:54 AM] training complete\n",
      "[2023-01-13 11:28:54 AM] training replay buffer....\n",
      "[2023-01-13 11:28:59 AM] training complete\n",
      "[2023-01-13 11:29:00 AM] training....\n",
      "[2023-01-13 11:29:03 AM] training complete\n",
      "[2023-01-13 11:29:03 AM] training replay buffer....\n",
      "[2023-01-13 11:29:08 AM] training complete\n",
      "[2023-01-13 11:29:09 AM] training....\n",
      "[2023-01-13 11:29:11 AM] training complete\n",
      "[2023-01-13 11:29:11 AM] training replay buffer....\n",
      "[2023-01-13 11:29:16 AM] training complete\n",
      "[2023-01-13 11:29:18 AM] training....\n",
      "[2023-01-13 11:29:22 AM] training complete\n",
      "[2023-01-13 11:29:22 AM] training replay buffer....\n",
      "[2023-01-13 11:29:27 AM] training complete\n",
      "[2023-01-13 11:29:28 AM] training....\n",
      "[2023-01-13 11:29:31 AM] training complete\n",
      "[2023-01-13 11:29:31 AM] training replay buffer....\n",
      "[2023-01-13 11:29:36 AM] training complete\n",
      "[2023-01-13 11:29:38 AM] training....\n",
      "[2023-01-13 11:29:41 AM] training complete\n",
      "[2023-01-13 11:29:41 AM] training replay buffer....\n",
      "[2023-01-13 11:29:46 AM] training complete\n",
      "[2023-01-13 11:29:47 AM] training....\n",
      "[2023-01-13 11:29:51 AM] training complete\n",
      "[2023-01-13 11:29:51 AM] training replay buffer....\n",
      "[2023-01-13 11:29:55 AM] training complete\n",
      "[2023-01-13 11:29:57 AM] training....\n",
      "[2023-01-13 11:30:00 AM] training complete\n",
      "[2023-01-13 11:30:00 AM] training replay buffer....\n",
      "[2023-01-13 11:30:05 AM] training complete\n",
      "[2023-01-13 11:30:07 AM] training....\n",
      "[2023-01-13 11:30:08 AM] training complete\n",
      "[2023-01-13 11:30:08 AM] training replay buffer....\n",
      "[2023-01-13 11:30:13 AM] training complete\n",
      "[2023-01-13 11:30:15 AM] training....\n",
      "[2023-01-13 11:30:18 AM] training complete\n",
      "[2023-01-13 11:30:18 AM] training replay buffer....\n",
      "[2023-01-13 11:30:22 AM] training complete\n",
      "[2023-01-13 11:30:24 AM] training....\n",
      "[2023-01-13 11:30:27 AM] training complete\n",
      "[2023-01-13 11:30:27 AM] training replay buffer....\n",
      "[2023-01-13 11:30:32 AM] training complete\n",
      "[2023-01-13 11:30:34 AM] training....\n",
      "[2023-01-13 11:30:37 AM] training complete\n",
      "[2023-01-13 11:30:37 AM] training replay buffer....\n",
      "[2023-01-13 11:30:41 AM] training complete\n",
      "[2023-01-13 11:30:43 AM] training....\n",
      "[2023-01-13 11:30:45 AM] training complete\n",
      "[2023-01-13 11:30:45 AM] training replay buffer....\n",
      "[2023-01-13 11:30:50 AM] training complete\n",
      "[2023-01-13 11:30:52 AM] training....\n",
      "[2023-01-13 11:30:55 AM] training complete\n",
      "[2023-01-13 11:30:55 AM] training replay buffer....\n",
      "[2023-01-13 11:30:59 AM] training complete\n",
      "[2023-01-13 11:31:01 AM] training....\n",
      "[2023-01-13 11:31:04 AM] training complete\n",
      "[2023-01-13 11:31:04 AM] training replay buffer....\n",
      "[2023-01-13 11:31:09 AM] training complete\n",
      "[2023-01-13 11:31:11 AM] training....\n",
      "[2023-01-13 11:31:14 AM] training complete\n",
      "[2023-01-13 11:31:14 AM] training replay buffer....\n",
      "[2023-01-13 11:31:19 AM] training complete\n",
      "[2023-01-13 11:31:20 AM] training....\n",
      "[2023-01-13 11:31:23 AM] training complete\n",
      "[2023-01-13 11:31:23 AM] training replay buffer....\n",
      "[2023-01-13 11:31:28 AM] training complete\n",
      "[2023-01-13 11:31:30 AM] training....\n",
      "[2023-01-13 11:31:33 AM] training complete\n",
      "[2023-01-13 11:31:33 AM] training replay buffer....\n",
      "[2023-01-13 11:31:38 AM] training complete\n",
      "[2023-01-13 11:31:39 AM] training....\n",
      "[2023-01-13 11:31:42 AM] training complete\n",
      "[2023-01-13 11:31:42 AM] training replay buffer....\n",
      "[2023-01-13 11:31:47 AM] training complete\n",
      "[2023-01-13 11:31:49 AM] training....\n",
      "[2023-01-13 11:31:53 AM] training complete\n",
      "[2023-01-13 11:31:53 AM] training replay buffer....\n",
      "[2023-01-13 11:31:58 AM] training complete\n",
      "[2023-01-13 11:31:59 AM] training....\n",
      "[2023-01-13 11:32:02 AM] training complete\n",
      "[2023-01-13 11:32:02 AM] training replay buffer....\n",
      "[2023-01-13 11:32:07 AM] training complete\n",
      "[2023-01-13 11:32:08 AM] training....\n",
      "[2023-01-13 11:32:12 AM] training complete\n",
      "[2023-01-13 11:32:12 AM] training replay buffer....\n",
      "[2023-01-13 11:32:17 AM] training complete\n",
      "[2023-01-13 11:32:18 AM] training....\n",
      "[2023-01-13 11:32:20 AM] training complete\n",
      "[2023-01-13 11:32:20 AM] training replay buffer....\n",
      "[2023-01-13 11:32:25 AM] training complete\n",
      "[2023-01-13 11:32:27 AM] training....\n",
      "[2023-01-13 11:32:28 AM] training complete\n",
      "[2023-01-13 11:32:28 AM] training replay buffer....\n",
      "[2023-01-13 11:32:33 AM] training complete\n",
      "[2023-01-13 11:32:35 AM] training....\n",
      "[2023-01-13 11:32:36 AM] training complete\n",
      "[2023-01-13 11:32:36 AM] training replay buffer....\n",
      "[2023-01-13 11:32:41 AM] training complete\n",
      "[2023-01-13 11:32:43 AM] training....\n",
      "[2023-01-13 11:32:46 AM] training complete\n",
      "[2023-01-13 11:32:46 AM] training replay buffer....\n",
      "[2023-01-13 11:32:50 AM] training complete\n",
      "[2023-01-13 11:32:52 AM] training....\n",
      "[2023-01-13 11:32:56 AM] training complete\n",
      "[2023-01-13 11:32:56 AM] training replay buffer....\n",
      "[2023-01-13 11:33:02 AM] training complete\n",
      "[2023-01-13 11:33:03 AM] training....\n",
      "[2023-01-13 11:33:04 AM] training complete\n",
      "[2023-01-13 11:33:04 AM] training replay buffer....\n",
      "[2023-01-13 11:33:10 AM] training complete\n",
      "[2023-01-13 11:33:11 AM] training....\n",
      "[2023-01-13 11:33:15 AM] training complete\n",
      "[2023-01-13 11:33:15 AM] training replay buffer....\n",
      "[2023-01-13 11:33:20 AM] training complete\n",
      "[2023-01-13 11:33:22 AM] training....\n",
      "[2023-01-13 11:33:23 AM] training complete\n",
      "[2023-01-13 11:33:23 AM] training replay buffer....\n",
      "[2023-01-13 11:33:29 AM] training complete\n",
      "[2023-01-13 11:33:31 AM] training....\n",
      "[2023-01-13 11:33:34 AM] training complete\n",
      "[2023-01-13 11:33:34 AM] training replay buffer....\n",
      "[2023-01-13 11:33:38 AM] training complete\n",
      "[2023-01-13 11:33:40 AM] training....\n",
      "[2023-01-13 11:33:43 AM] training complete\n",
      "[2023-01-13 11:33:43 AM] training replay buffer....\n",
      "[2023-01-13 11:33:48 AM] training complete\n",
      "[2023-01-13 11:33:49 AM] training....\n",
      "[2023-01-13 11:33:52 AM] training complete\n",
      "[2023-01-13 11:33:52 AM] training replay buffer....\n",
      "[2023-01-13 11:33:57 AM] training complete\n",
      "[2023-01-13 11:33:58 AM] training....\n",
      "[2023-01-13 11:34:01 AM] training complete\n",
      "[2023-01-13 11:34:01 AM] training replay buffer....\n",
      "[2023-01-13 11:34:06 AM] training complete\n",
      "[2023-01-13 11:34:08 AM] training....\n",
      "[2023-01-13 11:34:11 AM] training complete\n",
      "[2023-01-13 11:34:11 AM] training replay buffer....\n",
      "[2023-01-13 11:34:16 AM] training complete\n",
      "[2023-01-13 11:34:17 AM] training....\n",
      "[2023-01-13 11:34:19 AM] training complete\n",
      "[2023-01-13 11:34:19 AM] training replay buffer....\n",
      "[2023-01-13 11:34:23 AM] training complete\n",
      "[2023-01-13 11:34:25 AM] training....\n",
      "[2023-01-13 11:34:27 AM] training complete\n",
      "[2023-01-13 11:34:27 AM] training replay buffer....\n",
      "[2023-01-13 11:34:32 AM] training complete\n",
      "[2023-01-13 11:34:33 AM] training....\n",
      "[2023-01-13 11:34:36 AM] training complete\n",
      "[2023-01-13 11:34:36 AM] training replay buffer....\n",
      "[2023-01-13 11:34:41 AM] training complete\n",
      "[2023-01-13 11:34:42 AM] training....\n",
      "[2023-01-13 11:34:44 AM] training complete\n",
      "[2023-01-13 11:34:44 AM] training replay buffer....\n",
      "[2023-01-13 11:34:49 AM] training complete\n",
      "[2023-01-13 11:34:50 AM] training....\n",
      "[2023-01-13 11:34:53 AM] training complete\n",
      "[2023-01-13 11:34:53 AM] training replay buffer....\n",
      "[2023-01-13 11:34:58 AM] training complete\n",
      "[2023-01-13 11:35:00 AM] training....\n",
      "[2023-01-13 11:35:03 AM] training complete\n",
      "[2023-01-13 11:35:03 AM] training replay buffer....\n",
      "[2023-01-13 11:35:08 AM] training complete\n",
      "[2023-01-13 11:35:09 AM] training....\n",
      "[2023-01-13 11:35:10 AM] training complete\n",
      "[2023-01-13 11:35:10 AM] training replay buffer....\n",
      "[2023-01-13 11:35:15 AM] training complete\n",
      "[2023-01-13 11:35:17 AM] training....\n",
      "[2023-01-13 11:35:20 AM] training complete\n",
      "[2023-01-13 11:35:20 AM] training replay buffer....\n",
      "[2023-01-13 11:35:25 AM] training complete\n",
      "[2023-01-13 11:35:26 AM] training....\n",
      "[2023-01-13 11:35:27 AM] training complete\n",
      "[2023-01-13 11:35:27 AM] training replay buffer....\n",
      "[2023-01-13 11:35:32 AM] training complete\n",
      "[2023-01-13 11:35:33 AM] training....\n",
      "[2023-01-13 11:35:37 AM] training complete\n",
      "[2023-01-13 11:35:37 AM] training replay buffer....\n",
      "[2023-01-13 11:35:41 AM] training complete\n",
      "[2023-01-13 11:35:43 AM] training....\n",
      "[2023-01-13 11:35:46 AM] training complete\n",
      "[2023-01-13 11:35:46 AM] training replay buffer....\n",
      "[2023-01-13 11:35:51 AM] training complete\n",
      "[2023-01-13 11:35:52 AM] training....\n",
      "[2023-01-13 11:35:55 AM] training complete\n",
      "[2023-01-13 11:35:55 AM] training replay buffer....\n",
      "[2023-01-13 11:36:00 AM] training complete\n",
      "[2023-01-13 11:36:02 AM] training....\n",
      "[2023-01-13 11:36:05 AM] training complete\n",
      "[2023-01-13 11:36:05 AM] training replay buffer....\n",
      "[2023-01-13 11:36:09 AM] training complete\n",
      "[2023-01-13 11:36:11 AM] training....\n",
      "[2023-01-13 11:36:14 AM] training complete\n",
      "[2023-01-13 11:36:14 AM] training replay buffer....\n",
      "[2023-01-13 11:36:19 AM] training complete\n",
      "[2023-01-13 11:36:20 AM] training....\n",
      "[2023-01-13 11:36:23 AM] training complete\n",
      "[2023-01-13 11:36:23 AM] training replay buffer....\n",
      "[2023-01-13 11:36:28 AM] training complete\n",
      "[2023-01-13 11:36:30 AM] training....\n",
      "[2023-01-13 11:36:33 AM] training complete\n",
      "[2023-01-13 11:36:33 AM] training replay buffer....\n",
      "[2023-01-13 11:36:37 AM] training complete\n",
      "[2023-01-13 11:36:39 AM] training....\n",
      "[2023-01-13 11:36:40 AM] training complete\n",
      "[2023-01-13 11:36:40 AM] training replay buffer....\n",
      "[2023-01-13 11:36:45 AM] training complete\n",
      "[2023-01-13 11:36:46 AM] training....\n",
      "[2023-01-13 11:36:49 AM] training complete\n",
      "[2023-01-13 11:36:49 AM] training replay buffer....\n",
      "[2023-01-13 11:36:54 AM] training complete\n",
      "[2023-01-13 11:36:56 AM] training....\n",
      "[2023-01-13 11:36:57 AM] training complete\n",
      "[2023-01-13 11:36:57 AM] training replay buffer....\n",
      "[2023-01-13 11:37:02 AM] training complete\n",
      "[2023-01-13 11:37:03 AM] training....\n",
      "[2023-01-13 11:37:06 AM] training complete\n",
      "[2023-01-13 11:37:06 AM] training replay buffer....\n",
      "[2023-01-13 11:37:11 AM] training complete\n",
      "[2023-01-13 11:37:13 AM] training....\n",
      "[2023-01-13 11:37:14 AM] training complete\n",
      "[2023-01-13 11:37:14 AM] training replay buffer....\n",
      "[2023-01-13 11:37:19 AM] training complete\n",
      "[2023-01-13 11:37:20 AM] training....\n",
      "[2023-01-13 11:37:22 AM] training complete\n",
      "[2023-01-13 11:37:22 AM] training replay buffer....\n",
      "[2023-01-13 11:37:26 AM] training complete\n",
      "[2023-01-13 11:37:28 AM] training....\n",
      "[2023-01-13 11:37:29 AM] training complete\n",
      "[2023-01-13 11:37:29 AM] training replay buffer....\n",
      "[2023-01-13 11:37:34 AM] training complete\n",
      "[2023-01-13 11:37:36 AM] training....\n",
      "[2023-01-13 11:37:39 AM] training complete\n",
      "[2023-01-13 11:37:39 AM] training replay buffer....\n",
      "[2023-01-13 11:37:44 AM] training complete\n",
      "[2023-01-13 11:37:45 AM] training....\n",
      "[2023-01-13 11:37:48 AM] training complete\n",
      "[2023-01-13 11:37:48 AM] training replay buffer....\n",
      "[2023-01-13 11:37:53 AM] training complete\n",
      "[2023-01-13 11:37:54 AM] training....\n",
      "[2023-01-13 11:37:57 AM] training complete\n",
      "[2023-01-13 11:37:57 AM] training replay buffer....\n",
      "[2023-01-13 11:38:02 AM] training complete\n",
      "[2023-01-13 11:38:03 AM] training....\n",
      "[2023-01-13 11:38:06 AM] training complete\n",
      "[2023-01-13 11:38:06 AM] training replay buffer....\n",
      "[2023-01-13 11:38:11 AM] training complete\n",
      "[2023-01-13 11:38:13 AM] training....\n",
      "[2023-01-13 11:38:16 AM] training complete\n",
      "[2023-01-13 11:38:16 AM] training replay buffer....\n",
      "[2023-01-13 11:38:20 AM] training complete\n",
      "[2023-01-13 11:38:22 AM] training....\n",
      "[2023-01-13 11:38:25 AM] training complete\n",
      "[2023-01-13 11:38:25 AM] training replay buffer....\n",
      "[2023-01-13 11:38:30 AM] training complete\n",
      "[2023-01-13 11:38:31 AM] training....\n",
      "[2023-01-13 11:38:33 AM] training complete\n",
      "[2023-01-13 11:38:33 AM] training replay buffer....\n",
      "[2023-01-13 11:38:38 AM] training complete\n",
      "[2023-01-13 11:38:40 AM] training....\n",
      "[2023-01-13 11:38:43 AM] training complete\n",
      "[2023-01-13 11:38:43 AM] training replay buffer....\n",
      "[2023-01-13 11:38:47 AM] training complete\n",
      "[2023-01-13 11:38:49 AM] training....\n",
      "[2023-01-13 11:38:52 AM] training complete\n",
      "[2023-01-13 11:38:52 AM] training replay buffer....\n",
      "[2023-01-13 11:38:57 AM] training complete\n",
      "[2023-01-13 11:38:58 AM] training....\n",
      "[2023-01-13 11:39:01 AM] training complete\n",
      "[2023-01-13 11:39:01 AM] training replay buffer....\n",
      "[2023-01-13 11:39:06 AM] training complete\n",
      "[2023-01-13 11:39:07 AM] training....\n",
      "[2023-01-13 11:39:10 AM] training complete\n",
      "[2023-01-13 11:39:10 AM] training replay buffer....\n",
      "[2023-01-13 11:39:15 AM] training complete\n",
      "[2023-01-13 11:39:17 AM] training....\n",
      "[2023-01-13 11:39:20 AM] training complete\n",
      "[2023-01-13 11:39:20 AM] training replay buffer....\n",
      "[2023-01-13 11:39:25 AM] training complete\n",
      "[2023-01-13 11:39:26 AM] training....\n",
      "[2023-01-13 11:39:28 AM] training complete\n",
      "[2023-01-13 11:39:28 AM] training replay buffer....\n",
      "[2023-01-13 11:39:32 AM] training complete\n",
      "[2023-01-13 11:39:34 AM] training....\n",
      "[2023-01-13 11:39:35 AM] training complete\n",
      "[2023-01-13 11:39:35 AM] training replay buffer....\n",
      "[2023-01-13 11:39:40 AM] training complete\n",
      "[2023-01-13 11:39:41 AM] training....\n",
      "[2023-01-13 11:39:43 AM] training complete\n",
      "[2023-01-13 11:39:43 AM] training replay buffer....\n",
      "[2023-01-13 11:39:47 AM] training complete\n",
      "[2023-01-13 11:39:49 AM] training....\n",
      "[2023-01-13 11:39:52 AM] training complete\n",
      "[2023-01-13 11:39:52 AM] training replay buffer....\n",
      "[2023-01-13 11:39:57 AM] training complete\n",
      "[2023-01-13 11:39:58 AM] training....\n",
      "[2023-01-13 11:40:00 AM] training complete\n",
      "[2023-01-13 11:40:00 AM] training replay buffer....\n",
      "[2023-01-13 11:40:04 AM] training complete\n",
      "[2023-01-13 11:40:06 AM] training....\n",
      "[2023-01-13 11:40:07 AM] training complete\n",
      "[2023-01-13 11:40:07 AM] training replay buffer....\n",
      "[2023-01-13 11:40:12 AM] training complete\n",
      "[2023-01-13 11:40:13 AM] training....\n",
      "[2023-01-13 11:40:16 AM] training complete\n",
      "[2023-01-13 11:40:16 AM] training replay buffer....\n",
      "[2023-01-13 11:40:21 AM] training complete\n",
      "[2023-01-13 11:40:22 AM] training....\n",
      "[2023-01-13 11:40:24 AM] training complete\n",
      "[2023-01-13 11:40:24 AM] training replay buffer....\n",
      "[2023-01-13 11:40:29 AM] training complete\n",
      "[2023-01-13 11:40:30 AM] training....\n",
      "[2023-01-13 11:40:33 AM] training complete\n",
      "[2023-01-13 11:40:33 AM] training replay buffer....\n",
      "[2023-01-13 11:40:38 AM] training complete\n",
      "[2023-01-13 11:40:40 AM] training....\n",
      "[2023-01-13 11:40:43 AM] training complete\n",
      "[2023-01-13 11:40:43 AM] training replay buffer....\n",
      "[2023-01-13 11:40:47 AM] training complete\n",
      "[2023-01-13 11:40:49 AM] training....\n",
      "[2023-01-13 11:40:52 AM] training complete\n",
      "[2023-01-13 11:40:52 AM] training replay buffer....\n",
      "[2023-01-13 11:40:57 AM] training complete\n",
      "[2023-01-13 11:40:58 AM] training....\n",
      "[2023-01-13 11:41:00 AM] training complete\n",
      "[2023-01-13 11:41:00 AM] training replay buffer....\n",
      "[2023-01-13 11:41:04 AM] training complete\n",
      "[2023-01-13 11:41:06 AM] training....\n",
      "[2023-01-13 11:41:09 AM] training complete\n",
      "[2023-01-13 11:41:09 AM] training replay buffer....\n",
      "[2023-01-13 11:41:14 AM] training complete\n",
      "[2023-01-13 11:41:15 AM] training....\n",
      "[2023-01-13 11:41:18 AM] training complete\n",
      "[2023-01-13 11:41:18 AM] training replay buffer....\n",
      "[2023-01-13 11:41:23 AM] training complete\n",
      "[2023-01-13 11:41:25 AM] training....\n",
      "[2023-01-13 11:41:28 AM] training complete\n",
      "[2023-01-13 11:41:28 AM] training replay buffer....\n",
      "[2023-01-13 11:41:33 AM] training complete\n",
      "[2023-01-13 11:41:34 AM] training....\n",
      "[2023-01-13 11:41:37 AM] training complete\n",
      "[2023-01-13 11:41:37 AM] training replay buffer....\n",
      "[2023-01-13 11:41:42 AM] training complete\n",
      "[2023-01-13 11:41:43 AM] training....\n",
      "[2023-01-13 11:41:46 AM] training complete\n",
      "[2023-01-13 11:41:46 AM] training replay buffer....\n",
      "[2023-01-13 11:41:51 AM] training complete\n",
      "[2023-01-13 11:41:53 AM] training....\n",
      "[2023-01-13 11:41:55 AM] training complete\n",
      "[2023-01-13 11:41:55 AM] training replay buffer....\n",
      "[2023-01-13 11:42:00 AM] training complete\n",
      "[2023-01-13 11:42:01 AM] training....\n",
      "[2023-01-13 11:42:05 AM] training complete\n",
      "[2023-01-13 11:42:05 AM] training replay buffer....\n",
      "[2023-01-13 11:42:09 AM] training complete\n",
      "[2023-01-13 11:42:11 AM] training....\n",
      "[2023-01-13 11:42:12 AM] training complete\n",
      "[2023-01-13 11:42:12 AM] training replay buffer....\n",
      "[2023-01-13 11:42:17 AM] training complete\n",
      "[2023-01-13 11:42:18 AM] training....\n",
      "[2023-01-13 11:42:20 AM] training complete\n",
      "[2023-01-13 11:42:20 AM] training replay buffer....\n",
      "[2023-01-13 11:42:25 AM] training complete\n",
      "[2023-01-13 11:42:26 AM] training....\n",
      "[2023-01-13 11:42:30 AM] training complete\n",
      "[2023-01-13 11:42:30 AM] training replay buffer....\n",
      "[2023-01-13 11:42:34 AM] training complete\n",
      "[2023-01-13 11:42:35 AM] training....\n",
      "[2023-01-13 11:42:37 AM] training complete\n",
      "[2023-01-13 11:42:37 AM] training replay buffer....\n",
      "[2023-01-13 11:42:42 AM] training complete\n",
      "[2023-01-13 11:42:43 AM] training....\n",
      "[2023-01-13 11:42:46 AM] training complete\n",
      "[2023-01-13 11:42:46 AM] training replay buffer....\n",
      "[2023-01-13 11:42:51 AM] training complete\n",
      "[2023-01-13 11:42:52 AM] training....\n",
      "[2023-01-13 11:42:54 AM] training complete\n",
      "[2023-01-13 11:42:54 AM] training replay buffer....\n",
      "[2023-01-13 11:42:58 AM] training complete\n",
      "[2023-01-13 11:43:00 AM] training....\n",
      "[2023-01-13 11:43:02 AM] training complete\n",
      "[2023-01-13 11:43:02 AM] training replay buffer....\n",
      "[2023-01-13 11:43:06 AM] training complete\n",
      "[2023-01-13 11:43:08 AM] training....\n",
      "[2023-01-13 11:43:11 AM] training complete\n",
      "[2023-01-13 11:43:11 AM] training replay buffer....\n",
      "[2023-01-13 11:43:15 AM] training complete\n",
      "[2023-01-13 11:43:17 AM] training....\n",
      "[2023-01-13 11:43:20 AM] training complete\n",
      "[2023-01-13 11:43:20 AM] training replay buffer....\n",
      "[2023-01-13 11:43:25 AM] training complete\n",
      "[2023-01-13 11:43:26 AM] training....\n",
      "[2023-01-13 11:43:30 AM] training complete\n",
      "[2023-01-13 11:43:30 AM] training replay buffer....\n",
      "[2023-01-13 11:43:35 AM] training complete\n",
      "[2023-01-13 11:43:36 AM] training....\n",
      "[2023-01-13 11:43:37 AM] training complete\n",
      "[2023-01-13 11:43:37 AM] training replay buffer....\n",
      "[2023-01-13 11:43:42 AM] training complete\n",
      "[2023-01-13 11:43:44 AM] training....\n",
      "[2023-01-13 11:43:47 AM] training complete\n",
      "[2023-01-13 11:43:47 AM] training replay buffer....\n",
      "[2023-01-13 11:43:52 AM] training complete\n",
      "[2023-01-13 11:43:53 AM] training....\n",
      "[2023-01-13 11:43:56 AM] training complete\n",
      "[2023-01-13 11:43:56 AM] training replay buffer....\n",
      "[2023-01-13 11:44:03 AM] training complete\n",
      "[2023-01-13 11:44:05 AM] training....\n",
      "[2023-01-13 11:44:08 AM] training complete\n",
      "[2023-01-13 11:44:08 AM] training replay buffer....\n",
      "[2023-01-13 11:44:13 AM] training complete\n",
      "[2023-01-13 11:44:14 AM] training....\n",
      "[2023-01-13 11:44:16 AM] training complete\n",
      "[2023-01-13 11:44:16 AM] training replay buffer....\n",
      "[2023-01-13 11:44:20 AM] training complete\n",
      "[2023-01-13 11:44:22 AM] training....\n",
      "[2023-01-13 11:44:25 AM] training complete\n",
      "[2023-01-13 11:44:25 AM] training replay buffer....\n",
      "[2023-01-13 11:44:30 AM] training complete\n",
      "[2023-01-13 11:44:32 AM] training....\n",
      "[2023-01-13 11:44:33 AM] training complete\n",
      "[2023-01-13 11:44:33 AM] training replay buffer....\n",
      "[2023-01-13 11:44:38 AM] training complete\n",
      "[2023-01-13 11:44:39 AM] training....\n",
      "[2023-01-13 11:44:41 AM] training complete\n",
      "[2023-01-13 11:44:41 AM] training replay buffer....\n",
      "[2023-01-13 11:44:46 AM] training complete\n",
      "[2023-01-13 11:44:47 AM] training....\n",
      "[2023-01-13 11:44:50 AM] training complete\n",
      "[2023-01-13 11:44:50 AM] training replay buffer....\n",
      "[2023-01-13 11:44:54 AM] training complete\n",
      "[2023-01-13 11:44:56 AM] training....\n",
      "[2023-01-13 11:44:59 AM] training complete\n",
      "[2023-01-13 11:44:59 AM] training replay buffer....\n",
      "[2023-01-13 11:45:04 AM] training complete\n",
      "[2023-01-13 11:45:05 AM] training....\n",
      "[2023-01-13 11:45:07 AM] training complete\n",
      "[2023-01-13 11:45:07 AM] training replay buffer....\n",
      "[2023-01-13 11:45:12 AM] training complete\n",
      "[2023-01-13 11:45:13 AM] training....\n",
      "[2023-01-13 11:45:16 AM] training complete\n",
      "[2023-01-13 11:45:16 AM] training replay buffer....\n",
      "[2023-01-13 11:45:21 AM] training complete\n",
      "[2023-01-13 11:45:23 AM] training....\n",
      "[2023-01-13 11:45:26 AM] training complete\n",
      "[2023-01-13 11:45:26 AM] training replay buffer....\n",
      "[2023-01-13 11:45:31 AM] training complete\n",
      "[2023-01-13 11:45:32 AM] training....\n",
      "[2023-01-13 11:45:35 AM] training complete\n",
      "[2023-01-13 11:45:35 AM] training replay buffer....\n",
      "[2023-01-13 11:45:40 AM] training complete\n",
      "[2023-01-13 11:45:42 AM] training....\n",
      "[2023-01-13 11:45:43 AM] training complete\n",
      "[2023-01-13 11:45:43 AM] training replay buffer....\n",
      "[2023-01-13 11:45:48 AM] training complete\n",
      "[2023-01-13 11:45:50 AM] training....\n",
      "[2023-01-13 11:45:53 AM] training complete\n",
      "[2023-01-13 11:45:53 AM] training replay buffer....\n",
      "[2023-01-13 11:45:57 AM] training complete\n",
      "[2023-01-13 11:45:59 AM] training....\n",
      "[2023-01-13 11:46:02 AM] training complete\n",
      "[2023-01-13 11:46:02 AM] training replay buffer....\n",
      "[2023-01-13 11:46:07 AM] training complete\n",
      "[2023-01-13 11:46:08 AM] training....\n",
      "[2023-01-13 11:46:10 AM] training complete\n",
      "[2023-01-13 11:46:10 AM] training replay buffer....\n",
      "[2023-01-13 11:46:15 AM] training complete\n",
      "[2023-01-13 11:46:16 AM] training....\n",
      "[2023-01-13 11:46:20 AM] training complete\n",
      "[2023-01-13 11:46:20 AM] training replay buffer....\n",
      "[2023-01-13 11:46:24 AM] training complete\n",
      "[2023-01-13 11:46:26 AM] training....\n",
      "[2023-01-13 11:46:28 AM] training complete\n",
      "[2023-01-13 11:46:28 AM] training replay buffer....\n",
      "[2023-01-13 11:46:33 AM] training complete\n",
      "[2023-01-13 11:46:34 AM] training....\n",
      "[2023-01-13 11:46:35 AM] training complete\n",
      "[2023-01-13 11:46:35 AM] training replay buffer....\n",
      "[2023-01-13 11:46:40 AM] training complete\n",
      "[2023-01-13 11:46:41 AM] training....\n",
      "[2023-01-13 11:46:43 AM] training complete\n",
      "[2023-01-13 11:46:43 AM] training replay buffer....\n",
      "[2023-01-13 11:46:48 AM] training complete\n",
      "[2023-01-13 11:46:49 AM] training....\n",
      "[2023-01-13 11:46:51 AM] training complete\n",
      "[2023-01-13 11:46:51 AM] training replay buffer....\n",
      "[2023-01-13 11:46:56 AM] training complete\n",
      "[2023-01-13 11:46:57 AM] training....\n",
      "[2023-01-13 11:47:00 AM] training complete\n",
      "[2023-01-13 11:47:00 AM] training replay buffer....\n",
      "[2023-01-13 11:47:05 AM] training complete\n",
      "[2023-01-13 11:47:07 AM] training....\n",
      "[2023-01-13 11:47:10 AM] training complete\n",
      "[2023-01-13 11:47:10 AM] training replay buffer....\n",
      "[2023-01-13 11:47:15 AM] training complete\n",
      "[2023-01-13 11:47:16 AM] training....\n",
      "[2023-01-13 11:47:19 AM] training complete\n",
      "[2023-01-13 11:47:19 AM] training replay buffer....\n",
      "[2023-01-13 11:47:25 AM] training complete\n",
      "[2023-01-13 11:47:26 AM] training....\n",
      "[2023-01-13 11:47:29 AM] training complete\n",
      "[2023-01-13 11:47:29 AM] training replay buffer....\n",
      "[2023-01-13 11:47:34 AM] training complete\n",
      "[2023-01-13 11:47:35 AM] training....\n",
      "[2023-01-13 11:47:39 AM] training complete\n",
      "[2023-01-13 11:47:39 AM] training replay buffer....\n",
      "[2023-01-13 11:47:43 AM] training complete\n",
      "[2023-01-13 11:47:45 AM] training....\n",
      "[2023-01-13 11:47:48 AM] training complete\n",
      "[2023-01-13 11:47:48 AM] training replay buffer....\n",
      "[2023-01-13 11:47:53 AM] training complete\n",
      "[2023-01-13 11:47:54 AM] training....\n",
      "[2023-01-13 11:47:56 AM] training complete\n",
      "[2023-01-13 11:47:56 AM] training replay buffer....\n",
      "[2023-01-13 11:48:01 AM] training complete\n",
      "[2023-01-13 11:48:02 AM] training....\n",
      "[2023-01-13 11:48:03 AM] training complete\n",
      "[2023-01-13 11:48:03 AM] training replay buffer....\n",
      "[2023-01-13 11:48:08 AM] training complete\n",
      "[2023-01-13 11:48:09 AM] training....\n",
      "[2023-01-13 11:48:11 AM] training complete\n",
      "[2023-01-13 11:48:11 AM] training replay buffer....\n",
      "[2023-01-13 11:48:16 AM] training complete\n",
      "[2023-01-13 11:48:17 AM] training....\n",
      "[2023-01-13 11:48:20 AM] training complete\n",
      "[2023-01-13 11:48:20 AM] training replay buffer....\n",
      "[2023-01-13 11:48:25 AM] training complete\n",
      "[2023-01-13 11:48:26 AM] training....\n",
      "[2023-01-13 11:48:28 AM] training complete\n",
      "[2023-01-13 11:48:28 AM] training replay buffer....\n",
      "[2023-01-13 11:48:33 AM] training complete\n",
      "[2023-01-13 11:48:34 AM] training....\n",
      "[2023-01-13 11:48:36 AM] training complete\n",
      "[2023-01-13 11:48:36 AM] training replay buffer....\n",
      "[2023-01-13 11:48:41 AM] training complete\n",
      "[2023-01-13 11:48:42 AM] training....\n",
      "[2023-01-13 11:48:44 AM] training complete\n",
      "[2023-01-13 11:48:44 AM] training replay buffer....\n",
      "[2023-01-13 11:48:49 AM] training complete\n",
      "[2023-01-13 11:48:50 AM] training....\n",
      "[2023-01-13 11:48:52 AM] training complete\n",
      "[2023-01-13 11:48:52 AM] training replay buffer....\n",
      "[2023-01-13 11:48:57 AM] training complete\n",
      "[2023-01-13 11:48:58 AM] training....\n",
      "[2023-01-13 11:49:00 AM] training complete\n",
      "[2023-01-13 11:49:00 AM] training replay buffer....\n",
      "[2023-01-13 11:49:05 AM] training complete\n",
      "[2023-01-13 11:49:07 AM] training....\n",
      "[2023-01-13 11:49:08 AM] training complete\n",
      "[2023-01-13 11:49:08 AM] training replay buffer....\n",
      "[2023-01-13 11:49:13 AM] training complete\n",
      "[2023-01-13 11:49:15 AM] training....\n",
      "[2023-01-13 11:49:18 AM] training complete\n",
      "[2023-01-13 11:49:18 AM] training replay buffer....\n",
      "[2023-01-13 11:49:23 AM] training complete\n",
      "[2023-01-13 11:49:24 AM] training....\n",
      "[2023-01-13 11:49:26 AM] training complete\n",
      "[2023-01-13 11:49:26 AM] training replay buffer....\n",
      "[2023-01-13 11:49:31 AM] training complete\n",
      "[2023-01-13 11:49:32 AM] training....\n",
      "[2023-01-13 11:49:34 AM] training complete\n",
      "[2023-01-13 11:49:34 AM] training replay buffer....\n",
      "[2023-01-13 11:49:39 AM] training complete\n",
      "[2023-01-13 11:49:41 AM] training....\n",
      "[2023-01-13 11:49:44 AM] training complete\n",
      "[2023-01-13 11:49:44 AM] training replay buffer....\n",
      "[2023-01-13 11:49:49 AM] training complete\n",
      "[2023-01-13 11:49:51 AM] training....\n",
      "[2023-01-13 11:49:54 AM] training complete\n",
      "[2023-01-13 11:49:54 AM] training replay buffer....\n",
      "[2023-01-13 11:49:59 AM] training complete\n",
      "[2023-01-13 11:50:00 AM] training....\n",
      "[2023-01-13 11:50:03 AM] training complete\n",
      "[2023-01-13 11:50:03 AM] training replay buffer....\n",
      "[2023-01-13 11:50:08 AM] training complete\n",
      "[2023-01-13 11:50:09 AM] training....\n",
      "[2023-01-13 11:50:11 AM] training complete\n",
      "[2023-01-13 11:50:11 AM] training replay buffer....\n",
      "[2023-01-13 11:50:16 AM] training complete\n",
      "[2023-01-13 11:50:17 AM] training....\n",
      "[2023-01-13 11:50:19 AM] training complete\n",
      "[2023-01-13 11:50:19 AM] training replay buffer....\n",
      "[2023-01-13 11:50:24 AM] training complete\n",
      "[2023-01-13 11:50:25 AM] training....\n",
      "[2023-01-13 11:50:27 AM] training complete\n",
      "[2023-01-13 11:50:27 AM] training replay buffer....\n",
      "[2023-01-13 11:50:32 AM] training complete\n",
      "[2023-01-13 11:50:33 AM] training....\n",
      "[2023-01-13 11:50:37 AM] training complete\n",
      "[2023-01-13 11:50:37 AM] training replay buffer....\n",
      "[2023-01-13 11:50:42 AM] training complete\n",
      "[2023-01-13 11:50:43 AM] training....\n",
      "[2023-01-13 11:50:46 AM] training complete\n",
      "[2023-01-13 11:50:46 AM] training replay buffer....\n",
      "[2023-01-13 11:50:52 AM] training complete\n",
      "[2023-01-13 11:50:53 AM] training....\n",
      "[2023-01-13 11:50:56 AM] training complete\n",
      "[2023-01-13 11:50:56 AM] training replay buffer....\n",
      "[2023-01-13 11:51:01 AM] training complete\n",
      "[2023-01-13 11:51:03 AM] training....\n",
      "[2023-01-13 11:51:06 AM] training complete\n",
      "[2023-01-13 11:51:06 AM] training replay buffer....\n",
      "[2023-01-13 11:51:11 AM] training complete\n",
      "[2023-01-13 11:51:12 AM] training....\n",
      "[2023-01-13 11:51:16 AM] training complete\n",
      "[2023-01-13 11:51:16 AM] training replay buffer....\n",
      "[2023-01-13 11:51:20 AM] training complete\n",
      "[2023-01-13 11:51:22 AM] training....\n",
      "[2023-01-13 11:51:24 AM] training complete\n",
      "[2023-01-13 11:51:24 AM] training replay buffer....\n",
      "[2023-01-13 11:51:28 AM] training complete\n",
      "[2023-01-13 11:51:30 AM] training....\n",
      "[2023-01-13 11:51:33 AM] training complete\n",
      "[2023-01-13 11:51:33 AM] training replay buffer....\n",
      "[2023-01-13 11:51:38 AM] training complete\n",
      "[2023-01-13 11:51:39 AM] training....\n",
      "[2023-01-13 11:51:42 AM] training complete\n",
      "[2023-01-13 11:51:42 AM] training replay buffer....\n",
      "[2023-01-13 11:51:47 AM] training complete\n",
      "[2023-01-13 11:51:48 AM] training....\n",
      "[2023-01-13 11:51:50 AM] training complete\n",
      "[2023-01-13 11:51:50 AM] training replay buffer....\n",
      "[2023-01-13 11:51:55 AM] training complete\n",
      "[2023-01-13 11:51:56 AM] training....\n",
      "[2023-01-13 11:51:57 AM] training complete\n",
      "[2023-01-13 11:51:57 AM] training replay buffer....\n",
      "[2023-01-13 11:52:02 AM] training complete\n",
      "[2023-01-13 11:52:04 AM] training....\n",
      "[2023-01-13 11:52:05 AM] training complete\n",
      "[2023-01-13 11:52:05 AM] training replay buffer....\n",
      "[2023-01-13 11:52:10 AM] training complete\n",
      "[2023-01-13 11:52:12 AM] training....\n",
      "[2023-01-13 11:52:15 AM] training complete\n",
      "[2023-01-13 11:52:15 AM] training replay buffer....\n",
      "[2023-01-13 11:52:20 AM] training complete\n",
      "[2023-01-13 11:52:21 AM] training....\n",
      "[2023-01-13 11:52:23 AM] training complete\n",
      "[2023-01-13 11:52:23 AM] training replay buffer....\n",
      "[2023-01-13 11:52:27 AM] training complete\n",
      "[2023-01-13 11:52:29 AM] training....\n",
      "[2023-01-13 11:52:30 AM] training complete\n",
      "[2023-01-13 11:52:30 AM] training replay buffer....\n",
      "[2023-01-13 11:52:35 AM] training complete\n",
      "[2023-01-13 11:52:37 AM] training....\n",
      "[2023-01-13 11:52:38 AM] training complete\n",
      "[2023-01-13 11:52:38 AM] training replay buffer....\n",
      "[2023-01-13 11:52:43 AM] training complete\n",
      "[2023-01-13 11:52:44 AM] training....\n",
      "[2023-01-13 11:52:46 AM] training complete\n",
      "[2023-01-13 11:52:46 AM] training replay buffer....\n",
      "[2023-01-13 11:52:51 AM] training complete\n",
      "[2023-01-13 11:52:52 AM] training....\n",
      "[2023-01-13 11:52:54 AM] training complete\n",
      "[2023-01-13 11:52:54 AM] training replay buffer....\n",
      "[2023-01-13 11:52:58 AM] training complete\n",
      "[2023-01-13 11:53:00 AM] training....\n",
      "[2023-01-13 11:53:01 AM] training complete\n",
      "[2023-01-13 11:53:01 AM] training replay buffer....\n",
      "[2023-01-13 11:53:06 AM] training complete\n",
      "[2023-01-13 11:53:08 AM] training....\n",
      "[2023-01-13 11:53:09 AM] training complete\n",
      "[2023-01-13 11:53:09 AM] training replay buffer....\n",
      "[2023-01-13 11:53:14 AM] training complete\n",
      "[2023-01-13 11:53:15 AM] training....\n",
      "[2023-01-13 11:53:17 AM] training complete\n",
      "[2023-01-13 11:53:17 AM] training replay buffer....\n",
      "[2023-01-13 11:53:22 AM] training complete\n",
      "[2023-01-13 11:53:23 AM] training....\n",
      "[2023-01-13 11:53:25 AM] training complete\n",
      "[2023-01-13 11:53:25 AM] training replay buffer....\n",
      "[2023-01-13 11:53:30 AM] training complete\n",
      "[2023-01-13 11:53:31 AM] training....\n",
      "[2023-01-13 11:53:33 AM] training complete\n",
      "[2023-01-13 11:53:33 AM] training replay buffer....\n",
      "[2023-01-13 11:53:37 AM] training complete\n",
      "[2023-01-13 11:53:39 AM] training....\n",
      "[2023-01-13 11:53:42 AM] training complete\n",
      "[2023-01-13 11:53:42 AM] training replay buffer....\n",
      "[2023-01-13 11:53:47 AM] training complete\n",
      "[2023-01-13 11:53:49 AM] training....\n",
      "[2023-01-13 11:53:52 AM] training complete\n",
      "[2023-01-13 11:53:52 AM] training replay buffer....\n",
      "[2023-01-13 11:53:57 AM] training complete\n",
      "[2023-01-13 11:53:58 AM] training....\n",
      "[2023-01-13 11:54:01 AM] training complete\n",
      "[2023-01-13 11:54:01 AM] training replay buffer....\n",
      "[2023-01-13 11:54:06 AM] training complete\n",
      "[2023-01-13 11:54:07 AM] training....\n",
      "[2023-01-13 11:54:09 AM] training complete\n",
      "[2023-01-13 11:54:09 AM] training replay buffer....\n",
      "[2023-01-13 11:54:14 AM] training complete\n",
      "[2023-01-13 11:54:15 AM] training....\n",
      "[2023-01-13 11:54:19 AM] training complete\n",
      "[2023-01-13 11:54:19 AM] training replay buffer....\n",
      "[2023-01-13 11:54:24 AM] training complete\n",
      "[2023-01-13 11:54:25 AM] training....\n",
      "[2023-01-13 11:54:27 AM] training complete\n",
      "[2023-01-13 11:54:27 AM] training replay buffer....\n",
      "[2023-01-13 11:54:32 AM] training complete\n",
      "[2023-01-13 11:54:33 AM] training....\n",
      "[2023-01-13 11:54:36 AM] training complete\n",
      "[2023-01-13 11:54:36 AM] training replay buffer....\n",
      "[2023-01-13 11:54:41 AM] training complete\n",
      "[2023-01-13 11:54:42 AM] training....\n",
      "[2023-01-13 11:54:45 AM] training complete\n",
      "[2023-01-13 11:54:45 AM] training replay buffer....\n",
      "[2023-01-13 11:54:50 AM] training complete\n",
      "[2023-01-13 11:54:52 AM] training....\n",
      "[2023-01-13 11:54:53 AM] training complete\n",
      "[2023-01-13 11:54:53 AM] training replay buffer....\n",
      "[2023-01-13 11:54:58 AM] training complete\n",
      "[2023-01-13 11:54:59 AM] training....\n",
      "[2023-01-13 11:55:01 AM] training complete\n",
      "[2023-01-13 11:55:01 AM] training replay buffer....\n",
      "[2023-01-13 11:55:06 AM] training complete\n",
      "[2023-01-13 11:55:07 AM] training....\n",
      "[2023-01-13 11:55:10 AM] training complete\n",
      "[2023-01-13 11:55:10 AM] training replay buffer....\n",
      "[2023-01-13 11:55:15 AM] training complete\n",
      "[2023-01-13 11:55:16 AM] training....\n",
      "[2023-01-13 11:55:18 AM] training complete\n",
      "[2023-01-13 11:55:18 AM] training replay buffer....\n",
      "[2023-01-13 11:55:23 AM] training complete\n",
      "[2023-01-13 11:55:24 AM] training....\n",
      "[2023-01-13 11:55:26 AM] training complete\n",
      "[2023-01-13 11:55:26 AM] training replay buffer....\n",
      "[2023-01-13 11:55:30 AM] training complete\n",
      "[2023-01-13 11:55:32 AM] training....\n",
      "[2023-01-13 11:55:33 AM] training complete\n",
      "[2023-01-13 11:55:33 AM] training replay buffer....\n",
      "[2023-01-13 11:55:38 AM] training complete\n",
      "[2023-01-13 11:55:39 AM] training....\n",
      "[2023-01-13 11:55:41 AM] training complete\n",
      "[2023-01-13 11:55:41 AM] training replay buffer....\n",
      "[2023-01-13 11:55:46 AM] training complete\n",
      "[2023-01-13 11:55:47 AM] training....\n",
      "[2023-01-13 11:55:49 AM] training complete\n",
      "[2023-01-13 11:55:49 AM] training replay buffer....\n",
      "[2023-01-13 11:55:54 AM] training complete\n",
      "[2023-01-13 11:55:55 AM] training....\n",
      "[2023-01-13 11:55:57 AM] training complete\n",
      "[2023-01-13 11:55:57 AM] training replay buffer....\n",
      "[2023-01-13 11:56:02 AM] training complete\n",
      "[2023-01-13 11:56:03 AM] training....\n",
      "[2023-01-13 11:56:05 AM] training complete\n",
      "[2023-01-13 11:56:05 AM] training replay buffer....\n",
      "[2023-01-13 11:56:10 AM] training complete\n",
      "[2023-01-13 11:56:11 AM] training....\n",
      "[2023-01-13 11:56:13 AM] training complete\n",
      "[2023-01-13 11:56:13 AM] training replay buffer....\n",
      "[2023-01-13 11:56:18 AM] training complete\n",
      "[2023-01-13 11:56:19 AM] training....\n",
      "[2023-01-13 11:56:21 AM] training complete\n",
      "[2023-01-13 11:56:21 AM] training replay buffer....\n",
      "[2023-01-13 11:56:26 AM] training complete\n",
      "[2023-01-13 11:56:27 AM] training....\n",
      "[2023-01-13 11:56:29 AM] training complete\n",
      "[2023-01-13 11:56:29 AM] training replay buffer....\n",
      "[2023-01-13 11:56:34 AM] training complete\n",
      "[2023-01-13 11:56:35 AM] training....\n",
      "[2023-01-13 11:56:38 AM] training complete\n",
      "[2023-01-13 11:56:38 AM] training replay buffer....\n",
      "[2023-01-13 11:56:43 AM] training complete\n",
      "[2023-01-13 11:56:45 AM] training....\n",
      "[2023-01-13 11:56:48 AM] training complete\n",
      "[2023-01-13 11:56:48 AM] training replay buffer....\n",
      "[2023-01-13 11:56:53 AM] training complete\n",
      "[2023-01-13 11:56:54 AM] training....\n",
      "[2023-01-13 11:56:56 AM] training complete\n",
      "[2023-01-13 11:56:56 AM] training replay buffer....\n",
      "[2023-01-13 11:57:01 AM] training complete\n",
      "[2023-01-13 11:57:02 AM] training....\n",
      "[2023-01-13 11:57:04 AM] training complete\n",
      "[2023-01-13 11:57:04 AM] training replay buffer....\n",
      "[2023-01-13 11:57:09 AM] training complete\n",
      "[2023-01-13 11:57:10 AM] training....\n",
      "[2023-01-13 11:57:14 AM] training complete\n",
      "[2023-01-13 11:57:14 AM] training replay buffer....\n",
      "[2023-01-13 11:57:18 AM] training complete\n",
      "[2023-01-13 11:57:20 AM] training....\n",
      "[2023-01-13 11:57:21 AM] training complete\n",
      "[2023-01-13 11:57:21 AM] training replay buffer....\n",
      "[2023-01-13 11:57:26 AM] training complete\n",
      "[2023-01-13 11:57:28 AM] training....\n",
      "[2023-01-13 11:57:31 AM] training complete\n",
      "[2023-01-13 11:57:31 AM] training replay buffer....\n",
      "[2023-01-13 11:57:36 AM] training complete\n",
      "[2023-01-13 11:57:37 AM] training....\n",
      "[2023-01-13 11:57:40 AM] training complete\n",
      "[2023-01-13 11:57:40 AM] training replay buffer....\n",
      "[2023-01-13 11:57:45 AM] training complete\n",
      "[2023-01-13 11:57:46 AM] training....\n",
      "[2023-01-13 11:57:48 AM] training complete\n",
      "[2023-01-13 11:57:48 AM] training replay buffer....\n",
      "[2023-01-13 11:57:53 AM] training complete\n",
      "[2023-01-13 11:57:54 AM] training....\n",
      "[2023-01-13 11:57:58 AM] training complete\n",
      "[2023-01-13 11:57:58 AM] training replay buffer....\n",
      "[2023-01-13 11:58:02 AM] training complete\n",
      "[2023-01-13 11:58:03 AM] training....\n",
      "[2023-01-13 11:58:05 AM] training complete\n",
      "[2023-01-13 11:58:05 AM] training replay buffer....\n",
      "[2023-01-13 11:58:10 AM] training complete\n",
      "[2023-01-13 11:58:11 AM] training....\n",
      "[2023-01-13 11:58:14 AM] training complete\n",
      "[2023-01-13 11:58:14 AM] training replay buffer....\n",
      "[2023-01-13 11:58:19 AM] training complete\n",
      "[2023-01-13 11:58:21 AM] training....\n",
      "[2023-01-13 11:58:22 AM] training complete\n",
      "[2023-01-13 11:58:22 AM] training replay buffer....\n",
      "[2023-01-13 11:58:27 AM] training complete\n",
      "[2023-01-13 11:58:28 AM] training....\n",
      "[2023-01-13 11:58:30 AM] training complete\n",
      "[2023-01-13 11:58:30 AM] training replay buffer....\n",
      "[2023-01-13 11:58:35 AM] training complete\n",
      "[2023-01-13 11:58:36 AM] training....\n",
      "[2023-01-13 11:58:38 AM] training complete\n",
      "[2023-01-13 11:58:38 AM] training replay buffer....\n",
      "[2023-01-13 11:58:43 AM] training complete\n",
      "[2023-01-13 11:58:44 AM] training....\n",
      "[2023-01-13 11:58:46 AM] training complete\n",
      "[2023-01-13 11:58:46 AM] training replay buffer....\n",
      "[2023-01-13 11:58:50 AM] training complete\n",
      "[2023-01-13 11:58:52 AM] training....\n",
      "[2023-01-13 11:58:53 AM] training complete\n",
      "[2023-01-13 11:58:53 AM] training replay buffer....\n",
      "[2023-01-13 11:58:58 AM] training complete\n",
      "[2023-01-13 11:59:00 AM] training....\n",
      "[2023-01-13 11:59:03 AM] training complete\n",
      "[2023-01-13 11:59:03 AM] training replay buffer....\n",
      "[2023-01-13 11:59:08 AM] training complete\n",
      "[2023-01-13 11:59:09 AM] training....\n",
      "[2023-01-13 11:59:11 AM] training complete\n",
      "[2023-01-13 11:59:11 AM] training replay buffer....\n",
      "[2023-01-13 11:59:16 AM] training complete\n",
      "[2023-01-13 11:59:17 AM] training....\n",
      "[2023-01-13 11:59:20 AM] training complete\n",
      "[2023-01-13 11:59:20 AM] training replay buffer....\n",
      "[2023-01-13 11:59:25 AM] training complete\n",
      "[2023-01-13 11:59:26 AM] training....\n",
      "[2023-01-13 11:59:30 AM] training complete\n",
      "[2023-01-13 11:59:30 AM] training replay buffer....\n",
      "[2023-01-13 11:59:35 AM] training complete\n",
      "[2023-01-13 11:59:36 AM] training....\n",
      "[2023-01-13 11:59:37 AM] training complete\n",
      "[2023-01-13 11:59:37 AM] training replay buffer....\n",
      "[2023-01-13 11:59:42 AM] training complete\n",
      "[2023-01-13 11:59:43 AM] training....\n",
      "[2023-01-13 11:59:47 AM] training complete\n",
      "[2023-01-13 11:59:47 AM] training replay buffer....\n",
      "[2023-01-13 11:59:51 AM] training complete\n",
      "[2023-01-13 11:59:53 AM] training....\n",
      "[2023-01-13 11:59:55 AM] training complete\n",
      "[2023-01-13 11:59:55 AM] training replay buffer....\n",
      "[2023-01-13 11:59:59 AM] training complete\n",
      "[2023-01-13 12:00:01 PM] training....\n",
      "[2023-01-13 12:00:02 PM] training complete\n",
      "[2023-01-13 12:00:02 PM] training replay buffer....\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global adjacency, entropy_weight, learning_rate\n",
    "    model = actor_network()\n",
    "    # model.load_state_dict(torch.load(\"./history30/model.pth\")) # -1 , +1\n",
    "    # model.load_state_dict(torch.load(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/stacking/node10/model6/history3/model.pth\")) # original\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "\n",
    "    adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    # node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    # node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    # link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    # job_waiting_state = env.get_job_waiting_vector()\n",
    "\n",
    "    # network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "\n",
    "    # for each time step\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = []\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(nodeNum)]\n",
    "    void_selected_num = 0\n",
    "\n",
    "    pre_state_1 = {}\n",
    "    pre_state_2 = {}\n",
    "    \n",
    "    while True:\n",
    "        model = model.to('cpu')\n",
    "\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state_1 = getOmnetMessage()\n",
    "            state_2 = getOmnetMessage()\n",
    "\n",
    "            if len(state_1) == 0:\n",
    "                state_1 = state_2\n",
    "            \n",
    "\n",
    "            if len(pre_state_1) == 0: # action 시작\n",
    "                state_1 = json.loads(state_1) # state 받았으므로 action 하면됨.\n",
    "                state_2 = json.loads(state_2) # state 받았으므로 action 하면됨.\n",
    "                \n",
    "            else:\n",
    "                state_1 = json.loads(state_1)\n",
    "                pre_state_1['jobWaiting'] = state_1['jobWaiting']\n",
    "                pre_state_1['sojournTime'] = state_1['sojournTime']\n",
    "                state_1 = pre_state_1\n",
    "\n",
    "                state_2 = json.loads(state_2)\n",
    "                pre_state_2['jobWaiting'] = state_2['jobWaiting']\n",
    "                pre_state_2['sojournTime'] = state_2['sojournTime']\n",
    "                state_2 = pre_state_2\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            node_waiting_state_1 = np.array(eval(str(state_1['nodeState'])))\n",
    "            node_processing_state_1 = np.array(eval(state_1['nodeProcessing']))\n",
    "            link_state_1 = np.array(eval(state_1['linkWaiting']))\n",
    "            job_waiting_state_1 = np.array(eval(state_1['jobWaiting']))\n",
    "            activated_job_list_1 = eval(state_1['activatedJobList'])\n",
    "            isAction_1 = int(state_1['isAction'])\n",
    "            reward_1 = float(state_1['reward'])\n",
    "            averageLatency_1 = float(state_1['averageLatency'])\n",
    "            completeJobNum_1 = int(state_1['completeJobNum'])\n",
    "            sojournTime_1 = float(state_1['sojournTime'])\n",
    "\n",
    "            node_waiting_state_2 = np.array(eval(str(state_2['nodeState'])))\n",
    "            node_processing_state_2 = np.array(eval(state_2['nodeProcessing']))\n",
    "            link_state_2 = np.array(eval(state_2['linkWaiting']))\n",
    "            job_waiting_state_2 = np.array(eval(state_2['jobWaiting']))\n",
    "            activated_job_list_2 = eval(state_2['activatedJobList'])\n",
    "            isAction_2 = int(state_2['isAction'])\n",
    "            reward_2 = float(state_2['reward'])\n",
    "            averageLatency_2 = float(state_2['averageLatency'])\n",
    "            completeJobNum_2 = int(state_2['completeJobNum'])\n",
    "            sojournTime_2 = float(state_2['sojournTime'])\n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum_2 ,step)\n",
    "            \n",
    "\n",
    "            # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            if averageLatency_2 == -1:\n",
    "                reward_2 = 0\n",
    "            else:\n",
    "                reward_2 = completeJobNum_2\n",
    "            \n",
    "            writer.add_scalar(\"Reward/train\", reward_2, step)\n",
    "                \n",
    "            first_sample = True\n",
    "            \n",
    "\n",
    "            model.history = model.history[-replay_buffer_size:]\n",
    "\n",
    "            ################ s' 받아와서 저장 #################\n",
    "            if is_train and len(pre_state_1) == 0:\n",
    "                for history in temp_history:\n",
    "                    history[3] = reward_2\n",
    "                    history[4] = network_state\n",
    "                    history[5] = job_waiting_state\n",
    "                    model.history.append(history)\n",
    "                    model.put_data(history)\n",
    "            ###################################################\n",
    "\n",
    "            temp_history = []\n",
    "\n",
    "            job_index = int(state_2['jobIndex'])\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "\n",
    "            node_state_1 = np.concatenate((node_waiting_state_1,node_processing_state_1) ,axis = 1)\n",
    "            node_state_1 = torch.tensor(node_state_1, dtype=torch.float)\n",
    "\n",
    "            node_state_2 = np.concatenate((node_waiting_state_2,node_processing_state_2) ,axis = 1)\n",
    "            node_state_2 = torch.tensor(node_state_2, dtype=torch.float)\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state_1 = torch.tensor(link_state_1, dtype=torch.float)\n",
    "            link_state_2 = torch.tensor(link_state_2, dtype=torch.float)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = collections.deque()\n",
    "            for job in job_waiting_state_2:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state_1 = torch.tensor(job_waiting_state_1, dtype=torch.float).view(1, -1)\n",
    "            job_waiting_state_2 = torch.tensor(job_waiting_state_2, dtype=torch.float).view(1, -1)\n",
    "            # print(job_waiting_state)\n",
    "\n",
    "            network_state_1 = Data(x=node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "            network_state_2 = Data(x=node_state_2, edge_attr=link_state_2, edge_index=adjacency)\n",
    "\n",
    "            network_state = [network_state_1, network_state_2]\n",
    "            job_waiting_state = [job_waiting_state_1, job_waiting_state_2]\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            pre_state_1 = state_1\n",
    "            pre_state_2 = state_2\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward_2\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward_2 - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(nodeNum):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(nodeNum)] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward_2 != 0:\n",
    "                    with torch.no_grad():\n",
    "                        state = model.gnn([network_state, job_waiting_state])\n",
    "                        writer.add_scalar(\"Value/train\", torch.mean(model.v(state)), step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction_2 = False\n",
    "                \n",
    "\n",
    "            if isAction_2:\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = 1\n",
    "                for i in range(nodeNum):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[nodeNum:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    feature = model.gnn([network_state, job_waiting_state])\n",
    "                    new_feature = feature.repeat(modelNum, 1)\n",
    "                    prob, entropy, output = model.pi([new_feature, (torch.zeros(modelNum, 100), torch.zeros(modelNum, 100))])\n",
    "\n",
    "                writer.add_scalar(\"Entropy/train\", entropy[0], step)\n",
    "                #print(f'prob : {prob}')\n",
    "\n",
    "                # isVoid = F.sigmoid(dists[modelNum].sample())\n",
    "\n",
    "                m = Categorical(prob) \n",
    "                nodes = m.sample()\n",
    "\n",
    "                node = nodes[0].item()\n",
    "\n",
    "                #print(f'node : {node}')\n",
    "                \n",
    "                # void action 실험용\n",
    "                # node = nodeNum \n",
    "                \n",
    "                \n",
    "                # void action 뽑으면 void만 업데이트\n",
    "                if node == nodeNum and not scheduling_start: \n",
    "                    prob[0] = torch.Tensor([0] * nodeNum + [1.0])\n",
    "                    action_mask = [int(not scheduling_start) if i == node else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "\n",
    "                    temp_history.append([\n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]], \n",
    "                        node, 0, \n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]],\n",
    "                        prob[0][node].item(), \n",
    "                        0, action_mask, 0]\n",
    "                    )\n",
    "\n",
    "                    sendOmnetMessage(\"void\")\n",
    "\n",
    "                    #print(\"action finish.\")\n",
    "                    \n",
    "                    if getOmnetMessage() == \"ok\":\n",
    "                        void_selected_num += 1\n",
    "\n",
    "                else:\n",
    "                    scheduling_start = True\n",
    "\n",
    "                if scheduling_start:\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        feature = model.gnn([network_state, job_waiting_state])\n",
    "                        new_feature = feature.repeat(modelNum, 1)\n",
    "                        prob, entropy, output = model.pi([new_feature, (torch.zeros(modelNum, 100), torch.zeros(modelNum, 100))])\n",
    "\n",
    "                    output = output[:, 0:-1]\n",
    "\n",
    "                    prob = F.softmax(output, dim=0)\n",
    "                    prob = torch.concat([prob, torch.zeros(modelNum, 1)], dim =1)\n",
    "\n",
    "                    m = Categorical(prob)\n",
    "                    nodes = m.sample()\n",
    "                    action_mask = [1] * nodeNum + [0]\n",
    "\n",
    "                    for index in range(modelNum):\n",
    "\n",
    "                        node = nodes[index].item()\n",
    "\n",
    "                        temp_history.append([\n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]], \n",
    "                        node, -1, \n",
    "                        [-1, -1], \n",
    "                        [-1, -1],\n",
    "                        prob[index][node].item(), \n",
    "                        0, action_mask, index]\n",
    "                        )\n",
    "\n",
    "                        offloading_vector.append(node)\n",
    "                        node_selected_num[node] += 1\n",
    "                    \n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    \n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            pre_state_1 = {}\n",
    "            pre_state_2 = {}\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "            average_latency = float(episodic_reward['averageLatency'])\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            writer.add_scalar(\"CompleteNum/train\", complete_num, episode)\n",
    "            writer.add_scalar(\"averageLatency/train\", average_latency ,episode)\n",
    "\n",
    "            episode += 1\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            # learning_rate *= 0.995\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = pathName + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            average_reward = 0\n",
    "            average_reward_num = 0\n",
    "\n",
    "            # entropy_weight *= 0.99\n",
    "\n",
    "\n",
    "            # if len(model.data) > 0 and step % train_cycle == train_quitient:\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training....\")\n",
    "            #     model.train_net()\n",
    "            #     modelPathName = pathName + \"/model.pth\"\n",
    "            #     torch.save(model.state_dict(), modelPathName)\n",
    "            #     writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            #     average_reward = 0\n",
    "            #     average_reward_num = 0\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training complete\")\n",
    "            if is_train:\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training replay buffer....\")\n",
    "                model.train_net_history()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    modelPathName = pathName + f\"/model_{episode}.pth\"\n",
    "                \n",
    "                modelPathName = pathName + \"/model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 6, 11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 3D tensor with shape (128, 6, 11)\n",
    "x = torch.randn(128, 6, 11)\n",
    "\n",
    "# Indices of the elements you want to select (128,1) shape tensor\n",
    "indices = torch.randint(6, (128, 1), dtype=torch.long)\n",
    "\n",
    "# Select the elements from the second dimension (index 1)\n",
    "y = torch.index_select(x, 0, indices.squeeze())\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "tensor([-0.3581, -1.5859,  0.0890, -3.1543,  0.0245, -0.2118, -0.8021,  1.1990,\n",
      "         0.3272, -0.2907, -1.4774])\n",
      "tensor([-0.3581, -1.5859,  0.0890, -3.1543,  0.0245, -0.2118, -0.8021,  1.1990,\n",
      "         0.3272, -0.2907, -1.4774])\n",
      "torch.Size([128, 11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 3D tensor with shape (6, 128, 11)\n",
    "x = torch.randn(6, 128, 11)\n",
    "\n",
    "# Indices of the elements you want to select\n",
    "indices = torch.randint(6, (128, 1), dtype=torch.long).squeeze()\n",
    "\n",
    "print(indices.shape)\n",
    "print(x[indices[0]][0])\n",
    "\n",
    "\n",
    "# Select the elements from the first dimension (index 0)\n",
    "y = x[indices, torch.arange(128) , :]\n",
    "print(y[0])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2]],\n",
      "\n",
      "        [[3, 4]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = a.view(2, 1, 2)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2041, -0.1448,  0.3096],\n",
      "         [-1.0033,  0.2297,  2.5208]],\n",
      "\n",
      "        [[-0.0738, -0.6473, -0.5743],\n",
      "         [-0.6851, -1.8829,  1.1289]],\n",
      "\n",
      "        [[-0.4657,  0.1585,  0.2159],\n",
      "         [ 1.4043,  1.0900,  0.3072]],\n",
      "\n",
      "        [[ 0.0263, -1.9912,  0.6978],\n",
      "         [ 0.7387,  1.7765, -0.0661]],\n",
      "\n",
      "        [[-2.5051,  0.1290, -0.9326],\n",
      "         [ 0.6424, -0.7379, -1.7456]],\n",
      "\n",
      "        [[-1.9733,  1.2550,  0.0999],\n",
      "         [-0.6841,  0.0947, -0.3695]]])\n",
      "tensor([[ 0.2041, -0.1448,  0.3096],\n",
      "        [-1.0033,  0.2297,  2.5208]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(6, 2, 3)\n",
    "print(a)\n",
    "\n",
    "indices = torch.tensor([0], dtype=torch.long)\n",
    "indexed_outputs = torch.index_select(a, 0, indices).squeeze(0)\n",
    "print(indexed_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[2, 2],\n",
      "        [2, 2]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[0.2, 0.2, 0.6],[0.1, 0.1, 0.8]], [[0.2, 0.2, 0.6],[0.1, 0.1, 0.8]]])\n",
    "print(a.shape)\n",
    "\n",
    "m = Categorical(a).sample()\n",
    "print(m)\n",
    "print(m.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('omnetTest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12c9d4573c12dd45eabff63c44badb6fcd2b70b85de11a1a1b2c23254cbf5db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
