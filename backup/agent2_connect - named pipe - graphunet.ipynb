{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from library.ipynb\n",
      "importing Jupyter notebook from job.ipynb\n",
      "importing Jupyter notebook from jobqueue.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from dataplane.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history102\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "\n",
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet, TopKPooling, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sys import getsizeof\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent\")\n",
    "\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(pathName)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "modelNum = 0\n",
    "availableJobNum = 0\n",
    "nodeNum = 0\n",
    "jobWaitingLength = 0\n",
    "adjacency = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda:0\n",
      "<class 'torch.device'>\n",
      "<built-in method cuda of Tensor object at 0x00000230297D2FC0>\n",
      "<class 'torch.device'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.get_device_name(device = 0))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "\n",
    "print(type(device))\n",
    "\n",
    "kkkkk = torch.Tensor(1)\n",
    "\n",
    "kkkkk = kkkkk.cuda()\n",
    "\n",
    "print(kkkkk.cuda)\n",
    "print(type(kkkkk.device))\n",
    "print(kkkkk.device == device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.0003\n",
    "gamma           = 0.99\n",
    "entropy_weight  = 0.003\n",
    "val_loss_coef = 1.0\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.9\n",
    "eps_clip      = 0.2\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100\n",
    "hidden_feature_num = 0\n",
    "\n",
    "job_generate_rate = 0.003\n",
    "\n",
    "train_cycle = 300\n",
    "is_train = True\n",
    "train_quitient = 0\n",
    "\n",
    "if not is_train:\n",
    "    train_quitient = train_cycle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os.path\n",
    "# import os\n",
    "# FIFO_FILENAME = './fifo-test'\n",
    "\n",
    "# if not os.path.exists(FIFO_FILENAME):\n",
    "#     os.mkfifo(FIFO_FILENAME)\n",
    "#     if os.path.exists(FIFO_FILENAME):\n",
    "#         fp_fifo = open(FIFO_FILENAME, \"rw\")\n",
    "\n",
    "# for i in range(128):\n",
    "#     fp_fifo.write(\"Hello,MakeCode\\n\")\n",
    "#     fp_fifo.write(\"\")\n",
    "\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\omnetPipe\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 초기화 완료\n",
      "노드 개수 : 7\n",
      "네트워크 최대 job 개수 : 20\n",
      "job 대기 가능 개수 : 15\n",
      "최대 subtask 개수 : 5\n",
      "인접 리스트 : [[0, 3, 1, 3, 1, 4, 1, 5, 2, 3, 2, 4, 2, 5, 3, 4, 3, 6, 4, 5, 4, 6, 5, 6], [3, 0, 3, 1, 4, 1, 5, 1, 3, 2, 4, 2, 5, 2, 4, 3, 6, 3, 5, 4, 6, 4, 6, 5]]\n",
      "node_feature_num : 200\n",
      "queue_feature_num : 180\n",
      "episode_length : 20\n"
     ]
    }
   ],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "\n",
    "sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "print(f\"노드 개수 : {nodeNum}\")\n",
    "print(f\"네트워크 최대 job 개수 : {availableJobNum}\")\n",
    "print(f\"job 대기 가능 개수 : {jobWaitingLength}\")\n",
    "print(f\"최대 subtask 개수 : {modelNum}\")\n",
    "print(f\"인접 리스트 : {adjacency}\")\n",
    "print(f\"node_feature_num : {node_feature_num}\")\n",
    "print(f\"queue_feature_num : {queue_feature_num}\")\n",
    "print(f\"episode_length : {episode_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.x_mean = 0\n",
    "        self.x2_mean = 0\n",
    "        self.sd = 0\n",
    "        self.reward_sample_num = 0\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        \n",
    "        # print(f\"self.adjacency : \", self.adjacency.shape)\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_ecc1 = NNConv(node_feature_num, node_feature_num, self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_mlp2 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_ecc2 = NNConv(node_feature_num, node_feature_num, self.pi_mlp2, aggr='mean')\n",
    "\n",
    "        self.pi_mlp3 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_ecc3 = NNConv(node_feature_num, node_feature_num, self.pi_mlp2, aggr='mean')\n",
    "\n",
    "        self.pi_graph_u_net1 = GraphUNet(node_feature_num, 50, node_feature_num, 3, 0.8)\n",
    "        self.pi_graph_u_net2 = GraphUNet(node_feature_num, 30, node_feature_num, 4, 0.8)\n",
    "        self.pi_graph_u_net3 = GraphUNet(node_feature_num, 10, node_feature_num, 5, 0.8)\n",
    "\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear(node_feature_num + queue_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(hidden_feature_num, nodeNum + 1) # nodeNum + voidAction\n",
    "\n",
    "        self.v_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.v_s_ecc = NNConv(node_feature_num, node_feature_num, self.v_mlp1, aggr='mean')\n",
    "\n",
    "        self.v_graph_u_net = GraphUNet(node_feature_num, 10, node_feature_num, 3, 0.8)\n",
    "\n",
    "        self.v_backbone = nn.Sequential(\n",
    "            nn.Linear(node_feature_num + queue_feature_num, hidden_feature_num),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(hidden_feature_num, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "        data, job_waiting_feature = state # data = graph data\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        if job_waiting_feature.device == 'cuda':\n",
    "            link_feature = link_feature.cuda()\n",
    "            adjacency = adjacency.cuda()\n",
    "            \n",
    "        \"\"\"\n",
    "        node_feature = F.relu(self.conv1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.conv2(node_feature, adjacency, link_feature))\n",
    "        #node_feature = F.relu(self.conv3(node_feature, adjacency, link_feature))\n",
    "        readout = global_mean_pool(node_feature, data.batch) # 모든 노드의 feature를 평균내서 하나의 벡터로 만들어주기.\n",
    "        \"\"\"\n",
    "\n",
    "        #node_feature = F.relu(self.pi_s_gcn(node_feature, adjacency, link_feature))\n",
    "\n",
    "        node_feature = F.relu(self.pi_s_ecc1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.pi_graph_u_net1(node_feature, adjacency))\n",
    "        node_feature = F.relu(self.pi_s_ecc2(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.pi_graph_u_net2(node_feature, adjacency))\n",
    "        node_feature = F.relu(self.pi_s_ecc3(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.pi_graph_u_net3(node_feature, adjacency))\n",
    "\n",
    "        \n",
    "        readout = global_mean_pool(node_feature, data.batch)\n",
    "\n",
    "        #print(\"readout\", readout.shape)\n",
    "        #print(\"job_waiting_feature\", job_waiting_feature.shape)\n",
    "        concat = torch.cat([readout, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # print(concat.shape)\n",
    "\n",
    "        # concat = F.normalize(concat) # normalize\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        # print(feature_extract)\n",
    "\n",
    "        output = self.pi_prob_fc(feature_extract) \n",
    "\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # 아래는 엔트로피 구하는 과정\n",
    "        log_prob = F.log_softmax(output, dim=1)\n",
    "        entropy = (log_prob * prob).sum(1, keepdim=True)\n",
    "        \n",
    "        return prob, entropy, output\n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        if job_waiting_feature.device == 'cuda':\n",
    "            link_feature = link_feature.cuda()\n",
    "            adjacency = adjacency.cuda()\n",
    "\n",
    "        node_feature = F.relu(self.pi_s_ecc1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.pi_graph_u_net1(node_feature, adjacency))\n",
    "        node_feature = F.relu(self.pi_s_ecc2(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.pi_graph_u_net2(node_feature, adjacency))\n",
    "        node_feature = F.relu(self.pi_s_ecc3(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.pi_graph_u_net3(node_feature, adjacency))\n",
    "\n",
    "        \n",
    "        readout = global_mean_pool(node_feature, data.batch)\n",
    "\n",
    "\n",
    "        #print(\"readout\", readout.shape)\n",
    "        #print(\"job_waiting_feature\", job_waiting_feature.shape)\n",
    "        concat = torch.cat([readout, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # print(concat.shape)\n",
    "\n",
    "        # concat = F.normalize(concat) # normalize\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        \n",
    "        value = self.v_value_fc(feature_extract) # 앞부분은 pi랑 공유해야 하고, concat -> value_fc를 거치는 것만 다름.\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "        \n",
    "    def return_link_dict(sel, ad):\n",
    "        result = {}\n",
    "        for i in range(len(ad[0])//2):\n",
    "            result[f'{ad[0][2*i]}{ad[0][2 *(i)+1]}'] = i\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def return_new_mean(self, mean, num, new_data):\n",
    "        result = (mean * num + new_data) / (num + 1)\n",
    "        return result\n",
    "        \n",
    "    def return_new_sd(self, square_mean, mean):\n",
    "        result = (square_mean - mean**2)**0.5\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def return_normalize_reward(self, reward):\n",
    "        self.x_mean = self.return_new_mean(self.x_mean, self.reward_sample_num, reward)\n",
    "        self.x2_mean = self.return_new_mean(self.x2_mean, self.reward_sample_num, reward**2)\n",
    "        self.sd = self.return_new_sd(self.x2_mean, self.x_mean)\n",
    "        \n",
    "        if self.sd == 0:\n",
    "                z_normalized = 0\n",
    "        else:  \n",
    "            z_normalized = (reward - self.x_mean) / self.sd\n",
    "\n",
    "        self.reward_sample_num += 1\n",
    "        \n",
    "        return z_normalized\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "        \n",
    "    def make_batch(self):\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "                \n",
    "            r_lst.append([r/100.0])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask_lst\n",
    "\n",
    "    def make_first_batch(self):\n",
    "        self.data = self.data[::-1]\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "                \n",
    "            r_lst.append([r/100.0])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_net(self):\n",
    "        self = self.cuda()\n",
    "        first = True\n",
    "        pre_advantage = 0.0\n",
    "        while len(self.data) > 0:\n",
    "            if first:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_first_batch()\n",
    "                first = False\n",
    "            else:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_batch()\n",
    "            \n",
    "            network_batch = network_batch.cuda()\n",
    "            job_waiting = job_waiting.cuda()\n",
    "            a = a.cuda()\n",
    "            r = r.cuda()\n",
    "            next_network_batch = next_network_batch.cuda()\n",
    "            next_job_waiting = next_job_waiting.cuda()\n",
    "            prob_a = prob_a.cuda()\n",
    "            entropy = entropy.cuda()\n",
    "            sojourn_time = sojourn_time.cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).cuda()\n",
    "            action_mask = action_mask.cuda()\n",
    "\n",
    "            for i in range(3):\n",
    "                \n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                \n",
    "                delta = td_target - self.v([network_batch, job_waiting])\n",
    "                delta = delta.detach().to('cpu').numpy()\n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = (gamma_gpu**sojourn_time[i][0]) * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "                pre_advantage = advantage\n",
    "\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(self.v([network_batch, job_waiting]) , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                pi, _, outputs = self.pi([network_batch, job_waiting])\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                outputs = outputs * action_mask\n",
    "\n",
    "                exp_sum = torch.sum(torch.exp(outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "                exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "                outputs_a = outputs.gather(1,a)\n",
    "                pi_a = torch.exp(outputs_a) / exp_sum\n",
    "\n",
    "                # pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * advantage_lst\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage_lst\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-18 10:21:11 AM] training....\n",
      "[2022-08-18 10:21:22 AM] training complete\n",
      "[2022-08-18 10:21:26 AM] training....\n",
      "[2022-08-18 10:21:31 AM] training complete\n",
      "[2022-08-18 10:21:37 AM] training....\n",
      "[2022-08-18 10:21:43 AM] training complete\n",
      "[2022-08-18 10:21:48 AM] training....\n",
      "[2022-08-18 10:21:54 AM] training complete\n",
      "[2022-08-18 10:21:58 AM] training....\n",
      "[2022-08-18 10:22:04 AM] training complete\n",
      "[2022-08-18 10:22:08 AM] training....\n",
      "[2022-08-18 10:22:14 AM] training complete\n",
      "[2022-08-18 10:22:19 AM] training....\n",
      "[2022-08-18 10:22:24 AM] training complete\n",
      "[2022-08-18 10:22:28 AM] training....\n",
      "[2022-08-18 10:22:34 AM] training complete\n",
      "[2022-08-18 10:22:38 AM] training....\n",
      "[2022-08-18 10:22:44 AM] training complete\n",
      "[2022-08-18 10:22:49 AM] training....\n",
      "[2022-08-18 10:22:55 AM] training complete\n",
      "[2022-08-18 10:23:00 AM] training....\n",
      "[2022-08-18 10:23:04 AM] training complete\n",
      "[2022-08-18 10:23:08 AM] training....\n",
      "[2022-08-18 10:23:14 AM] training complete\n",
      "[2022-08-18 10:23:18 AM] training....\n",
      "[2022-08-18 10:23:23 AM] training complete\n",
      "[2022-08-18 10:23:27 AM] training....\n",
      "[2022-08-18 10:23:32 AM] training complete\n",
      "[2022-08-18 10:23:36 AM] training....\n",
      "[2022-08-18 10:23:42 AM] training complete\n",
      "[2022-08-18 10:23:47 AM] training....\n",
      "[2022-08-18 10:23:58 AM] training complete\n",
      "[2022-08-18 10:24:00 AM] training....\n",
      "[2022-08-18 10:24:05 AM] training complete\n",
      "[2022-08-18 10:24:09 AM] training....\n",
      "[2022-08-18 10:24:15 AM] training complete\n",
      "[2022-08-18 10:24:18 AM] training....\n",
      "[2022-08-18 10:24:24 AM] training complete\n",
      "[2022-08-18 10:24:28 AM] training....\n",
      "[2022-08-18 10:24:33 AM] training complete\n",
      "[2022-08-18 10:24:37 AM] training....\n",
      "[2022-08-18 10:24:42 AM] training complete\n",
      "[2022-08-18 10:24:45 AM] training....\n",
      "[2022-08-18 10:24:50 AM] training complete\n",
      "[2022-08-18 10:24:53 AM] training....\n",
      "[2022-08-18 10:24:59 AM] training complete\n",
      "[2022-08-18 10:25:02 AM] training....\n",
      "[2022-08-18 10:25:08 AM] training complete\n",
      "[2022-08-18 10:25:11 AM] training....\n",
      "[2022-08-18 10:25:17 AM] training complete\n",
      "[2022-08-18 10:25:21 AM] training....\n",
      "[2022-08-18 10:25:26 AM] training complete\n",
      "[2022-08-18 10:25:28 AM] training....\n",
      "[2022-08-18 10:25:33 AM] training complete\n",
      "[2022-08-18 10:25:37 AM] training....\n",
      "[2022-08-18 10:25:42 AM] training complete\n",
      "[2022-08-18 10:25:45 AM] training....\n",
      "[2022-08-18 10:25:49 AM] training complete\n",
      "[2022-08-18 10:25:52 AM] training....\n",
      "[2022-08-18 10:25:57 AM] training complete\n",
      "[2022-08-18 10:26:00 AM] training....\n",
      "[2022-08-18 10:26:05 AM] training complete\n",
      "[2022-08-18 10:26:08 AM] training....\n",
      "[2022-08-18 10:26:13 AM] training complete\n",
      "[2022-08-18 10:26:16 AM] training....\n",
      "[2022-08-18 10:26:21 AM] training complete\n",
      "[2022-08-18 10:26:24 AM] training....\n",
      "[2022-08-18 10:26:29 AM] training complete\n",
      "[2022-08-18 10:26:32 AM] training....\n",
      "[2022-08-18 10:26:37 AM] training complete\n",
      "[2022-08-18 10:26:39 AM] training....\n",
      "[2022-08-18 10:26:43 AM] training complete\n",
      "[2022-08-18 10:26:47 AM] training....\n",
      "[2022-08-18 10:26:51 AM] training complete\n",
      "[2022-08-18 10:26:53 AM] training....\n",
      "[2022-08-18 10:26:58 AM] training complete\n",
      "[2022-08-18 10:27:00 AM] training....\n",
      "[2022-08-18 10:27:05 AM] training complete\n",
      "[2022-08-18 10:27:09 AM] training....\n",
      "[2022-08-18 10:27:18 AM] training complete\n",
      "[2022-08-18 10:27:22 AM] training....\n",
      "[2022-08-18 10:27:28 AM] training complete\n",
      "[2022-08-18 10:27:31 AM] training....\n",
      "[2022-08-18 10:27:35 AM] training complete\n",
      "[2022-08-18 10:27:39 AM] training....\n",
      "[2022-08-18 10:27:45 AM] training complete\n",
      "[2022-08-18 10:27:49 AM] training....\n",
      "[2022-08-18 10:27:55 AM] training complete\n",
      "[2022-08-18 10:27:59 AM] training....\n",
      "[2022-08-18 10:28:05 AM] training complete\n",
      "[2022-08-18 10:28:08 AM] training....\n",
      "[2022-08-18 10:28:13 AM] training complete\n",
      "[2022-08-18 10:28:18 AM] training....\n",
      "[2022-08-18 10:28:23 AM] training complete\n",
      "[2022-08-18 10:28:27 AM] training....\n",
      "[2022-08-18 10:28:32 AM] training complete\n",
      "[2022-08-18 10:28:35 AM] training....\n",
      "[2022-08-18 10:28:40 AM] training complete\n",
      "[2022-08-18 10:28:43 AM] training....\n",
      "[2022-08-18 10:28:49 AM] training complete\n",
      "[2022-08-18 10:28:53 AM] training....\n",
      "[2022-08-18 10:28:58 AM] training complete\n",
      "[2022-08-18 10:29:01 AM] training....\n",
      "[2022-08-18 10:29:06 AM] training complete\n",
      "[2022-08-18 10:29:09 AM] training....\n",
      "[2022-08-18 10:29:14 AM] training complete\n",
      "[2022-08-18 10:29:19 AM] training....\n",
      "[2022-08-18 10:29:24 AM] training complete\n",
      "[2022-08-18 10:29:27 AM] training....\n",
      "[2022-08-18 10:29:32 AM] training complete\n",
      "[2022-08-18 10:29:36 AM] training....\n",
      "[2022-08-18 10:29:41 AM] training complete\n",
      "[2022-08-18 10:29:44 AM] training....\n",
      "[2022-08-18 10:29:49 AM] training complete\n",
      "[2022-08-18 10:29:53 AM] training....\n",
      "[2022-08-18 10:29:58 AM] training complete\n",
      "[2022-08-18 10:30:01 AM] training....\n",
      "[2022-08-18 10:30:07 AM] training complete\n",
      "[2022-08-18 10:30:10 AM] training....\n",
      "[2022-08-18 10:30:14 AM] training complete\n",
      "[2022-08-18 10:30:18 AM] training....\n",
      "[2022-08-18 10:30:23 AM] training complete\n",
      "[2022-08-18 10:30:27 AM] training....\n",
      "[2022-08-18 10:30:32 AM] training complete\n",
      "[2022-08-18 10:30:35 AM] training....\n",
      "[2022-08-18 10:30:39 AM] training complete\n",
      "[2022-08-18 10:30:42 AM] training....\n",
      "[2022-08-18 10:30:48 AM] training complete\n",
      "[2022-08-18 10:30:53 AM] training....\n",
      "[2022-08-18 10:30:58 AM] training complete\n",
      "[2022-08-18 10:31:03 AM] training....\n",
      "[2022-08-18 10:31:08 AM] training complete\n",
      "[2022-08-18 10:31:11 AM] training....\n",
      "[2022-08-18 10:31:17 AM] training complete\n",
      "[2022-08-18 10:31:20 AM] training....\n",
      "[2022-08-18 10:31:26 AM] training complete\n",
      "[2022-08-18 10:31:30 AM] training....\n",
      "[2022-08-18 10:31:35 AM] training complete\n",
      "[2022-08-18 10:31:38 AM] training....\n",
      "[2022-08-18 10:31:44 AM] training complete\n",
      "[2022-08-18 10:31:47 AM] training....\n",
      "[2022-08-18 10:31:53 AM] training complete\n",
      "[2022-08-18 10:31:57 AM] training....\n",
      "[2022-08-18 10:32:02 AM] training complete\n",
      "[2022-08-18 10:32:04 AM] training....\n",
      "[2022-08-18 10:32:09 AM] training complete\n",
      "[2022-08-18 10:32:12 AM] training....\n",
      "[2022-08-18 10:32:17 AM] training complete\n",
      "[2022-08-18 10:32:20 AM] training....\n",
      "[2022-08-18 10:32:24 AM] training complete\n",
      "[2022-08-18 10:32:28 AM] training....\n",
      "[2022-08-18 10:32:34 AM] training complete\n",
      "[2022-08-18 10:32:38 AM] training....\n",
      "[2022-08-18 10:32:41 AM] training complete\n",
      "[2022-08-18 10:32:45 AM] training....\n",
      "[2022-08-18 10:32:51 AM] training complete\n",
      "[2022-08-18 10:32:52 AM] training....\n",
      "[2022-08-18 10:32:56 AM] training complete\n",
      "[2022-08-18 10:32:59 AM] training....\n",
      "[2022-08-18 10:33:04 AM] training complete\n",
      "[2022-08-18 10:33:06 AM] training....\n",
      "[2022-08-18 10:33:10 AM] training complete\n",
      "[2022-08-18 10:33:12 AM] training....\n",
      "[2022-08-18 10:33:17 AM] training complete\n",
      "[2022-08-18 10:33:20 AM] training....\n",
      "[2022-08-18 10:33:24 AM] training complete\n",
      "[2022-08-18 10:33:27 AM] training....\n",
      "[2022-08-18 10:33:31 AM] training complete\n",
      "[2022-08-18 10:33:33 AM] training....\n",
      "[2022-08-18 10:33:37 AM] training complete\n",
      "[2022-08-18 10:33:39 AM] training....\n",
      "[2022-08-18 10:33:43 AM] training complete\n",
      "[2022-08-18 10:33:45 AM] training....\n",
      "[2022-08-18 10:33:49 AM] training complete\n",
      "[2022-08-18 10:33:51 AM] training....\n",
      "[2022-08-18 10:33:55 AM] training complete\n",
      "[2022-08-18 10:33:57 AM] training....\n",
      "[2022-08-18 10:34:02 AM] training complete\n",
      "[2022-08-18 10:34:04 AM] training....\n",
      "[2022-08-18 10:34:09 AM] training complete\n",
      "[2022-08-18 10:34:11 AM] training....\n",
      "[2022-08-18 10:34:15 AM] training complete\n",
      "[2022-08-18 10:34:17 AM] training....\n",
      "[2022-08-18 10:34:22 AM] training complete\n",
      "[2022-08-18 10:34:24 AM] training....\n",
      "[2022-08-18 10:34:28 AM] training complete\n",
      "[2022-08-18 10:34:31 AM] training....\n",
      "[2022-08-18 10:34:35 AM] training complete\n",
      "[2022-08-18 10:34:37 AM] training....\n",
      "[2022-08-18 10:34:41 AM] training complete\n",
      "[2022-08-18 10:34:43 AM] training....\n",
      "[2022-08-18 10:34:47 AM] training complete\n",
      "[2022-08-18 10:34:50 AM] training....\n",
      "[2022-08-18 10:34:54 AM] training complete\n",
      "[2022-08-18 10:34:56 AM] training....\n",
      "[2022-08-18 10:35:00 AM] training complete\n",
      "[2022-08-18 10:35:02 AM] training....\n",
      "[2022-08-18 10:35:07 AM] training complete\n",
      "[2022-08-18 10:35:09 AM] training....\n",
      "[2022-08-18 10:35:14 AM] training complete\n",
      "[2022-08-18 10:35:17 AM] training....\n",
      "[2022-08-18 10:35:22 AM] training complete\n",
      "[2022-08-18 10:35:24 AM] training....\n",
      "[2022-08-18 10:35:28 AM] training complete\n",
      "[2022-08-18 10:35:30 AM] training....\n",
      "[2022-08-18 10:35:34 AM] training complete\n",
      "[2022-08-18 10:35:36 AM] training....\n",
      "[2022-08-18 10:35:42 AM] training complete\n",
      "[2022-08-18 10:35:44 AM] training....\n",
      "[2022-08-18 10:35:49 AM] training complete\n",
      "[2022-08-18 10:35:51 AM] training....\n",
      "[2022-08-18 10:35:56 AM] training complete\n",
      "[2022-08-18 10:35:58 AM] training....\n",
      "[2022-08-18 10:36:02 AM] training complete\n",
      "[2022-08-18 10:36:04 AM] training....\n",
      "[2022-08-18 10:36:08 AM] training complete\n",
      "[2022-08-18 10:36:10 AM] training....\n",
      "[2022-08-18 10:36:15 AM] training complete\n",
      "[2022-08-18 10:36:17 AM] training....\n",
      "[2022-08-18 10:36:21 AM] training complete\n",
      "[2022-08-18 10:36:23 AM] training....\n",
      "[2022-08-18 10:36:28 AM] training complete\n",
      "[2022-08-18 10:36:29 AM] training....\n",
      "[2022-08-18 10:36:34 AM] training complete\n",
      "[2022-08-18 10:36:36 AM] training....\n",
      "[2022-08-18 10:36:41 AM] training complete\n",
      "[2022-08-18 10:36:42 AM] training....\n",
      "[2022-08-18 10:36:46 AM] training complete\n",
      "[2022-08-18 10:36:49 AM] training....\n",
      "[2022-08-18 10:36:53 AM] training complete\n",
      "[2022-08-18 10:36:55 AM] training....\n",
      "[2022-08-18 10:36:59 AM] training complete\n",
      "[2022-08-18 10:37:02 AM] training....\n",
      "[2022-08-18 10:37:06 AM] training complete\n",
      "[2022-08-18 10:37:08 AM] training....\n",
      "[2022-08-18 10:37:12 AM] training complete\n",
      "[2022-08-18 10:37:14 AM] training....\n",
      "[2022-08-18 10:37:19 AM] training complete\n",
      "[2022-08-18 10:37:22 AM] training....\n",
      "[2022-08-18 10:37:27 AM] training complete\n",
      "[2022-08-18 10:37:29 AM] training....\n",
      "[2022-08-18 10:37:35 AM] training complete\n",
      "[2022-08-18 10:37:38 AM] training....\n",
      "[2022-08-18 10:37:43 AM] training complete\n",
      "[2022-08-18 10:37:46 AM] training....\n",
      "[2022-08-18 10:37:51 AM] training complete\n",
      "[2022-08-18 10:37:53 AM] training....\n",
      "[2022-08-18 10:37:57 AM] training complete\n",
      "[2022-08-18 10:38:00 AM] training....\n",
      "[2022-08-18 10:38:05 AM] training complete\n",
      "[2022-08-18 10:38:07 AM] training....\n",
      "[2022-08-18 10:38:11 AM] training complete\n",
      "[2022-08-18 10:38:13 AM] training....\n",
      "[2022-08-18 10:38:18 AM] training complete\n",
      "[2022-08-18 10:38:21 AM] training....\n",
      "[2022-08-18 10:38:26 AM] training complete\n",
      "[2022-08-18 10:38:29 AM] training....\n",
      "[2022-08-18 10:38:33 AM] training complete\n",
      "[2022-08-18 10:38:36 AM] training....\n",
      "[2022-08-18 10:38:41 AM] training complete\n",
      "[2022-08-18 10:38:44 AM] training....\n",
      "[2022-08-18 10:38:49 AM] training complete\n",
      "[2022-08-18 10:38:52 AM] training....\n",
      "[2022-08-18 10:38:57 AM] training complete\n",
      "[2022-08-18 10:38:59 AM] training....\n",
      "[2022-08-18 10:39:04 AM] training complete\n",
      "[2022-08-18 10:39:07 AM] training....\n",
      "[2022-08-18 10:39:12 AM] training complete\n",
      "[2022-08-18 10:39:14 AM] training....\n",
      "[2022-08-18 10:39:19 AM] training complete\n",
      "[2022-08-18 10:39:22 AM] training....\n",
      "[2022-08-18 10:39:27 AM] training complete\n",
      "[2022-08-18 10:39:31 AM] training....\n",
      "[2022-08-18 10:39:37 AM] training complete\n",
      "[2022-08-18 10:39:39 AM] training....\n",
      "[2022-08-18 10:39:44 AM] training complete\n",
      "[2022-08-18 10:39:47 AM] training....\n",
      "[2022-08-18 10:39:52 AM] training complete\n",
      "[2022-08-18 10:39:55 AM] training....\n",
      "[2022-08-18 10:40:01 AM] training complete\n",
      "[2022-08-18 10:40:03 AM] training....\n",
      "[2022-08-18 10:40:08 AM] training complete\n",
      "[2022-08-18 10:40:11 AM] training....\n",
      "[2022-08-18 10:40:17 AM] training complete\n",
      "[2022-08-18 10:40:20 AM] training....\n",
      "[2022-08-18 10:40:25 AM] training complete\n",
      "[2022-08-18 10:40:26 AM] training....\n",
      "[2022-08-18 10:40:30 AM] training complete\n",
      "[2022-08-18 10:40:33 AM] training....\n",
      "[2022-08-18 10:40:38 AM] training complete\n",
      "[2022-08-18 10:40:41 AM] training....\n",
      "[2022-08-18 10:40:47 AM] training complete\n",
      "[2022-08-18 10:40:51 AM] training....\n",
      "[2022-08-18 10:40:57 AM] training complete\n",
      "[2022-08-18 10:40:59 AM] training....\n",
      "[2022-08-18 10:41:04 AM] training complete\n",
      "[2022-08-18 10:41:07 AM] training....\n",
      "[2022-08-18 10:41:12 AM] training complete\n",
      "[2022-08-18 10:41:16 AM] training....\n",
      "[2022-08-18 10:41:21 AM] training complete\n",
      "[2022-08-18 10:41:24 AM] training....\n",
      "[2022-08-18 10:41:30 AM] training complete\n",
      "[2022-08-18 10:41:34 AM] training....\n",
      "[2022-08-18 10:41:40 AM] training complete\n",
      "[2022-08-18 10:41:42 AM] training....\n",
      "[2022-08-18 10:41:48 AM] training complete\n",
      "[2022-08-18 10:41:52 AM] training....\n",
      "[2022-08-18 10:41:57 AM] training complete\n",
      "[2022-08-18 10:41:58 AM] training....\n",
      "[2022-08-18 10:42:03 AM] training complete\n",
      "[2022-08-18 10:42:05 AM] training....\n",
      "[2022-08-18 10:42:09 AM] training complete\n",
      "[2022-08-18 10:42:11 AM] training....\n",
      "[2022-08-18 10:42:16 AM] training complete\n",
      "[2022-08-18 10:42:19 AM] training....\n",
      "[2022-08-18 10:42:24 AM] training complete\n",
      "[2022-08-18 10:42:27 AM] training....\n",
      "[2022-08-18 10:42:33 AM] training complete\n",
      "[2022-08-18 10:42:35 AM] training....\n",
      "[2022-08-18 10:42:40 AM] training complete\n",
      "[2022-08-18 10:42:42 AM] training....\n",
      "[2022-08-18 10:42:47 AM] training complete\n",
      "[2022-08-18 10:42:50 AM] training....\n",
      "[2022-08-18 10:42:55 AM] training complete\n",
      "[2022-08-18 10:42:59 AM] training....\n",
      "[2022-08-18 10:43:03 AM] training complete\n",
      "[2022-08-18 10:43:06 AM] training....\n",
      "[2022-08-18 10:43:11 AM] training complete\n",
      "[2022-08-18 10:43:14 AM] training....\n",
      "[2022-08-18 10:43:18 AM] training complete\n",
      "[2022-08-18 10:43:22 AM] training....\n",
      "[2022-08-18 10:43:26 AM] training complete\n",
      "[2022-08-18 10:43:29 AM] training....\n",
      "[2022-08-18 10:43:33 AM] training complete\n",
      "[2022-08-18 10:43:35 AM] training....\n",
      "[2022-08-18 10:43:40 AM] training complete\n",
      "[2022-08-18 10:43:43 AM] training....\n",
      "[2022-08-18 10:43:48 AM] training complete\n",
      "[2022-08-18 10:43:51 AM] training....\n",
      "[2022-08-18 10:43:56 AM] training complete\n",
      "[2022-08-18 10:43:58 AM] training....\n",
      "[2022-08-18 10:44:02 AM] training complete\n",
      "[2022-08-18 10:44:04 AM] training....\n",
      "[2022-08-18 10:44:09 AM] training complete\n",
      "[2022-08-18 10:44:12 AM] training....\n",
      "[2022-08-18 10:44:17 AM] training complete\n",
      "[2022-08-18 10:44:19 AM] training....\n",
      "[2022-08-18 10:44:24 AM] training complete\n",
      "[2022-08-18 10:44:26 AM] training....\n",
      "[2022-08-18 10:44:31 AM] training complete\n",
      "[2022-08-18 10:44:32 AM] training....\n",
      "[2022-08-18 10:44:36 AM] training complete\n",
      "[2022-08-18 10:44:40 AM] training....\n",
      "[2022-08-18 10:44:45 AM] training complete\n",
      "[2022-08-18 10:44:47 AM] training....\n",
      "[2022-08-18 10:44:51 AM] training complete\n",
      "[2022-08-18 10:44:54 AM] training....\n",
      "[2022-08-18 10:44:59 AM] training complete\n",
      "[2022-08-18 10:45:00 AM] training....\n",
      "[2022-08-18 10:45:05 AM] training complete\n",
      "[2022-08-18 10:45:07 AM] training....\n",
      "[2022-08-18 10:45:11 AM] training complete\n",
      "[2022-08-18 10:45:14 AM] training....\n",
      "[2022-08-18 10:45:19 AM] training complete\n",
      "[2022-08-18 10:45:22 AM] training....\n",
      "[2022-08-18 10:45:28 AM] training complete\n",
      "[2022-08-18 10:45:31 AM] training....\n",
      "[2022-08-18 10:45:37 AM] training complete\n",
      "[2022-08-18 10:45:40 AM] training....\n",
      "[2022-08-18 10:45:46 AM] training complete\n",
      "[2022-08-18 10:45:49 AM] training....\n",
      "[2022-08-18 10:45:53 AM] training complete\n",
      "[2022-08-18 10:45:57 AM] training....\n",
      "[2022-08-18 10:46:02 AM] training complete\n",
      "[2022-08-18 10:46:05 AM] training....\n",
      "[2022-08-18 10:46:09 AM] training complete\n",
      "[2022-08-18 10:46:11 AM] training....\n",
      "[2022-08-18 10:46:16 AM] training complete\n",
      "[2022-08-18 10:46:18 AM] training....\n",
      "[2022-08-18 10:46:22 AM] training complete\n",
      "[2022-08-18 10:46:25 AM] training....\n",
      "[2022-08-18 10:46:29 AM] training complete\n",
      "[2022-08-18 10:46:32 AM] training....\n",
      "[2022-08-18 10:46:37 AM] training complete\n",
      "[2022-08-18 10:46:40 AM] training....\n",
      "[2022-08-18 10:46:45 AM] training complete\n",
      "[2022-08-18 10:46:48 AM] training....\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global adjacency, entropy_weight\n",
    "    model = actor_network()\n",
    "    # model.load_state_dict(torch.load(\"./history30/model.pth\")) # -1 , +1\n",
    "    # model.load_state_dict(torch.load(\"./history80/model.pth\")) # original\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "\n",
    "    adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    # node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    # node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    # link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    # job_waiting_state = env.get_job_waiting_vector()\n",
    "\n",
    "    # network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "\n",
    "    # for each time step\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = []\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(nodeNum)]\n",
    "    void_selected_num = 0\n",
    "    \n",
    "    while True:\n",
    "        model = model.to('cpu')\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state = getOmnetMessage()\n",
    "\n",
    "            state = json.loads(state) # state 받았으므로 action 하면됨.\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            node_waiting_state = np.array(eval(state['nodeState']))\n",
    "            node_processing_state = np.array(eval(state['nodeProcessing']))\n",
    "            link_state = np.array(eval(state['linkWaiting']))\n",
    "            job_waiting_state = np.array(eval(state['jobWaiting']))\n",
    "            activated_job_list = eval(state['activatedJobList'])\n",
    "            isAction = int(state['isAction'])\n",
    "            reward = float(state['reward'])\n",
    "            averageLatency = float(state['averageLatency'])\n",
    "            completeJobNum = int(state['completeJobNum'])\n",
    "            sojournTime = float(state['sojournTime'])\n",
    "\n",
    "            if averageLatency != -1:\n",
    "                writer.add_scalar(\"averageLatency/train\", averageLatency ,step)\n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum ,step)\n",
    "            writer.add_scalar(\"Reward/train\", reward, step)\n",
    "\n",
    "            # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            first_sample = True\n",
    "            if is_train:\n",
    "                if len(temp_history) > 0:\n",
    "                    temp_history[-1][8] = sojournTime\n",
    "                for history in temp_history:\n",
    "                    history[3] = reward/100.0\n",
    "                    model.put_data(history)\n",
    "\n",
    "            temp_history = []\n",
    "\n",
    "            job_index = int(state['jobIndex'])\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "\n",
    "            node_state = np.concatenate((node_waiting_state,node_processing_state) ,axis = 1)\n",
    "            node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = collections.deque()\n",
    "            for job in job_waiting_state:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state = torch.tensor(job_waiting_state, dtype=torch.float).view(1, -1)\n",
    "            # print(job_waiting_state)\n",
    "\n",
    "            network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(nodeNum):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(nodeNum)] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward != 0:\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                    writer.add_scalar(\"Value/train\", model.v([network_state, job_waiting_state])[0], step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction = False\n",
    "                \n",
    "\n",
    "            if isAction:\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = 1\n",
    "                for i in range(nodeNum):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[nodeNum:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "                #print(\"action start.\")\n",
    "                for order in range(len(subtasks)):\n",
    "                    if subtasks[order] == 0:\n",
    "                        break\n",
    "\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                    # print(node_state.shape)\n",
    "                    # print(link_state.shape)\n",
    "                    # print(adjacency.shape)\n",
    "                    # print(job_waiting_state.shape)\n",
    "                    prob, entropy, output = model.pi([network_state, job_waiting_state])\n",
    "                    \n",
    "                    #print(f'prob : {prob}')\n",
    "                    \n",
    "                    m = Categorical(prob) \n",
    "                    node = m.sample().item()\n",
    "                    #print(f'node : {node}')\n",
    "                    \n",
    "                    # void action 실험용\n",
    "                    # node = nodeNum \n",
    "                    \n",
    "                    \n",
    "                    # void action 뽑으면\n",
    "                    if node == nodeNum and not scheduling_start: \n",
    "                        # print(\"void\")\n",
    "                        prob[0] = torch.Tensor([0] * nodeNum + [1.0])\n",
    "                        action_mask = [int(not scheduling_start) if i == node else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, network_state, job_waiting_state, prob[0][node].item(), entropy, 0, action_mask])\n",
    "                        sendOmnetMessage(\"void\")\n",
    "\n",
    "                        #print(\"action finish.\")\n",
    "                        \n",
    "                        if getOmnetMessage() == \"ok\":\n",
    "                            void_selected_num += 1\n",
    "                            \n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        scheduling_start = True\n",
    "\n",
    "                    if scheduling_start:\n",
    "\n",
    "                        prob = torch.Tensor([F.softmax(output[0][:nodeNum], dim=0).tolist()])\n",
    "                        prob = torch.cat([prob[0], torch.tensor([0])]) # void action masking\n",
    "                        prob = torch.Tensor([prob.tolist()]).cuda()\n",
    "\n",
    "\n",
    "\n",
    "                        m = Categorical(prob[0])\n",
    "                        node = m.sample().item()\n",
    "                        \n",
    "\n",
    "                        offloading_vector.append(node)\n",
    "\n",
    "                        node_selected_num[node] += 1\n",
    "                        \n",
    "                        # state transition(환경으로부터 매번 state를 업데이트하는게 아닌, 현재 state를 기반으로 action에 해당하는 waiting만 더해줌).\n",
    "                        # 아래의 구체적인 코드는 이해하시기 보단 OMNeT++에서 받아온 데이터로 새로 짜시는게 빠를 것 같습니다.\n",
    "                        next_node_state = node_state.clone().detach()\n",
    "                        next_job_waiting_state = job_waiting_state.clone().detach()\n",
    "                        # print(next_job_waiting_state)\n",
    "                        \n",
    "                        next_node_state[node][modelNum * job_idx + order] += (subtasks[order]/100) # 100으로 나누는 이유 : 이렇게 해야 액션이 한쪽 노드로 쏠리게끔 학습이 되는 것을 어느정도 방지할 수 있는 것을 확인.\n",
    "                        # 100으로 안나눠주면 너무 큰 값이 state에 추가되어서 inference시 가중치랑 곱해지면서 액션이 한쪽으로 확 쏠리는 걸로 예상됨.\n",
    "                        next_network_state = Data(x=next_node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                        next_job_waiting_state[0][nodeNum + order] = 0\n",
    "\n",
    "                        action_mask = [int(not scheduling_start) if i == nodeNum else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy, 0, action_mask])\n",
    "\n",
    "                        # model.put_data([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy])\n",
    "                        node_state = next_node_state\n",
    "                        job_waiting_state = next_job_waiting_state\n",
    "\n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            \n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            writer.add_scalar(\"CompleteNum/train\", complete_num, episode)\n",
    "            episode += 1\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = pathName + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            average_reward = 0\n",
    "            average_reward_num = 0\n",
    "\n",
    "            # entropy_weight *= 0.99\n",
    "\n",
    "\n",
    "            # if len(model.data) > 0 and step % train_cycle == train_quitient:\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training....\")\n",
    "            #     model.train_net()\n",
    "            #     modelPathName = pathName + \"/model.pth\"\n",
    "            #     torch.save(model.state_dict(), modelPathName)\n",
    "            #     writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            #     average_reward = 0\n",
    "            #     average_reward_num = 0\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training complete\")\n",
    "            if is_train:\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                modelPathName = pathName + \"/model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                \n",
    "                \n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.Tensor([[-0.0256,  0.0438,  0.0015, -0.0308, -0.0386, -0.0370, -0.0170, -0.0413]])\n",
    "print(output[0][:nodeNum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [17.941176], [17.941176], [17.941176], [17.941176], [17.941176], [43.333333], [43.333333], [43.333333], [43.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [10.0], [10.0], [10.0], [10.0], [10.0], [26.25], [26.25], [26.25], [26.25], [24.054054], [24.054054], [24.054054], [24.054054], [24.054054], [14.137931], [14.137931], [41.5], [41.5], [21.538462], [21.538462], [37.058824], [21.315789], [21.315789], [21.315789], [21.315789], [24.571429], [24.571429], [24.571429], [24.571429], [32.307692], [32.307692], [32.307692], [32.307692], [7.592593], [7.592593], [7.592593], [7.592593], [7.592593], [14.705882], [112.5], [21.666667], [21.666667], [21.666667], [21.666667], [21.666667], [19.574468], [19.574468], [19.574468], [19.574468], [19.574468], [68.0], [68.0], [68.0], [17.692308], [17.692308], [17.692308], [43.636364], [43.636364], [43.636364], [43.636364], [1.52381], [1.52381], [1.52381], [1.52381], [1.52381], [2.966102], [35.333333], [35.333333], [3.608247], [3.608247], [3.608247], [3.608247], [3.608247], [17.037037], [17.037037], [17.037037], [10.0], [10.0], [10.0], [34.074074], [34.074074], [34.074074], [34.074074], [10.0], [87.142857], [77.272727], [10.09901], [10.09901], [70.833333], [70.833333], [70.833333], [70.833333], [59.375], [59.375], [59.375], [59.375], [33.214286], [33.214286], [33.214286], [46.0], [46.0], [46.0], [46.0], [20.357143], [20.357143], [20.357143], [20.357143], [28.4375], [22.857143], [22.857143], [22.857143], [22.857143], [56.923077], [56.923077], [56.923077], [56.923077], [56.923077], [16.842105], [18.666667], [18.666667], [18.666667], [18.666667], [18.666667], [5.806452], [5.806452], [5.806452], [35.0], [35.0], [35.0], [35.0], [32.352941], [32.352941], [10.0], [10.0], [16.666667], [16.666667], [16.666667], [16.666667], [34.375], [34.375], [34.375], [34.375], [34.375], [35.172414], [35.172414], [35.172414], [14.0], [14.0], [14.0], [8.695652], [18.4], [16.153846], [16.153846], [16.153846], [16.153846], [16.153846], [27.575758], [27.575758], [14.761905], [14.761905], [14.761905], [22.1875], [22.1875], [59.333333], [10.0], [10.0], [10.0], [27.666667], [20.0], [20.0], [20.0], [118.0], [118.0], [118.0], [118.0], [10.0], [10.0], [19.677419], [19.677419], [19.677419], [19.677419], [40.769231], [40.769231], [40.769231], [19.393939], [19.393939], [19.393939], [19.393939], [7.173913], [7.173913], [7.173913], [31.052632], [31.052632], [10.0], [25.714286], [25.714286], [25.714286], [25.714286], [28.888889], [40.526316], [31.363636], [31.363636], [31.363636], [31.363636], [21.5], [21.5], [21.5], [92.857143], [92.857143], [92.857143], [8.148148], [10.0], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [83.994253], [83.994253], [83.994253], [83.994253], [83.994253], [83.994253], [3.873874], [3.873874], [3.873874], [3.873874], [5.416667], [5.416667], [5.416667], [9.904762], [9.904762], [9.904762], [9.904762], [9.904762], [34.0], [26.956522], [26.956522], [26.956522], [3.714286], [3.714286], [3.714286], [3.714286], [7.037037], [7.037037], [7.037037], [7.037037], [24.615385], [16.25], [16.25], [27.142857], [27.142857], [4.736842], [4.736842], [4.736842], [10.0], [23.0], [23.0], [34.583333], [34.583333], [10.0], [10.0], [34.705882], [19.02439], [19.02439], [19.02439], [19.02439], [19.02439], [40.0], [40.0], [40.0], [44.444444], [44.444444], [44.444444], [44.444444], [44.444444], [20.416667], [20.416667], [20.416667], [5.30303], [5.30303], [5.30303], [6.666667], [6.666667]]\n",
    "\n",
    "a = np.array(a)\n",
    "\n",
    "print(sum(a))\n",
    "print(np.std(a))\n",
    "print(np.mean(a))\n",
    "a = (a-np.mean(a)) / np.std(a)\n",
    "a = a.tolist()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 2, 3, 4, 5])\n",
    "b = torch.Tensor([[1], [2], [3], [4], [5]])\n",
    "\n",
    "a = torch.Tensor([3])\n",
    "\n",
    "print(a**b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "omnettest"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
