{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from library.ipynb\n",
      "importing Jupyter notebook from job.ipynb\n",
      "importing Jupyter notebook from jobqueue.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from dataplane.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history109\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "\n",
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet, TopKPooling, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sys import getsizeof\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent\")\n",
    "\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(pathName)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "modelNum = 0\n",
    "availableJobNum = 0\n",
    "nodeNum = 0\n",
    "jobWaitingLength = 0\n",
    "adjacency = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [8],\n",
       "        [1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[1],[2],[3],[4],[5],[6],[7],[8],[98]])\n",
    "a = torch.randperm(8)[-3:]\n",
    "\n",
    "b[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "tensor([[0., 2., 3.],\n",
      "        [0., 0., 3.],\n",
      "        [0., 0., 3.]])\n",
      "tensor([0.2689, 0.7311])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([[27.4746],\n",
      "        [20.0855],\n",
      "        [20.0855]])\n",
      "tensor([[2.],\n",
      "        [3.],\n",
      "        [3.]])\n",
      "tensor([[0.2689],\n",
      "        [1.0000],\n",
      "        [1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17404\\3439327629.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(outputs[0][1:]))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17404\\3439327629.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(outputs[0][2:]))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17404\\3439327629.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(F.softmax(outputs[0][2:]))\n"
     ]
    }
   ],
   "source": [
    "outputs = torch.tensor([[1,2,3],[1,2,3],[1,2,3]], dtype=torch.float)\n",
    "action_mask = torch.tensor([[0,1,1],[0,0,1],[0,0,1]])\n",
    "a = torch.tensor([[1],[2],[2]], dtype = torch.long)\n",
    "\n",
    "print(F.softmax(outputs, dim=1))\n",
    "\n",
    "outputs = outputs * action_mask\n",
    "\n",
    "print(outputs)\n",
    "print(F.softmax(outputs[0][1:]))\n",
    "print(F.softmax(outputs[0][2:]))\n",
    "print(F.softmax(outputs[0][2:]))\n",
    "\n",
    "exp_sum = torch.sum(torch.exp(outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "exp_sum = exp_sum.view(-1, 1)\n",
    "\n",
    "print(exp_sum)\n",
    "\n",
    "outputs_a = outputs.gather(1,a)\n",
    "print(outputs_a)\n",
    "\n",
    "pi_a = torch.exp(outputs_a) / exp_sum\n",
    "\n",
    "print(pi_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9911)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([ 0.0119,  0.0013,  0.0172, -0.0185, -0.0216, -0.0058,  0.0059])\n",
    "\n",
    "torch.sum(torch.exp(a))  - torch.sum(torch.tensor([True,False]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda:0\n",
      "<class 'torch.device'>\n",
      "<built-in method cuda of Tensor object at 0x00000248D8D328E0>\n",
      "<class 'torch.device'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.get_device_name(device = 0))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "\n",
    "print(type(device))\n",
    "\n",
    "kkkkk = torch.Tensor(1)\n",
    "\n",
    "kkkkk = kkkkk.cuda()\n",
    "\n",
    "print(kkkkk.cuda)\n",
    "print(type(kkkkk.device))\n",
    "print(kkkkk.device == device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.0003\n",
    "gamma           = 0.99\n",
    "entropy_weight  = 0.003\n",
    "val_loss_coef = 0.5\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.9\n",
    "eps_clip      = 0.2\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100\n",
    "hidden_feature_num = 0\n",
    "\n",
    "job_generate_rate = 0.003\n",
    "\n",
    "train_cycle = 300\n",
    "is_train = True\n",
    "train_quitient = 0\n",
    "\n",
    "if not is_train:\n",
    "    train_quitient = train_cycle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\agent2_connect - named pipe - pooling.ipynb 셀 5\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20named%20pipe%20-%20pooling.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20named%20pipe%20-%20pooling.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20named%20pipe%20-%20pooling.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m win32pipe\u001b[39m.\u001b[39mConnectNamedPipe(pipe, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# import os.path\n",
    "# import os\n",
    "# FIFO_FILENAME = './fifo-test'\n",
    "\n",
    "# if not os.path.exists(FIFO_FILENAME):\n",
    "#     os.mkfifo(FIFO_FILENAME)\n",
    "#     if os.path.exists(FIFO_FILENAME):\n",
    "#         fp_fifo = open(FIFO_FILENAME, \"rw\")\n",
    "\n",
    "# for i in range(128):\n",
    "#     fp_fifo.write(\"Hello,MakeCode\\n\")\n",
    "#     fp_fifo.write(\"\")\n",
    "\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\omnetPipe\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 초기화 완료\n",
      "노드 개수 : 7\n",
      "네트워크 최대 job 개수 : 20\n",
      "job 대기 가능 개수 : 15\n",
      "최대 subtask 개수 : 5\n",
      "인접 리스트 : [[0, 3, 1, 3, 1, 4, 1, 5, 2, 3, 2, 4, 2, 5, 3, 4, 3, 6, 4, 5, 4, 6, 5, 6], [3, 0, 3, 1, 4, 1, 5, 1, 3, 2, 4, 2, 5, 2, 4, 3, 6, 3, 5, 4, 6, 4, 6, 5]]\n",
      "node_feature_num : 200\n",
      "queue_feature_num : 180\n",
      "episode_length : 500\n"
     ]
    }
   ],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num // 4 + queue_feature_num)\n",
    "\n",
    "sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "print(f\"노드 개수 : {nodeNum}\")\n",
    "print(f\"네트워크 최대 job 개수 : {availableJobNum}\")\n",
    "print(f\"job 대기 가능 개수 : {jobWaitingLength}\")\n",
    "print(f\"최대 subtask 개수 : {modelNum}\")\n",
    "print(f\"인접 리스트 : {adjacency}\")\n",
    "print(f\"node_feature_num : {node_feature_num}\")\n",
    "print(f\"queue_feature_num : {queue_feature_num}\")\n",
    "print(f\"episode_length : {episode_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.x_mean = 0\n",
    "        self.x2_mean = 0\n",
    "        self.sd = 0\n",
    "        self.reward_sample_num = 0\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        self.adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "        # print(f\"self.adjacency : \", self.adjacency.shape)\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_ecc1 = NNConv(node_feature_num, node_feature_num, self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_mlp2 = nn.Sequential(nn.Linear(1, node_feature_num * (node_feature_num // 2)), nn.ReLU())\n",
    "        self.pi_s_ecc2 = NNConv(node_feature_num, (node_feature_num // 2), self.pi_mlp2, aggr='mean')\n",
    "        \n",
    "        self.pi_conv1 = GCNConv((node_feature_num // 2), (node_feature_num // 2))\n",
    "        self.pi_conv2 = GCNConv((node_feature_num // 2), (node_feature_num // 4))\n",
    "\n",
    "        self.pi_pooling1 = TopKPooling(node_feature_num // 2, 0.8)\n",
    "        self.pi_pooling2 = TopKPooling(node_feature_num // 4, 0.8)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear(node_feature_num // 4 + queue_feature_num, hidden_feature_num),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(hidden_feature_num, nodeNum + 1) # nodeNum + voidAction\n",
    "\n",
    "        self.v_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.v_s_ecc = NNConv(node_feature_num, node_feature_num, self.v_mlp1, aggr='mean')\n",
    "\n",
    "        self.v_graph_u_net = GraphUNet(node_feature_num, 10, node_feature_num, 3, 0.8)\n",
    "\n",
    "        self.v_backbone = nn.Sequential(\n",
    "            nn.Linear(node_feature_num + queue_feature_num, hidden_feature_num),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(hidden_feature_num, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, self.adjacency\n",
    "        if job_waiting_feature.device == device:\n",
    "            link_feature = link_feature.cuda()\n",
    "            adjacency = self.adjacency.cuda()\n",
    "\n",
    "        node_feature = node_feature.view(-1, nodeNum, node_feature_num)\n",
    "        link_feature = link_feature.view(-1, len(self.adjacency[0]), 1)\n",
    "        \n",
    "\n",
    "        readout_lst = [0] * len(node_feature)\n",
    "\n",
    "        for idx, node_feature_graph in enumerate(node_feature):\n",
    "\n",
    "            # print(f\"node_feature_graph : {node_feature_graph.shape}\")\n",
    "            # print(f\"node_feature_graph : {node_feature_graph.device}\")\n",
    "            # print(f\"adjacency : {adjacency.shape}\")\n",
    "            # print(f\"adjacency : {adjacency.device}\")\n",
    "            # print(f\"link_feature : {link_feature[idx].shape}\")\n",
    "            # print(f\"link_feature : {link_feature[idx].device}\")\n",
    "\n",
    "            node_feature_calc = F.relu(self.pi_s_ecc1(node_feature_graph, adjacency, link_feature[idx]))\n",
    "            node_feature_calc = F.relu(self.pi_s_ecc2(node_feature_calc, adjacency, link_feature[idx]))\n",
    "            \n",
    "            node_feature_calc = self.pi_conv1(node_feature_calc, adjacency)\n",
    "            node_feature_calc, adjacency_graph, _, _, perm, _ = self.pi_pooling1(node_feature_calc, adjacency)\n",
    "            \n",
    "            node_feature_calc = self.pi_conv2(node_feature_calc, adjacency_graph)\n",
    "            node_feature_calc, adjacency_graph, _, _, perm, _ = self.pi_pooling2(node_feature_calc, adjacency_graph)\n",
    "\n",
    "            readout = global_mean_pool(node_feature_calc, None)\n",
    "\n",
    "            readout_lst[idx] = readout\n",
    "\n",
    "        readout_lst = torch.stack(readout_lst).view(-1, (node_feature_num // 4))\n",
    "\n",
    "        # print(f\"readout_lst : {readout_lst}\")\n",
    "        # print(f\"job_waiting_feature : {job_waiting_feature}\")\n",
    "            \n",
    "        concat = torch.cat([readout_lst, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # print(concat.shape)\n",
    "\n",
    "        concat = F.normalize(concat) # normalize\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        # print(feature_extract)\n",
    "\n",
    "        output = self.pi_prob_fc(feature_extract) \n",
    "\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # 아래는 엔트로피 구하는 과정\n",
    "        log_prob = F.log_softmax(output, dim=1)\n",
    "        entropy = (log_prob * prob).sum(1, keepdim=True)\n",
    "        \n",
    "        return prob, entropy, output\n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, self.adjacency\n",
    "        if job_waiting_feature.device == device:\n",
    "            link_feature = link_feature.cuda()\n",
    "            adjacency = self.adjacency.cuda()\n",
    "\n",
    "        node_feature = node_feature.view(-1, nodeNum, node_feature_num)\n",
    "        link_feature = link_feature.view(-1, len(self.adjacency[0]), 1)\n",
    "        \n",
    "\n",
    "        readout_lst = [0] * len(node_feature)\n",
    "\n",
    "        for idx, node_feature_graph in enumerate(node_feature):\n",
    "\n",
    "            # print(f\"node_feature_graph : {node_feature_graph.shape}\")\n",
    "            # print(f\"node_feature_graph : {node_feature_graph.device}\")\n",
    "            # print(f\"adjacency : {adjacency.shape}\")\n",
    "            # print(f\"adjacency : {adjacency.device}\")\n",
    "            # print(f\"link_feature : {link_feature[idx].shape}\")\n",
    "            # print(f\"link_feature : {link_feature[idx].device}\")\n",
    "\n",
    "            node_feature_calc = F.relu(self.pi_s_ecc1(node_feature_graph, adjacency, link_feature[idx]))\n",
    "            node_feature_calc = F.relu(self.pi_s_ecc2(node_feature_calc, adjacency, link_feature[idx]))\n",
    "            \n",
    "            node_feature_calc = self.pi_conv1(node_feature_calc, adjacency)\n",
    "            node_feature_calc, adjacency_graph, _, _, perm, _ = self.pi_pooling1(node_feature_calc, adjacency)\n",
    "            \n",
    "            node_feature_calc = self.pi_conv2(node_feature_calc, adjacency_graph)\n",
    "            node_feature_calc, adjacency_graph, _, _, perm, _ = self.pi_pooling2(node_feature_calc, adjacency_graph)\n",
    "\n",
    "            readout = global_mean_pool(node_feature_calc, None)\n",
    "\n",
    "            readout_lst[idx] = readout\n",
    "\n",
    "        readout_lst = torch.stack(readout_lst).view(-1, (node_feature_num // 4))\n",
    "\n",
    "        # print(f\"readout_lst : {readout_lst}\")\n",
    "        # print(f\"job_waiting_feature : {job_waiting_feature}\")\n",
    "            \n",
    "        concat = torch.cat([readout_lst, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # print(concat.shape)\n",
    "\n",
    "        concat = F.normalize(concat) # normalize\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        \n",
    "        value = self.v_value_fc(feature_extract) # 앞부분은 pi랑 공유해야 하고, concat -> value_fc를 거치는 것만 다름.\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "        \n",
    "    def return_link_dict(sel, ad):\n",
    "        result = {}\n",
    "        for i in range(len(ad[0])//2):\n",
    "            result[f'{ad[0][2*i]}{ad[0][2 *(i)+1]}'] = i\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def return_new_mean(self, mean, num, new_data):\n",
    "        result = (mean * num + new_data) / (num + 1)\n",
    "        return result\n",
    "        \n",
    "    def return_new_sd(self, square_mean, mean):\n",
    "        result = (square_mean - mean**2)**0.5\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def return_normalize_reward(self, reward):\n",
    "        self.x_mean = self.return_new_mean(self.x_mean, self.reward_sample_num, reward)\n",
    "        self.x2_mean = self.return_new_mean(self.x2_mean, self.reward_sample_num, reward**2)\n",
    "        self.sd = self.return_new_sd(self.x2_mean, self.x_mean)\n",
    "        \n",
    "        if self.sd == 0:\n",
    "                z_normalized = 0\n",
    "        else:  \n",
    "            z_normalized = (reward - self.x_mean) / self.sd\n",
    "\n",
    "        self.reward_sample_num += 1\n",
    "        \n",
    "        return z_normalized\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "        \n",
    "    def make_batch(self):\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst = [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time = transition\n",
    "                \n",
    "            r_lst.append([r])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 7])\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=batch_size)\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=batch_size)\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time\n",
    "\n",
    "    def make_first_batch(self):\n",
    "        self.data = self.data[::-1]\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst = [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time = transition\n",
    "                \n",
    "            r_lst.append([r/float(episode_length)])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 7])\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=batch_size)\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=batch_size)\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_net(self):\n",
    "        self = self.cuda()\n",
    "        first = True\n",
    "        pre_advantage = 0.0\n",
    "        while len(self.data) > 0:\n",
    "            if first:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time = self.make_first_batch()\n",
    "                first = False\n",
    "            else:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time = self.make_batch()\n",
    "            \n",
    "            network_batch = network_batch.cuda()\n",
    "            job_waiting = job_waiting.cuda()\n",
    "            a = a.cuda()\n",
    "            r = r.cuda()\n",
    "            next_network_batch = next_network_batch.cuda()\n",
    "            next_job_waiting = next_job_waiting.cuda()\n",
    "            prob_a = prob_a.cuda()\n",
    "            entropy = entropy.cuda()\n",
    "            sojourn_time = sojourn_time.cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).cuda()\n",
    "\n",
    "            for i in range(3):\n",
    "                \n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                \n",
    "                delta = td_target - self.v([network_batch, job_waiting])\n",
    "                delta = delta.detach().to('cpu').numpy()\n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = (gamma_gpu**sojourn_time[i][0]) * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "                pre_advantage = advantage\n",
    "\n",
    "                v_loss = F.smooth_l1_loss(self.v([network_batch, job_waiting]) , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                \"\"\"\n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "                \"\"\"\n",
    "\n",
    "                pi, _, _ = self.pi([network_batch, job_waiting])\n",
    "                pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * advantage_lst\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage_lst\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * entropy + val_loss_coef * v_loss\n",
    "\n",
    "                #pi_loss = -(torch.log(pi_a)) * advantage\n",
    "\n",
    "                #print(\"pi_loss\", pi_loss)\n",
    "\n",
    "                #print(\"pi_loss\", pi_loss)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 2.5000, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1, 1, 1],[2, 2, 2, ],[3, 3, 3,],[4, 4, 4]])\n",
    "\n",
    "a = a.mean(dim=0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-14 04:08:03 AM] training....\n",
      "[2022-08-14 04:18:32 AM] training complete\n",
      "[2022-08-14 04:19:13 AM] training....\n",
      "[2022-08-14 04:29:20 AM] training complete\n",
      "[2022-08-14 04:29:49 AM] training....\n",
      "[2022-08-14 04:38:43 AM] training complete\n",
      "[2022-08-14 04:39:10 AM] training....\n",
      "[2022-08-14 04:47:34 AM] training complete\n",
      "[2022-08-14 04:48:01 AM] training....\n",
      "[2022-08-14 04:56:19 AM] training complete\n",
      "[2022-08-14 04:56:47 AM] training....\n",
      "[2022-08-14 05:05:16 AM] training complete\n",
      "[2022-08-14 05:05:42 AM] training....\n",
      "[2022-08-14 05:13:58 AM] training complete\n",
      "[2022-08-14 05:14:25 AM] training....\n",
      "[2022-08-14 05:22:54 AM] training complete\n",
      "[2022-08-14 05:23:20 AM] training....\n",
      "[2022-08-14 05:31:30 AM] training complete\n",
      "[2022-08-14 05:31:55 AM] training....\n",
      "[2022-08-14 05:39:59 AM] training complete\n",
      "[2022-08-14 05:40:25 AM] training....\n",
      "[2022-08-14 05:48:34 AM] training complete\n",
      "[2022-08-14 05:49:01 AM] training....\n",
      "[2022-08-14 05:57:19 AM] training complete\n",
      "[2022-08-14 05:57:45 AM] training....\n",
      "[2022-08-14 06:05:39 AM] training complete\n",
      "[2022-08-14 06:06:04 AM] training....\n",
      "[2022-08-14 06:14:01 AM] training complete\n",
      "[2022-08-14 06:14:27 AM] training....\n",
      "[2022-08-14 06:22:35 AM] training complete\n",
      "[2022-08-14 06:23:00 AM] training....\n",
      "[2022-08-14 06:30:45 AM] training complete\n",
      "[2022-08-14 06:31:10 AM] training....\n",
      "[2022-08-14 06:39:01 AM] training complete\n",
      "[2022-08-14 06:39:27 AM] training....\n",
      "[2022-08-14 06:47:29 AM] training complete\n",
      "[2022-08-14 06:47:55 AM] training....\n",
      "[2022-08-14 06:56:01 AM] training complete\n",
      "[2022-08-14 06:56:27 AM] training....\n",
      "[2022-08-14 07:04:27 AM] training complete\n",
      "[2022-08-14 07:04:54 AM] training....\n",
      "[2022-08-14 07:13:05 AM] training complete\n",
      "[2022-08-14 07:13:33 AM] training....\n",
      "[2022-08-14 07:21:51 AM] training complete\n",
      "[2022-08-14 07:22:17 AM] training....\n",
      "[2022-08-14 07:30:25 AM] training complete\n",
      "[2022-08-14 07:30:52 AM] training....\n",
      "[2022-08-14 07:39:25 AM] training complete\n",
      "[2022-08-14 07:39:49 AM] training....\n",
      "[2022-08-14 07:47:27 AM] training complete\n",
      "[2022-08-14 07:47:54 AM] training....\n",
      "[2022-08-14 07:56:21 AM] training complete\n",
      "[2022-08-14 07:56:47 AM] training....\n",
      "[2022-08-14 08:04:50 AM] training complete\n",
      "[2022-08-14 08:05:17 AM] training....\n",
      "[2022-08-14 08:13:51 AM] training complete\n",
      "[2022-08-14 08:14:17 AM] training....\n",
      "[2022-08-14 08:22:23 AM] training complete\n",
      "[2022-08-14 08:22:51 AM] training....\n",
      "[2022-08-14 08:31:33 AM] training complete\n",
      "[2022-08-14 08:32:01 AM] training....\n",
      "[2022-08-14 08:40:40 AM] training complete\n",
      "[2022-08-14 08:41:05 AM] training....\n",
      "[2022-08-14 08:48:54 AM] training complete\n",
      "[2022-08-14 08:49:20 AM] training....\n",
      "[2022-08-14 08:57:44 AM] training complete\n",
      "[2022-08-14 08:58:10 AM] training....\n",
      "[2022-08-14 09:06:25 AM] training complete\n",
      "[2022-08-14 09:06:49 AM] training....\n",
      "[2022-08-14 09:14:22 AM] training complete\n",
      "[2022-08-14 09:14:47 AM] training....\n",
      "[2022-08-14 09:22:23 AM] training complete\n",
      "[2022-08-14 09:22:49 AM] training....\n",
      "[2022-08-14 09:31:11 AM] training complete\n",
      "[2022-08-14 09:31:38 AM] training....\n",
      "[2022-08-14 09:39:50 AM] training complete\n",
      "[2022-08-14 09:40:15 AM] training....\n",
      "[2022-08-14 09:48:17 AM] training complete\n",
      "[2022-08-14 09:48:43 AM] training....\n",
      "[2022-08-14 09:56:31 AM] training complete\n",
      "[2022-08-14 09:56:58 AM] training....\n",
      "[2022-08-14 10:05:20 AM] training complete\n",
      "[2022-08-14 10:05:44 AM] training....\n",
      "[2022-08-14 10:13:26 AM] training complete\n",
      "[2022-08-14 10:13:52 AM] training....\n",
      "[2022-08-14 10:22:05 AM] training complete\n",
      "[2022-08-14 10:22:33 AM] training....\n",
      "[2022-08-14 10:31:06 AM] training complete\n",
      "[2022-08-14 10:31:32 AM] training....\n",
      "[2022-08-14 10:39:17 AM] training complete\n",
      "[2022-08-14 10:39:45 AM] training....\n",
      "[2022-08-14 10:47:25 AM] training complete\n",
      "[2022-08-14 10:47:56 AM] training....\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # global adjacency\n",
    "    model = actor_network()\n",
    "    # model.load_state_dict(torch.load(\"./history30/model.pth\")) # -1 , +1\n",
    "    # model.load_state_dict(torch.load(\"./history80/model.pth\")) # original\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "    \n",
    "\n",
    "    # node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    # node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    # link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    # job_waiting_state = env.get_job_waiting_vector()\n",
    "\n",
    "    # network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "\n",
    "    # for each time step\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = []\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(nodeNum)]\n",
    "    void_selected_num = 0\n",
    "    \n",
    "    while True:\n",
    "        model = model.to('cpu')\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state = getOmnetMessage()\n",
    "\n",
    "            state = json.loads(state) # state 받았으므로 action 하면됨.\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            node_waiting_state = np.array(eval(state['nodeState']))\n",
    "            node_processing_state = np.array(eval(state['nodeProcessing']))\n",
    "            link_state = np.array(eval(state['linkWaiting']))\n",
    "            job_waiting_state = np.array(eval(state['jobWaiting']))\n",
    "            activated_job_list = eval(state['activatedJobList'])\n",
    "            isAction = int(state['isAction'])\n",
    "            reward = float(state['reward'])\n",
    "            averageLatency = float(state['averageLatency'])\n",
    "            completeJobNum = int(state['completeJobNum'])\n",
    "            sojournTime = float(state['sojournTime'])\n",
    "\n",
    "            if averageLatency != -1:\n",
    "                writer.add_scalar(\"averageLatency/train\", averageLatency ,step)\n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum ,step)\n",
    "            writer.add_scalar(\"Reward/train\", reward, step)\n",
    "\n",
    "            # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            if is_train:\n",
    "                for history in temp_history:\n",
    "                    # history[3] = reward\n",
    "                    history[8] = sojournTime\n",
    "                    model.put_data(history)\n",
    "\n",
    "            temp_history = []\n",
    "\n",
    "            job_index = int(state['jobIndex'])\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "\n",
    "            node_state = np.concatenate((node_waiting_state,node_processing_state) ,axis = 1)\n",
    "            node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = collections.deque()\n",
    "            for job in job_waiting_state:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state = torch.tensor(job_waiting_state, dtype=torch.float).view(1, -1)\n",
    "            # print(job_waiting_state)\n",
    "\n",
    "            network_state = Data(x=node_state, edge_attr=link_state, edge_index=model.adjacency)\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(nodeNum):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(nodeNum)] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward != 0:\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=model.adjacency)\n",
    "                    writer.add_scalar(\"Value/train\", model.v([network_state, job_waiting_state])[0], step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction = False\n",
    "                \n",
    "\n",
    "            if isAction:\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = -1\n",
    "                for i in range(nodeNum):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[nodeNum:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "                #print(\"action start.\")\n",
    "                for order in range(len(subtasks)):\n",
    "                    if subtasks[order] == 0:\n",
    "                        break\n",
    "\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                    # print(node_state.shape)\n",
    "                    # print(link_state.shape)\n",
    "                    # print(adjacency.shape)\n",
    "                    # print(job_waiting_state.shape)\n",
    "                    prob, entropy, output = model.pi([network_state, job_waiting_state])\n",
    "                    \n",
    "                    #print(f'prob : {prob}')\n",
    "                    \n",
    "                    m = Categorical(prob) \n",
    "                    node = m.sample().item()\n",
    "                    #print(f'node : {node}')\n",
    "                    \n",
    "                    # void action 실험용\n",
    "                    # node = nodeNum \n",
    "                    \n",
    "                    \n",
    "                    # void action 뽑으면\n",
    "                    if node == nodeNum and not scheduling_start: \n",
    "                        # print(\"void\")\n",
    "                        prob[0] = torch.Tensor([0] * nodeNum + [1.0])\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, network_state, job_waiting_state, prob[0][node].item(), entropy, 0])\n",
    "                        sendOmnetMessage(\"void\")\n",
    "                        #print(\"action finish.\")\n",
    "                        \n",
    "                        if getOmnetMessage() == \"ok\":\n",
    "                            void_selected_num += 1\n",
    "                            \n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        scheduling_start = True\n",
    "\n",
    "                    if scheduling_start:\n",
    "\n",
    "                        prob = torch.Tensor([F.softmax(output[0][:nodeNum], dim=0).tolist()])\n",
    "                        prob = torch.cat([prob[0], torch.tensor([0])]) # void action masking\n",
    "                        prob = torch.Tensor([prob.tolist()]).cuda()\n",
    "\n",
    "\n",
    "                        m = Categorical(prob[0])\n",
    "                        node = m.sample().item()\n",
    "                        \n",
    "\n",
    "                        offloading_vector.append(node)\n",
    "\n",
    "                        node_selected_num[node] += 1\n",
    "                        \n",
    "                        # state transition(환경으로부터 매번 state를 업데이트하는게 아닌, 현재 state를 기반으로 action에 해당하는 waiting만 더해줌).\n",
    "                        # 아래의 구체적인 코드는 이해하시기 보단 OMNeT++에서 받아온 데이터로 새로 짜시는게 빠를 것 같습니다.\n",
    "                        next_node_state = node_state.clone().detach()\n",
    "                        next_job_waiting_state = job_waiting_state.clone().detach()\n",
    "                        # print(next_job_waiting_state)\n",
    "                        \n",
    "                        next_node_state[node][modelNum * job_idx + order] += (subtasks[order]/100) # 100으로 나누는 이유 : 이렇게 해야 액션이 한쪽 노드로 쏠리게끔 학습이 되는 것을 어느정도 방지할 수 있는 것을 확인.\n",
    "                        # 100으로 안나눠주면 너무 큰 값이 state에 추가되어서 inference시 가중치랑 곱해지면서 액션이 한쪽으로 확 쏠리는 걸로 예상됨.\n",
    "                        next_network_state = Data(x=next_node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                        next_job_waiting_state[0][nodeNum + order] = 0\n",
    "\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy, 0])\n",
    "\n",
    "                        # model.put_data([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy])\n",
    "                        node_state = next_node_state\n",
    "                        job_waiting_state = next_job_waiting_state\n",
    "\n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            \n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = int(episodic_reward['reward'])\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            episode += 1\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            if is_train:\n",
    "                model.set_reward(normalized_finish_num)\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = pathName + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "\n",
    "            # if len(model.data) > 0 and step % train_cycle == train_quitient:\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training....\")\n",
    "            #     model.train_net()\n",
    "            #     modelPathName = pathName + \"/model.pth\"\n",
    "            #     torch.save(model.state_dict(), modelPathName)\n",
    "            #     writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            #     average_reward = 0\n",
    "            #     average_reward_num = 0\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training complete\")\n",
    "            if is_train:\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                modelPathName = pathName + \"/model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "                average_reward = 0\n",
    "                average_reward_num = 0\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.Tensor([[-0.0256,  0.0438,  0.0015, -0.0308, -0.0386, -0.0370, -0.0170, -0.0413]])\n",
    "print(output[0][:nodeNum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [17.941176], [17.941176], [17.941176], [17.941176], [17.941176], [43.333333], [43.333333], [43.333333], [43.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [10.0], [10.0], [10.0], [10.0], [10.0], [26.25], [26.25], [26.25], [26.25], [24.054054], [24.054054], [24.054054], [24.054054], [24.054054], [14.137931], [14.137931], [41.5], [41.5], [21.538462], [21.538462], [37.058824], [21.315789], [21.315789], [21.315789], [21.315789], [24.571429], [24.571429], [24.571429], [24.571429], [32.307692], [32.307692], [32.307692], [32.307692], [7.592593], [7.592593], [7.592593], [7.592593], [7.592593], [14.705882], [112.5], [21.666667], [21.666667], [21.666667], [21.666667], [21.666667], [19.574468], [19.574468], [19.574468], [19.574468], [19.574468], [68.0], [68.0], [68.0], [17.692308], [17.692308], [17.692308], [43.636364], [43.636364], [43.636364], [43.636364], [1.52381], [1.52381], [1.52381], [1.52381], [1.52381], [2.966102], [35.333333], [35.333333], [3.608247], [3.608247], [3.608247], [3.608247], [3.608247], [17.037037], [17.037037], [17.037037], [10.0], [10.0], [10.0], [34.074074], [34.074074], [34.074074], [34.074074], [10.0], [87.142857], [77.272727], [10.09901], [10.09901], [70.833333], [70.833333], [70.833333], [70.833333], [59.375], [59.375], [59.375], [59.375], [33.214286], [33.214286], [33.214286], [46.0], [46.0], [46.0], [46.0], [20.357143], [20.357143], [20.357143], [20.357143], [28.4375], [22.857143], [22.857143], [22.857143], [22.857143], [56.923077], [56.923077], [56.923077], [56.923077], [56.923077], [16.842105], [18.666667], [18.666667], [18.666667], [18.666667], [18.666667], [5.806452], [5.806452], [5.806452], [35.0], [35.0], [35.0], [35.0], [32.352941], [32.352941], [10.0], [10.0], [16.666667], [16.666667], [16.666667], [16.666667], [34.375], [34.375], [34.375], [34.375], [34.375], [35.172414], [35.172414], [35.172414], [14.0], [14.0], [14.0], [8.695652], [18.4], [16.153846], [16.153846], [16.153846], [16.153846], [16.153846], [27.575758], [27.575758], [14.761905], [14.761905], [14.761905], [22.1875], [22.1875], [59.333333], [10.0], [10.0], [10.0], [27.666667], [20.0], [20.0], [20.0], [118.0], [118.0], [118.0], [118.0], [10.0], [10.0], [19.677419], [19.677419], [19.677419], [19.677419], [40.769231], [40.769231], [40.769231], [19.393939], [19.393939], [19.393939], [19.393939], [7.173913], [7.173913], [7.173913], [31.052632], [31.052632], [10.0], [25.714286], [25.714286], [25.714286], [25.714286], [28.888889], [40.526316], [31.363636], [31.363636], [31.363636], [31.363636], [21.5], [21.5], [21.5], [92.857143], [92.857143], [92.857143], [8.148148], [10.0], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [83.994253], [83.994253], [83.994253], [83.994253], [83.994253], [83.994253], [3.873874], [3.873874], [3.873874], [3.873874], [5.416667], [5.416667], [5.416667], [9.904762], [9.904762], [9.904762], [9.904762], [9.904762], [34.0], [26.956522], [26.956522], [26.956522], [3.714286], [3.714286], [3.714286], [3.714286], [7.037037], [7.037037], [7.037037], [7.037037], [24.615385], [16.25], [16.25], [27.142857], [27.142857], [4.736842], [4.736842], [4.736842], [10.0], [23.0], [23.0], [34.583333], [34.583333], [10.0], [10.0], [34.705882], [19.02439], [19.02439], [19.02439], [19.02439], [19.02439], [40.0], [40.0], [40.0], [44.444444], [44.444444], [44.444444], [44.444444], [44.444444], [20.416667], [20.416667], [20.416667], [5.30303], [5.30303], [5.30303], [6.666667], [6.666667]]\n",
    "\n",
    "a = np.array(a)\n",
    "\n",
    "print(sum(a))\n",
    "print(np.std(a))\n",
    "print(np.mean(a))\n",
    "a = (a-np.mean(a)) / np.std(a)\n",
    "a = a.tolist()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 2, 3, 4, 5])\n",
    "b = torch.Tensor([[1], [2], [3], [4], [5]])\n",
    "\n",
    "a = torch.Tensor([3])\n",
    "\n",
    "print(a**b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "omnettest"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
