{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from library.ipynb\n",
      "importing Jupyter notebook from job.ipynb\n",
      "importing Jupyter notebook from jobqueue.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from dataplane.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history3\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "\n",
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet, TopKPooling, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sys import getsizeof\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/stacking/node10/model6\")\n",
    "\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(pathName)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "modelNum = 0\n",
    "availableJobNum = 0\n",
    "nodeNum = 0\n",
    "jobWaitingLength = 0\n",
    "adjacency = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda:0\n",
      "<class 'torch.device'>\n",
      "<built-in method cuda of Tensor object at 0x0000019E18D8D850>\n",
      "<class 'torch.device'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.get_device_name(device = 0))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "\n",
    "print(type(device))\n",
    "\n",
    "kkkkk = torch.Tensor(1)\n",
    "\n",
    "kkkkk = kkkkk.cuda()\n",
    "\n",
    "print(kkkkk.cuda)\n",
    "print(type(kkkkk.device))\n",
    "print(kkkkk.device == device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.0003 # 0.0003\n",
    "gamma           = 0.95\n",
    "entropy_weight  = 0.01 # 0.001\n",
    "val_loss_coef = 0.8\n",
    "eps = 1.0\n",
    "eps_gamma = 0.99\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.8\n",
    "eps_clip      = 0.2\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100\n",
    "hidden_feature_num = 0\n",
    "\n",
    "job_generate_rate = 0.003\n",
    "\n",
    "train_cycle = 300\n",
    "is_train = True\n",
    "train_quitient = 0\n",
    "\n",
    "replay_buffer_size = 10000\n",
    "history_learning_time = 4\n",
    "\n",
    "if not is_train:\n",
    "    train_quitient = train_cycle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os.path\n",
    "# import os\n",
    "# FIFO_FILENAME = './fifo-test'\n",
    "\n",
    "# if not os.path.exists(FIFO_FILENAME):\n",
    "#     os.mkfifo(FIFO_FILENAME)\n",
    "#     if os.path.exists(FIFO_FILENAME):\n",
    "#         fp_fifo = open(FIFO_FILENAME, \"rw\")\n",
    "\n",
    "# for i in range(128):\n",
    "#     fp_fifo.write(\"Hello,MakeCode\\n\")\n",
    "#     fp_fifo.write(\"\")\n",
    "\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "# PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\omnetPipe2\"\n",
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\worker4\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"modelNum\":\"6\", \"availableJobNum\":\"5\", \"nodeNum\":\"10\", \"jobWaitingQueueLength\":\"15\", \"adjacencyList\":\"[[0,1,0,2,0,3,0,6,0,8,1,3,1,4,1,5,1,7,1,8,3,4,4,5,4,6,4,7,5,9,7,9,],[1,0,2,0,3,0,6,0,8,0,3,1,4,1,5,1,7,1,8,1,4,3,5,4,6,4,7,4,9,5,9,7,]]\", \"episode_length\":\"100\", \"job_generate_rate\":\"30\", \"node_capacity\":\"0.030000, 0.090000\"}\n",
      "네트워크 초기화 완료\n",
      "\n",
      "노드 개수 : 10\n",
      "네트워크 최대 job 개수 : 5\n",
      "job 대기 가능 개수 : 15\n",
      "최대 subtask 개수 : 6\n",
      "인접 리스트 : [[0, 1, 0, 2, 0, 3, 0, 6, 0, 8, 1, 3, 1, 4, 1, 5, 1, 7, 1, 8, 3, 4, 4, 5, 4, 6, 4, 7, 5, 9, 7, 9], [1, 0, 2, 0, 3, 0, 6, 0, 8, 0, 3, 1, 4, 1, 5, 1, 7, 1, 8, 1, 4, 3, 5, 4, 6, 4, 7, 4, 9, 5, 9, 7]]\n",
      "node_feature_num : 60\n",
      "queue_feature_num : 240\n",
      "episode_length : 100\n",
      "node_capacity : 0.030000, 0.090000\n",
      "entropy_weight : 0.01\n",
      "reward_weight : 0.03333333333333333\n",
      "job_generate_rate : 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "\n",
    "print(initial_message)\n",
    "\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "node_capacity = networkInfo['node_capacity']\n",
    "job_generate_rate = networkInfo['job_generate_rate']\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "\n",
    "reward_weight = 1/(modelNum * 5)\n",
    "\n",
    "sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "info = f\"\"\"\n",
    "노드 개수 : {nodeNum}\n",
    "네트워크 최대 job 개수 : {availableJobNum}\n",
    "job 대기 가능 개수 : {jobWaitingLength}\n",
    "최대 subtask 개수 : {modelNum}\n",
    "인접 리스트 : {adjacency}\n",
    "node_feature_num : {node_feature_num}\n",
    "queue_feature_num : {queue_feature_num}\n",
    "episode_length : {episode_length}\n",
    "node_capacity : {node_capacity}\n",
    "entropy_weight : {entropy_weight}\n",
    "reward_weight : {reward_weight}\n",
    "job_generate_rate : {job_generate_rate}\n",
    "\"\"\"\n",
    "print(info)\n",
    "\n",
    "with open(f'{pathName}/info.txt', 'w') as f:\n",
    "    f.write(f'{info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = [[] for i in range(modelNum + 1)]\n",
    "        self.history = [[] for i in range(modelNum + 1)]\n",
    "\n",
    "        self.x_mean = 0\n",
    "        self.x2_mean = 0\n",
    "        self.sd = 0\n",
    "        self.reward_sample_num = 0\n",
    "\n",
    "        self.step = 0\n",
    "        #self.entropy_normalize_weight = 1/torch.log(torch.tensor(nodeNum))\n",
    "        self.entropy_normalize_weight = 1\n",
    "\n",
    "        \n",
    "        # print(f\"self.adjacency : \", self.adjacency.shape)\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * int(node_feature_num//2)), nn.ReLU())\n",
    "        self.pi_s_ecc1 = NNConv(node_feature_num, int(node_feature_num//2), self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_mlp2 = nn.Sequential(nn.Linear(1, int(node_feature_num//2) * int(node_feature_num//2)), nn.ReLU())\n",
    "        self.pi_s_ecc2 = NNConv(int(node_feature_num//2), int(node_feature_num//2), self.pi_mlp2, aggr='mean')\n",
    "\n",
    "        self.pi_graph_u_net1 = GraphUNet(int(node_feature_num//2), 50, int(node_feature_num//2), 3, 0.8)\n",
    "        self.pi_graph_u_net2 = GraphUNet(int(node_feature_num//2), 50, int(node_feature_num//2), 3, 0.8)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear((int(node_feature_num//2) + queue_feature_num) * 2, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(hidden_feature_num, hidden_feature_num), # 38부터 적용되는 코드. 38이전은 두 줄 지우고 실행하면됨.\n",
    "            #nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear((int(node_feature_num//2) + queue_feature_num) * 2, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num), # 38부터 적용되는 코드. 38이전은 두 줄 지우고 실행하면됨.\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        self.heads = [{\"mu\" : nn.Linear(hidden_feature_num, 1), \"std\" : nn.Linear(hidden_feature_num, 1), \"v\" : nn.Linear(hidden_feature_num, 1)} for i in range(modelNum)]\n",
    "        self.void = nn.Linear(hidden_feature_num, 2)\n",
    "        self.void_v = nn.Linear(hidden_feature_num, 1)\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        # self.pi_prob_fc = nn.Linear(hidden_feature_num, nodeNum + 1) # nodeNum + voidAction\n",
    "\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(hidden_feature_num, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state, sub_index):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature_1, link_feature_1, adjacency_1 = data[0].x, data[0].edge_attr, data[0].edge_index\n",
    "        node_feature_2, link_feature_2, adjacency_2 = data[1].x, data[1].edge_attr, data[1].edge_index\n",
    "\n",
    "        job_waiting_feature_1 = job_waiting_feature[0]\n",
    "        job_waiting_feature_2 = job_waiting_feature[1]\n",
    "\n",
    "        if job_waiting_feature_2.device == 'cuda':\n",
    "            link_feature_1 = link_feature_1.cuda()\n",
    "            adjacency_1 = adjacency_1.cuda()\n",
    "\n",
    "            link_feature_2 = link_feature_2.cuda()\n",
    "            adjacency_2 = adjacency_2.cuda()\n",
    "\n",
    "        \n",
    "        # =========================================================================\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc1(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net1(node_feature_1, adjacency_1))\n",
    "\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc2(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net2(node_feature_1, adjacency_1))\n",
    "\n",
    "        data_num_1 = len(node_feature_1) // nodeNum\n",
    "        \n",
    "        readout_1 = global_mean_pool(node_feature_1, data[0].batch)\n",
    "\n",
    "        concat_1 = torch.cat([readout_1, job_waiting_feature_1], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc1(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net1(node_feature_2, adjacency_2))\n",
    "\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc2(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net2(node_feature_2, adjacency_2))\n",
    "\n",
    "        data_num_2 = len(node_feature_2) // nodeNum\n",
    "        \n",
    "        readout_2 = global_mean_pool(node_feature_2, data[0].batch)\n",
    "\n",
    "        concat_2 = torch.cat([readout_2, job_waiting_feature_2], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        concat = torch.cat([concat_1, concat_2], dim=1)\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        # print(feature_extract)\n",
    "\n",
    "        mu = self.heads[sub_index][\"mu\"](feature_extract)\n",
    "        std = F.softplus(self.heads[sub_index][\"std\"](feature_extract))\n",
    "        \n",
    "        dist = Normal(mu, std)\n",
    "        entropy = dist.entropy()\n",
    "\n",
    "        void_output = self.void(feature_extract)\n",
    "        void_prob = F.softmax(void_output, dim=1)\n",
    "        \n",
    "        return dist, entropy, void_prob\n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state, sub_index):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature_1, link_feature_1, adjacency_1 = data[0].x, data[0].edge_attr, data[0].edge_index\n",
    "        node_feature_2, link_feature_2, adjacency_2 = data[1].x, data[1].edge_attr, data[1].edge_index\n",
    "\n",
    "        job_waiting_feature_1 = job_waiting_feature[0]\n",
    "        job_waiting_feature_2 = job_waiting_feature[1]\n",
    "\n",
    "        if job_waiting_feature_2.device == 'cuda':\n",
    "            link_feature_1 = link_feature_1.cuda()\n",
    "            adjacency_1 = adjacency_1.cuda()\n",
    "\n",
    "            link_feature_2 = link_feature_2.cuda()\n",
    "            adjacency_2 = adjacency_2.cuda()\n",
    "\n",
    "        \n",
    "        # =========================================================================\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc1(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net1(node_feature_1, adjacency_1))\n",
    "\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc2(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net2(node_feature_1, adjacency_1))\n",
    "\n",
    "        data_num_1 = len(node_feature_1) // nodeNum\n",
    "        \n",
    "        readout_1 = global_mean_pool(node_feature_1, data[0].batch)\n",
    "\n",
    "        concat_1 = torch.cat([readout_1, job_waiting_feature_1], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc1(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net1(node_feature_2, adjacency_2))\n",
    "\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc2(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net2(node_feature_2, adjacency_2))\n",
    "\n",
    "        data_num_2 = len(node_feature_2) // nodeNum\n",
    "        \n",
    "        readout_2 = global_mean_pool(node_feature_2, data[0].batch)\n",
    "\n",
    "        concat_2 = torch.cat([readout_2, job_waiting_feature_2], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        concat = torch.cat([concat_1, concat_2], dim=1)\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        value = self.v_value_fc(feature_extract)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def v_void(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature_1, link_feature_1, adjacency_1 = data[0].x, data[0].edge_attr, data[0].edge_index\n",
    "        node_feature_2, link_feature_2, adjacency_2 = data[1].x, data[1].edge_attr, data[1].edge_index\n",
    "\n",
    "        job_waiting_feature_1 = job_waiting_feature[0]\n",
    "        job_waiting_feature_2 = job_waiting_feature[1]\n",
    "\n",
    "        if job_waiting_feature_2.device == 'cuda':\n",
    "            link_feature_1 = link_feature_1.cuda()\n",
    "            adjacency_1 = adjacency_1.cuda()\n",
    "\n",
    "            link_feature_2 = link_feature_2.cuda()\n",
    "            adjacency_2 = adjacency_2.cuda()\n",
    "\n",
    "        \n",
    "        # =========================================================================\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc1(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net1(node_feature_1, adjacency_1))\n",
    "\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc2(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net2(node_feature_1, adjacency_1))\n",
    "\n",
    "        data_num_1 = len(node_feature_1) // nodeNum\n",
    "        \n",
    "        readout_1 = global_mean_pool(node_feature_1, data[0].batch)\n",
    "\n",
    "        concat_1 = torch.cat([readout_1, job_waiting_feature_1], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc1(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net1(node_feature_2, adjacency_2))\n",
    "\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc2(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net2(node_feature_2, adjacency_2))\n",
    "\n",
    "        data_num_2 = len(node_feature_2) // nodeNum\n",
    "        \n",
    "        readout_2 = global_mean_pool(node_feature_2, data[0].batch)\n",
    "\n",
    "        concat_2 = torch.cat([readout_2, job_waiting_feature_2], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        concat = torch.cat([concat_1, concat_2], dim=1)\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        value = self.void_v(feature_extract)\n",
    "\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition, index):\n",
    "        self.data[index].append(transition)\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "        \n",
    "    def return_link_dict(sel, ad):\n",
    "        result = {}\n",
    "        for i in range(len(ad[0])//2):\n",
    "            result[f'{ad[0][2*i]}{ad[0][2 *(i)+1]}'] = i\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def return_new_mean(self, mean, num, new_data):\n",
    "        result = (mean * num + new_data) / (num + 1)\n",
    "        return result\n",
    "        \n",
    "    def return_new_sd(self, square_mean, mean):\n",
    "        result = (square_mean - mean**2)**0.5\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def return_normalize_reward(self, reward):\n",
    "        self.x_mean = self.return_new_mean(self.x_mean, self.reward_sample_num, reward)\n",
    "        self.x2_mean = self.return_new_mean(self.x2_mean, self.reward_sample_num, reward**2)\n",
    "        self.sd = self.return_new_sd(self.x2_mean, self.x_mean)\n",
    "        \n",
    "        if self.sd == 0:\n",
    "                z_normalized = 0\n",
    "        else:  \n",
    "            z_normalized = (reward - self.x_mean) / self.sd\n",
    "\n",
    "        self.reward_sample_num += 1\n",
    "        \n",
    "        return z_normalized\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "    \"\"\"\n",
    "    temp_history.append([\n",
    "        [network_state[0], network_state[1]], \n",
    "        [job_waiting_state[0], job_waiting_state[1]], \n",
    "        node, 0, \n",
    "        [network_state[0], network_state[1]], \n",
    "        [job_waiting_state[0], job_waiting_state[1]],\n",
    "        void[isVoid].item(), \n",
    "        0, isVoid]\n",
    "    )\n",
    "\n",
    "    temp_history.append([\n",
    "        [network_state[0], network_state[1]], \n",
    "        [job_waiting_state[0], job_waiting_state[1]], \n",
    "        node, -1, \n",
    "        [-1, -1], \n",
    "        [-1, -1],\n",
    "        dist.log_prob(node), \n",
    "        0, False]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def make_batch_history(self, sub_index):\n",
    "        sample_index = torch.randperm(len(self.history[sub_index]))[:batch_size]\n",
    "\n",
    "        if len(sample_index) == 0:\n",
    "            return -1\n",
    "\n",
    "        sampled_data = []\n",
    "        for sample_idx in sample_index:\n",
    "            sampled_data.append(self.history[sub_index][sample_idx])\n",
    "\n",
    "        b_size = len(sample_index)\n",
    "\n",
    "        network_1_lst = [None] * b_size\n",
    "        network_2_lst = [None] * b_size\n",
    "        next_network_1_lst = [None] * b_size\n",
    "        next_network_2_lst = [None] * b_size\n",
    "        job_waiting_1_lst = [None] * b_size\n",
    "        job_waiting_2_lst = [None] * b_size\n",
    "        next_job_waiting_1_lst = [None] * b_size\n",
    "        next_job_waiting_2_lst = [None] * b_size\n",
    "        a_lst = [None] * b_size\n",
    "        r_lst = [None] * b_size\n",
    "        prob_a_lst = [None] * b_size\n",
    "        sojourn_time_lst = [None] * b_size\n",
    "        isVoid_lst = [None] * b_size\n",
    "        subtask_index_lst = [None] * b_size\n",
    "\n",
    "        for idx, transition in enumerate(sampled_data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, sojourn_time, isVoid, subtask_index = transition\n",
    "\n",
    "            r_lst[idx] = [r*reward_weight]\n",
    "\n",
    "            network_1_lst[idx] = network[0]\n",
    "            network_2_lst[idx] = network[1]\n",
    "\n",
    "            job_waiting_1_lst[idx] = job_waiting[0].tolist()\n",
    "            job_waiting_2_lst[idx] = job_waiting[1].tolist()\n",
    "\n",
    "            a_lst[idx] = [a]\n",
    "\n",
    "            next_network_1_lst[idx] = nxt_network[0]\n",
    "            next_network_2_lst[idx] = nxt_network[1]\n",
    "            \n",
    "            next_job_waiting_1_lst[idx] = nxt_job_waiting[0].tolist()\n",
    "            next_job_waiting_2_lst[idx] = nxt_job_waiting[1].tolist()\n",
    "\n",
    "            prob_a_lst[idx] = [prob_a]\n",
    "            sojourn_time_lst[idx] = [sojourn_time * 5]\n",
    "\n",
    "            isVoid_lst[idx] = [isVoid]\n",
    "            subtask_index_lst[idx] = [subtask_index]\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data[sub_index] = self.data[sub_index][batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data[sub_index] = []\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_1_loader = DataLoader(network_1_lst, batch_size=len(network_1_lst))\n",
    "        next_network_1_loader = DataLoader(next_network_1_lst, batch_size=len(network_1_lst))\n",
    "        network_1_batch = next(iter(network_1_loader))\n",
    "        next_network_1_batch = next(iter(next_network_1_loader))\n",
    "\n",
    "        network_2_loader = DataLoader(network_2_lst, batch_size=len(network_2_lst))\n",
    "        next_network_2_loader = DataLoader(next_network_2_lst, batch_size=len(network_2_lst))\n",
    "        network_2_batch = next(iter(network_2_loader))\n",
    "        next_network_2_batch = next(iter(next_network_2_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting_1 = torch.squeeze(torch.tensor(np.array(job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        job_waiting_2 = torch.squeeze(torch.tensor(np.array(job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "\n",
    "        next_job_waiting_1 = torch.squeeze(torch.tensor(np.array(next_job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        next_job_waiting_2 = torch.squeeze(torch.tensor(np.array(next_job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "        \n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        isVoid_lst = torch.tensor(isVoid_lst, dtype=torch.bool)\n",
    "\n",
    "        subtask_index_lst = torch.tensor(subtask_index_lst, dtype=torch.bool)\n",
    "        \n",
    "        # self.data = []\n",
    "        return [network_1_batch, network_2_batch], [job_waiting_1, job_waiting_2], a, r, [next_network_1_batch, next_network_2_batch], [next_job_waiting_1, next_job_waiting_2], prob_a, sojourn_time, isVoid_lst, subtask_index_lst\n",
    "\n",
    "        \n",
    "    def make_batch(self, isFirst, sub_index):\n",
    "\n",
    "        if(isFirst):\n",
    "            self.data[sub_index] = self.data[sub_index][::-1]\n",
    "\n",
    "        b_size = min(len(self.data[sub_index]), batch_size)\n",
    "\n",
    "        network_1_lst = [None] * b_size\n",
    "        network_2_lst = [None] * b_size\n",
    "        next_network_1_lst = [None] * b_size\n",
    "        next_network_2_lst = [None] * b_size\n",
    "        job_waiting_1_lst = [None] * b_size\n",
    "        job_waiting_2_lst = [None] * b_size\n",
    "        next_job_waiting_1_lst = [None] * b_size\n",
    "        next_job_waiting_2_lst = [None] * b_size\n",
    "        a_lst = [None] * b_size\n",
    "        r_lst = [None] * b_size\n",
    "        prob_a_lst = [None] * b_size\n",
    "        sojourn_time_lst = [None] * b_size\n",
    "        isVoid_lst = [None] * b_size\n",
    "        subtask_index_lst = [None] * b_size\n",
    "\n",
    "        for idx, transition in enumerate(self.data[sub_index]):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, sojourn_time, isVoid, subtask_index = transition\n",
    "\n",
    "            r_lst[idx] = [r*reward_weight]\n",
    "\n",
    "            network_1_lst[idx] = network[0]\n",
    "            network_2_lst[idx] = network[1]\n",
    "\n",
    "            job_waiting_1_lst[idx] = job_waiting[0].tolist()\n",
    "            job_waiting_2_lst[idx] = job_waiting[1].tolist()\n",
    "\n",
    "            a_lst[idx] = [a]\n",
    "\n",
    "            next_network_1_lst[idx] = nxt_network[0]\n",
    "            next_network_2_lst[idx] = nxt_network[1]\n",
    "            \n",
    "            next_job_waiting_1_lst[idx] = nxt_job_waiting[0].tolist()\n",
    "            next_job_waiting_2_lst[idx] = nxt_job_waiting[1].tolist()\n",
    "\n",
    "            prob_a_lst[idx] = [prob_a]\n",
    "            sojourn_time_lst[idx] = [sojourn_time * 5]\n",
    "\n",
    "            isVoid_lst[idx] = [isVoid]\n",
    "            subtask_index_lst[idx] = [subtask_index]\n",
    "\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data[sub_index] = self.data[sub_index][batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data[sub_index] = []\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_1_loader = DataLoader(network_1_lst, batch_size=len(network_1_lst))\n",
    "        next_network_1_loader = DataLoader(next_network_1_lst, batch_size=len(network_1_lst))\n",
    "        network_1_batch = next(iter(network_1_loader))\n",
    "        next_network_1_batch = next(iter(next_network_1_loader))\n",
    "\n",
    "        network_2_loader = DataLoader(network_2_lst, batch_size=len(network_2_lst))\n",
    "        next_network_2_loader = DataLoader(next_network_2_lst, batch_size=len(network_2_lst))\n",
    "        network_2_batch = next(iter(network_2_loader))\n",
    "        next_network_2_batch = next(iter(next_network_2_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting_1 = torch.squeeze(torch.tensor(np.array(job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        job_waiting_2 = torch.squeeze(torch.tensor(np.array(job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "\n",
    "        next_job_waiting_1 = torch.squeeze(torch.tensor(np.array(next_job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        next_job_waiting_2 = torch.squeeze(torch.tensor(np.array(next_job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "        \n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        isVoid_lst = torch.tensor(isVoid_lst, dtype=torch.bool)\n",
    "        \n",
    "        subtask_index_lst = torch.tensor(subtask_index_lst, dtype=torch.bool)\n",
    "        \n",
    "        # self.data = []\n",
    "        return [network_1_batch, network_2_batch], [job_waiting_1, job_waiting_2], a, r, [next_network_1_batch, next_network_2_batch], [next_job_waiting_1, next_job_waiting_2], prob_a, sojourn_time, isVoid_lst, subtask_index_lst\n",
    "    \n",
    "    def train_net(self):\n",
    "        self = self.cuda()\n",
    "        for head in self.heads:\n",
    "            for module in head.values():\n",
    "                module.cuda()\n",
    "        \n",
    "        pre_advantage = [0.0] * (modelNum + 1)\n",
    "        for index in range(modelNum + 1):\n",
    "            first = True\n",
    "            while len(self.data[index]) > 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, old_log_prob, sojourn_time, isVoid, subtask_index = self.make_batch(first, index)\n",
    "\n",
    "                if first:\n",
    "                    first = False\n",
    "                \n",
    "                network_batch_1 = network_batch[0].clone().cuda()\n",
    "                network_batch_2 = network_batch[1].clone().cuda()\n",
    "\n",
    "                network_batch = [network_batch_1, network_batch_2]\n",
    "\n",
    "                job_waiting_1 = job_waiting[0].clone().cuda()\n",
    "                job_waiting_2 = job_waiting[1].clone().cuda()\n",
    "\n",
    "                job_waiting = [job_waiting_1, job_waiting_2]\n",
    "\n",
    "                a = a.clone().cuda()\n",
    "                r = r.clone().cuda()\n",
    "\n",
    "                next_network_batch_1 = next_network_batch[0].clone().cuda()\n",
    "                next_network_batch_2 = next_network_batch[1].clone().cuda()\n",
    "\n",
    "                next_network_batch = [next_network_batch_1, next_network_batch_2]\n",
    "\n",
    "                next_job_waiting_1 = next_job_waiting[0].clone().cuda()\n",
    "                next_job_waiting_2 = next_job_waiting[1].clone().cuda()\n",
    "\n",
    "                next_job_waiting = [next_job_waiting_1, next_job_waiting_2]\n",
    "\n",
    "                old_log_prob = old_log_prob.clone().cuda()\n",
    "                sojourn_time = sojourn_time.clone().cuda()\n",
    "                gamma_gpu = torch.Tensor([gamma]).clone().cuda()\n",
    "                isVoid = isVoid.clone().cuda()\n",
    "                subtask_index = subtask_index.clone().cuda()\n",
    "\n",
    "                for i in range(2):\n",
    "                    \n",
    "                    # void\n",
    "                    if index == modelNum:\n",
    "                        next_v = self.v_void([next_network_batch, next_job_waiting])\n",
    "                        cur_v = self.v_void([network_batch, job_waiting])\n",
    "                    else:\n",
    "                        next_v = self.v([next_network_batch, next_job_waiting], index)\n",
    "                        cur_v = self.v([network_batch, job_waiting], index)\n",
    "\n",
    "                    td_target = r + gamma_gpu * next_v\n",
    "                    \n",
    "                    delta = td_target - cur_v\n",
    "                    delta = delta.detach().to('cpu').numpy()\n",
    "                    advantage_lst = []\n",
    "                    advantage = pre_advantage[index]\n",
    "                    for i, delta_t in enumerate(delta):\n",
    "                        advantage = gamma_gpu * lmbda * advantage + delta_t[0]\n",
    "                        advantage_lst.append([advantage])\n",
    "                    # advantage_lst.reverse()\n",
    "                    advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "\n",
    "                    pre_advantage[index] = advantage\n",
    "\n",
    "                    v_loss = val_loss_coef * F.smooth_l1_loss(cur_v , td_target.detach())\n",
    "                    #print(\"v_loss\", v_loss)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    v_loss.mean().backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    new_log_prob = []\n",
    "                    current_entropy = []\n",
    "\n",
    "\n",
    "                    dist, current_entropy, _ = self.pi([network_batch, job_waiting], index)\n",
    "\n",
    "                    current_entropy = self.entropy_normalize_weight * current_entropy\n",
    "\n",
    "                    # true가 valid action\n",
    "\n",
    "                    #outputs = outputs.clone().detach()\n",
    "\n",
    "                    new_log_prob = dist.log_prob(a)\n",
    "\n",
    "                    # pi_a = pi.gather(1,a)\n",
    "                    ratio = torch.exp(torch.min(torch.tensor(88), new_log_prob - old_log_prob))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                    surr1 = ratio * advantage_lst\n",
    "                    surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage_lst\n",
    "                    pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    pi_loss.mean().backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "    def train_net_history(self):\n",
    "\n",
    "        self = self.cuda()\n",
    "        for head in self.heads:\n",
    "            for module in head.values():\n",
    "                module.cuda()\n",
    "        # pre_advantage = 0.0\n",
    "        if len(self.history) <= 0:\n",
    "            return\n",
    "\n",
    "\n",
    "        for index in range(modelNum + 1):\n",
    "            for i in range(history_learning_time):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                temp_return = self.make_batch_history(index)\n",
    "\n",
    "                if temp_return == -1:\n",
    "                    return\n",
    "\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, old_log_prob, sojourn_time, isVoid, subtask_index = temp_return\n",
    "\n",
    "                if len(network_batch) == 0:\n",
    "                    return\n",
    "                \n",
    "                network_batch_1 = network_batch[0].clone().cuda()\n",
    "                network_batch_2 = network_batch[1].clone().cuda()\n",
    "\n",
    "                network_batch = [network_batch_1, network_batch_2]\n",
    "\n",
    "                job_waiting_1 = job_waiting[0].clone().cuda()\n",
    "                job_waiting_2 = job_waiting[1].clone().cuda()\n",
    "\n",
    "                job_waiting = [job_waiting_1, job_waiting_2]\n",
    "\n",
    "                a = a.clone().cuda()\n",
    "                r = r.clone().cuda()\n",
    "\n",
    "                next_network_batch_1 = next_network_batch[0].clone().cuda()\n",
    "                next_network_batch_2 = next_network_batch[1].clone().cuda()\n",
    "\n",
    "                next_network_batch = [next_network_batch_1, next_network_batch_2]\n",
    "\n",
    "                next_job_waiting_1 = next_job_waiting[0].clone().cuda()\n",
    "                next_job_waiting_2 = next_job_waiting[1].clone().cuda()\n",
    "\n",
    "                next_job_waiting = [next_job_waiting_1, next_job_waiting_2]\n",
    "\n",
    "                old_log_prob = old_log_prob.clone().cuda()\n",
    "                sojourn_time = sojourn_time.clone().cuda()\n",
    "                gamma_gpu = torch.Tensor([gamma]).clone().cuda()\n",
    "                isVoid = isVoid.clone().cuda()\n",
    "                subtask_index = subtask_index.clone().cuda()\n",
    "\n",
    "                if index == modelNum:\n",
    "                    next_v = self.v_void([next_network_batch, next_job_waiting])\n",
    "                    cur_v = self.v_void([network_batch, job_waiting])\n",
    "                else:\n",
    "                    next_v = self.v([next_network_batch, next_job_waiting], index)\n",
    "                    cur_v = self.v([network_batch, job_waiting], index)\n",
    "\n",
    "                td_target = r + gamma_gpu * next_v\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(cur_v , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                td_target = r + gamma_gpu * next_v\n",
    "                delta = td_target - cur_v\n",
    "\n",
    "                new_log_prob = []\n",
    "                current_entropy = []\n",
    "\n",
    "\n",
    "                dist, current_entropy, _ = self.pi([network_batch, job_waiting], index)\n",
    "\n",
    "                current_entropy = self.entropy_normalize_weight * current_entropy\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                #outputs = outputs.clone().detach()\n",
    "\n",
    "                new_log_prob = dist.log_prob(a)\n",
    "\n",
    "                # pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.min(torch.tensor(88), new_log_prob - old_log_prob))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * delta.detach()\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * delta.detach()\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-10 12:04:03 AM] training....\n",
      "[2023-01-10 12:04:25 AM] training complete\n",
      "[2023-01-10 12:04:25 AM] training replay buffer....\n",
      "[2023-01-10 12:05:05 AM] training complete\n",
      "[2023-01-10 12:05:11 AM] training....\n",
      "[2023-01-10 12:05:27 AM] training complete\n",
      "[2023-01-10 12:05:27 AM] training replay buffer....\n",
      "[2023-01-10 12:06:08 AM] training complete\n",
      "[2023-01-10 12:06:14 AM] training....\n",
      "[2023-01-10 12:06:34 AM] training complete\n",
      "[2023-01-10 12:06:34 AM] training replay buffer....\n",
      "[2023-01-10 12:07:11 AM] training complete\n",
      "[2023-01-10 12:07:16 AM] training....\n",
      "[2023-01-10 12:07:36 AM] training complete\n",
      "[2023-01-10 12:07:36 AM] training replay buffer....\n",
      "[2023-01-10 12:08:16 AM] training complete\n",
      "[2023-01-10 12:08:21 AM] training....\n",
      "[2023-01-10 12:08:40 AM] training complete\n",
      "[2023-01-10 12:08:40 AM] training replay buffer....\n",
      "[2023-01-10 12:09:24 AM] training complete\n",
      "[2023-01-10 12:09:30 AM] training....\n",
      "[2023-01-10 12:09:45 AM] training complete\n",
      "[2023-01-10 12:09:45 AM] training replay buffer....\n",
      "[2023-01-10 12:10:29 AM] training complete\n",
      "[2023-01-10 12:10:34 AM] training....\n",
      "[2023-01-10 12:10:54 AM] training complete\n",
      "[2023-01-10 12:10:54 AM] training replay buffer....\n",
      "[2023-01-10 12:11:33 AM] training complete\n",
      "[2023-01-10 12:11:40 AM] training....\n",
      "[2023-01-10 12:12:00 AM] training complete\n",
      "[2023-01-10 12:12:00 AM] training replay buffer....\n",
      "[2023-01-10 12:12:40 AM] training complete\n",
      "[2023-01-10 12:12:44 AM] training....\n",
      "[2023-01-10 12:13:04 AM] training complete\n",
      "[2023-01-10 12:13:04 AM] training replay buffer....\n",
      "[2023-01-10 12:13:48 AM] training complete\n",
      "[2023-01-10 12:13:54 AM] training....\n",
      "[2023-01-10 12:14:12 AM] training complete\n",
      "[2023-01-10 12:14:12 AM] training replay buffer....\n",
      "[2023-01-10 12:14:56 AM] training complete\n",
      "[2023-01-10 12:15:01 AM] training....\n",
      "[2023-01-10 12:15:18 AM] training complete\n",
      "[2023-01-10 12:15:18 AM] training replay buffer....\n",
      "[2023-01-10 12:16:01 AM] training complete\n",
      "[2023-01-10 12:16:07 AM] training....\n",
      "[2023-01-10 12:16:27 AM] training complete\n",
      "[2023-01-10 12:16:27 AM] training replay buffer....\n",
      "[2023-01-10 12:17:06 AM] training complete\n",
      "[2023-01-10 12:17:13 AM] training....\n",
      "[2023-01-10 12:17:33 AM] training complete\n",
      "[2023-01-10 12:17:33 AM] training replay buffer....\n",
      "[2023-01-10 12:18:12 AM] training complete\n",
      "[2023-01-10 12:18:18 AM] training....\n",
      "[2023-01-10 12:18:38 AM] training complete\n",
      "[2023-01-10 12:18:38 AM] training replay buffer....\n",
      "[2023-01-10 12:19:22 AM] training complete\n",
      "[2023-01-10 12:19:28 AM] training....\n",
      "[2023-01-10 12:19:48 AM] training complete\n",
      "[2023-01-10 12:19:48 AM] training replay buffer....\n",
      "[2023-01-10 12:20:32 AM] training complete\n",
      "[2023-01-10 12:20:37 AM] training....\n",
      "[2023-01-10 12:20:52 AM] training complete\n",
      "[2023-01-10 12:20:52 AM] training replay buffer....\n",
      "[2023-01-10 12:21:36 AM] training complete\n",
      "[2023-01-10 12:21:41 AM] training....\n",
      "[2023-01-10 12:22:01 AM] training complete\n",
      "[2023-01-10 12:22:01 AM] training replay buffer....\n",
      "[2023-01-10 12:22:41 AM] training complete\n",
      "[2023-01-10 12:22:46 AM] training....\n",
      "[2023-01-10 12:23:06 AM] training complete\n",
      "[2023-01-10 12:23:06 AM] training replay buffer....\n",
      "[2023-01-10 12:23:45 AM] training complete\n",
      "[2023-01-10 12:23:50 AM] training....\n",
      "[2023-01-10 12:24:09 AM] training complete\n",
      "[2023-01-10 12:24:09 AM] training replay buffer....\n",
      "[2023-01-10 12:24:53 AM] training complete\n",
      "[2023-01-10 12:24:59 AM] training....\n",
      "[2023-01-10 12:25:16 AM] training complete\n",
      "[2023-01-10 12:25:16 AM] training replay buffer....\n",
      "[2023-01-10 12:26:00 AM] training complete\n",
      "[2023-01-10 12:26:03 AM] training....\n",
      "[2023-01-10 12:26:23 AM] training complete\n",
      "[2023-01-10 12:26:23 AM] training replay buffer....\n",
      "[2023-01-10 12:27:02 AM] training complete\n",
      "[2023-01-10 12:27:08 AM] training....\n",
      "[2023-01-10 12:27:28 AM] training complete\n",
      "[2023-01-10 12:27:28 AM] training replay buffer....\n",
      "[2023-01-10 12:28:07 AM] training complete\n",
      "[2023-01-10 12:28:14 AM] training....\n",
      "[2023-01-10 12:28:35 AM] training complete\n",
      "[2023-01-10 12:28:35 AM] training replay buffer....\n",
      "[2023-01-10 12:29:15 AM] training complete\n",
      "[2023-01-10 12:29:21 AM] training....\n",
      "[2023-01-10 12:29:42 AM] training complete\n",
      "[2023-01-10 12:29:42 AM] training replay buffer....\n",
      "[2023-01-10 12:30:26 AM] training complete\n",
      "[2023-01-10 12:30:32 AM] training....\n",
      "[2023-01-10 12:30:47 AM] training complete\n",
      "[2023-01-10 12:30:47 AM] training replay buffer....\n",
      "[2023-01-10 12:31:31 AM] training complete\n",
      "[2023-01-10 12:31:37 AM] training....\n",
      "[2023-01-10 12:31:56 AM] training complete\n",
      "[2023-01-10 12:31:56 AM] training replay buffer....\n",
      "[2023-01-10 12:32:37 AM] training complete\n",
      "[2023-01-10 12:32:43 AM] training....\n",
      "[2023-01-10 12:33:03 AM] training complete\n",
      "[2023-01-10 12:33:03 AM] training replay buffer....\n",
      "[2023-01-10 12:33:42 AM] training complete\n",
      "[2023-01-10 12:33:49 AM] training....\n",
      "[2023-01-10 12:34:09 AM] training complete\n",
      "[2023-01-10 12:34:09 AM] training replay buffer....\n",
      "[2023-01-10 12:34:48 AM] training complete\n",
      "[2023-01-10 12:34:55 AM] training....\n",
      "[2023-01-10 12:35:15 AM] training complete\n",
      "[2023-01-10 12:35:15 AM] training replay buffer....\n",
      "[2023-01-10 12:36:00 AM] training complete\n",
      "[2023-01-10 12:36:06 AM] training....\n",
      "[2023-01-10 12:36:26 AM] training complete\n",
      "[2023-01-10 12:36:26 AM] training replay buffer....\n",
      "[2023-01-10 12:37:10 AM] training complete\n",
      "[2023-01-10 12:37:16 AM] training....\n",
      "[2023-01-10 12:37:31 AM] training complete\n",
      "[2023-01-10 12:37:31 AM] training replay buffer....\n",
      "[2023-01-10 12:38:16 AM] training complete\n",
      "[2023-01-10 12:38:22 AM] training....\n",
      "[2023-01-10 12:38:42 AM] training complete\n",
      "[2023-01-10 12:38:42 AM] training replay buffer....\n",
      "[2023-01-10 12:39:19 AM] training complete\n",
      "[2023-01-10 12:39:25 AM] training....\n",
      "[2023-01-10 12:39:45 AM] training complete\n",
      "[2023-01-10 12:39:45 AM] training replay buffer....\n",
      "[2023-01-10 12:40:24 AM] training complete\n",
      "[2023-01-10 12:40:30 AM] training....\n",
      "[2023-01-10 12:40:50 AM] training complete\n",
      "[2023-01-10 12:40:50 AM] training replay buffer....\n",
      "[2023-01-10 12:41:34 AM] training complete\n",
      "[2023-01-10 12:41:40 AM] training....\n",
      "[2023-01-10 12:42:01 AM] training complete\n",
      "[2023-01-10 12:42:01 AM] training replay buffer....\n",
      "[2023-01-10 12:42:46 AM] training complete\n",
      "[2023-01-10 12:42:52 AM] training....\n",
      "[2023-01-10 12:43:06 AM] training complete\n",
      "[2023-01-10 12:43:06 AM] training replay buffer....\n",
      "[2023-01-10 12:43:50 AM] training complete\n",
      "[2023-01-10 12:43:56 AM] training....\n",
      "[2023-01-10 12:44:16 AM] training complete\n",
      "[2023-01-10 12:44:16 AM] training replay buffer....\n",
      "[2023-01-10 12:44:55 AM] training complete\n",
      "[2023-01-10 12:45:01 AM] training....\n",
      "[2023-01-10 12:45:20 AM] training complete\n",
      "[2023-01-10 12:45:20 AM] training replay buffer....\n",
      "[2023-01-10 12:46:00 AM] training complete\n",
      "[2023-01-10 12:46:08 AM] training....\n",
      "[2023-01-10 12:46:28 AM] training complete\n",
      "[2023-01-10 12:46:28 AM] training replay buffer....\n",
      "[2023-01-10 12:47:11 AM] training complete\n",
      "[2023-01-10 12:47:18 AM] training....\n",
      "[2023-01-10 12:47:38 AM] training complete\n",
      "[2023-01-10 12:47:38 AM] training replay buffer....\n",
      "[2023-01-10 12:48:23 AM] training complete\n",
      "[2023-01-10 12:48:29 AM] training....\n",
      "[2023-01-10 12:48:44 AM] training complete\n",
      "[2023-01-10 12:48:44 AM] training replay buffer....\n",
      "[2023-01-10 12:49:28 AM] training complete\n",
      "[2023-01-10 12:49:34 AM] training....\n",
      "[2023-01-10 12:49:54 AM] training complete\n",
      "[2023-01-10 12:49:54 AM] training replay buffer....\n",
      "[2023-01-10 12:50:33 AM] training complete\n",
      "[2023-01-10 12:50:39 AM] training....\n",
      "[2023-01-10 12:50:58 AM] training complete\n",
      "[2023-01-10 12:50:58 AM] training replay buffer....\n",
      "[2023-01-10 12:51:38 AM] training complete\n",
      "[2023-01-10 12:51:44 AM] training....\n",
      "[2023-01-10 12:52:04 AM] training complete\n",
      "[2023-01-10 12:52:04 AM] training replay buffer....\n",
      "[2023-01-10 12:52:46 AM] training complete\n",
      "[2023-01-10 12:52:52 AM] training....\n",
      "[2023-01-10 12:53:12 AM] training complete\n",
      "[2023-01-10 12:53:12 AM] training replay buffer....\n",
      "[2023-01-10 12:53:56 AM] training complete\n",
      "[2023-01-10 12:54:02 AM] training....\n",
      "[2023-01-10 12:54:17 AM] training complete\n",
      "[2023-01-10 12:54:17 AM] training replay buffer....\n",
      "[2023-01-10 12:55:02 AM] training complete\n",
      "[2023-01-10 12:55:09 AM] training....\n",
      "[2023-01-10 12:55:27 AM] training complete\n",
      "[2023-01-10 12:55:27 AM] training replay buffer....\n",
      "[2023-01-10 12:56:08 AM] training complete\n",
      "[2023-01-10 12:56:15 AM] training....\n",
      "[2023-01-10 12:56:35 AM] training complete\n",
      "[2023-01-10 12:56:35 AM] training replay buffer....\n",
      "[2023-01-10 12:57:15 AM] training complete\n",
      "[2023-01-10 12:57:21 AM] training....\n",
      "[2023-01-10 12:57:41 AM] training complete\n",
      "[2023-01-10 12:57:41 AM] training replay buffer....\n",
      "[2023-01-10 12:58:20 AM] training complete\n",
      "[2023-01-10 12:58:27 AM] training....\n",
      "[2023-01-10 12:58:47 AM] training complete\n",
      "[2023-01-10 12:58:47 AM] training replay buffer....\n",
      "[2023-01-10 12:59:32 AM] training complete\n",
      "[2023-01-10 12:59:38 AM] training....\n",
      "[2023-01-10 12:59:58 AM] training complete\n",
      "[2023-01-10 12:59:58 AM] training replay buffer....\n",
      "[2023-01-10 01:00:43 AM] training complete\n",
      "[2023-01-10 01:00:51 AM] training....\n",
      "[2023-01-10 01:01:05 AM] training complete\n",
      "[2023-01-10 01:01:05 AM] training replay buffer....\n",
      "[2023-01-10 01:01:50 AM] training complete\n",
      "[2023-01-10 01:01:56 AM] training....\n",
      "[2023-01-10 01:02:16 AM] training complete\n",
      "[2023-01-10 01:02:16 AM] training replay buffer....\n",
      "[2023-01-10 01:02:56 AM] training complete\n",
      "[2023-01-10 01:03:01 AM] training....\n",
      "[2023-01-10 01:03:21 AM] training complete\n",
      "[2023-01-10 01:03:21 AM] training replay buffer....\n",
      "[2023-01-10 01:04:01 AM] training complete\n",
      "[2023-01-10 01:04:08 AM] training....\n",
      "[2023-01-10 01:04:28 AM] training complete\n",
      "[2023-01-10 01:04:28 AM] training replay buffer....\n",
      "[2023-01-10 01:05:08 AM] training complete\n",
      "[2023-01-10 01:05:15 AM] training....\n",
      "[2023-01-10 01:05:35 AM] training complete\n",
      "[2023-01-10 01:05:35 AM] training replay buffer....\n",
      "[2023-01-10 01:06:20 AM] training complete\n",
      "[2023-01-10 01:06:27 AM] training....\n",
      "[2023-01-10 01:06:46 AM] training complete\n",
      "[2023-01-10 01:06:46 AM] training replay buffer....\n",
      "[2023-01-10 01:07:30 AM] training complete\n",
      "[2023-01-10 01:07:37 AM] training....\n",
      "[2023-01-10 01:07:52 AM] training complete\n",
      "[2023-01-10 01:07:52 AM] training replay buffer....\n",
      "[2023-01-10 01:08:36 AM] training complete\n",
      "[2023-01-10 01:08:42 AM] training....\n",
      "[2023-01-10 01:09:03 AM] training complete\n",
      "[2023-01-10 01:09:03 AM] training replay buffer....\n",
      "[2023-01-10 01:09:43 AM] training complete\n",
      "[2023-01-10 01:09:48 AM] training....\n",
      "[2023-01-10 01:10:07 AM] training complete\n",
      "[2023-01-10 01:10:07 AM] training replay buffer....\n",
      "[2023-01-10 01:10:46 AM] training complete\n",
      "[2023-01-10 01:10:53 AM] training....\n",
      "[2023-01-10 01:11:13 AM] training complete\n",
      "[2023-01-10 01:11:13 AM] training replay buffer....\n",
      "[2023-01-10 01:11:57 AM] training complete\n",
      "[2023-01-10 01:12:04 AM] training....\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (21, 1)) of distribution Normal(loc: torch.Size([21, 1]), scale: torch.Size([21, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\agent2_connect - graphunet - model sharing - state stacking - repa.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 367>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=363'>364</a>\u001b[0m                 torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), modelPathName)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=366'>367</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=367'>368</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\agent2_connect - graphunet - model sharing - state stacking - repa.ipynb Cell 11\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=344'>345</a>\u001b[0m time_string \u001b[39m=\u001b[39m strftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mI:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS \u001b[39m\u001b[39m%\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m, tm)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=345'>346</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mtime_string\u001b[39m}\u001b[39;00m\u001b[39m] training....\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=346'>347</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain_net()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=347'>348</a>\u001b[0m tm \u001b[39m=\u001b[39m localtime(time\u001b[39m.\u001b[39mtime())\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=348'>349</a>\u001b[0m time_string \u001b[39m=\u001b[39m strftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mI:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS \u001b[39m\u001b[39m%\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m, tm)\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\agent2_connect - graphunet - model sharing - state stacking - repa.ipynb Cell 11\u001b[0m in \u001b[0;36mactor_network.train_net\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=548'>549</a>\u001b[0m new_log_prob \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=549'>550</a>\u001b[0m current_entropy \u001b[39m=\u001b[39m []\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=552'>553</a>\u001b[0m dist, current_entropy, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpi([network_batch, job_waiting], index)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=554'>555</a>\u001b[0m current_entropy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentropy_normalize_weight \u001b[39m*\u001b[39m current_entropy\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=556'>557</a>\u001b[0m \u001b[39m# true가 valid action\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=557'>558</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=558'>559</a>\u001b[0m \u001b[39m#outputs = outputs.clone().detach()\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\agent2_connect - graphunet - model sharing - state stacking - repa.ipynb Cell 11\u001b[0m in \u001b[0;36mactor_network.pi\u001b[1;34m(self, state, sub_index)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads[sub_index][\u001b[39m\"\u001b[39m\u001b[39mmu\u001b[39m\u001b[39m\"\u001b[39m](feature_extract)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m std \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftplus(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads[sub_index][\u001b[39m\"\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m\"\u001b[39m](feature_extract))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m dist \u001b[39m=\u001b[39m Normal(mu, std)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m entropy \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mentropy()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/agent2_connect%20-%20graphunet%20-%20model%20sharing%20-%20state%20stacking%20-%20repa.ipynb#X13sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m void_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvoid(feature_extract)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\torch\\distributions\\normal.py:50\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[1;32m---> 50\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\torch\\distributions\\distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[0;32m     54\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m---> 55\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     56\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (21, 1)) of distribution Normal(loc: torch.Size([21, 1]), scale: torch.Size([21, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global adjacency, entropy_weight, learning_rate, eps\n",
    "    model = actor_network()\n",
    "    # model.load_state_dict(torch.load(\"./history30/model.pth\")) # -1 , +1\n",
    "    # model.load_state_dict(torch.load(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/stacking/history18/model.pth\")) # original\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "\n",
    "    adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    # node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    # node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    # link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    # job_waiting_state = env.get_job_waiting_vector()\n",
    "\n",
    "    # network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "\n",
    "    # for each time step\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = [[] for i in range(modelNum + 1)]\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(nodeNum)]\n",
    "    void_selected_num = 0\n",
    "\n",
    "    pre_state_1 = {}\n",
    "    pre_state_2 = {}\n",
    "    \n",
    "    while True:\n",
    "        model = model.to('cpu')\n",
    "        for head in model.heads:\n",
    "            for module in head.values():\n",
    "                module.to('cpu')\n",
    "\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state_1 = getOmnetMessage()\n",
    "            state_2 = getOmnetMessage()\n",
    "\n",
    "            if len(state_1) == 0:\n",
    "                state_1 = state_2\n",
    "            \n",
    "\n",
    "            if len(pre_state_1) == 0: # action 시작\n",
    "                state_1 = json.loads(state_1) # state 받았으므로 action 하면됨.\n",
    "                state_2 = json.loads(state_2) # state 받았으므로 action 하면됨.\n",
    "                \n",
    "            else:\n",
    "                state_1 = json.loads(state_1)\n",
    "                pre_state_1['jobWaiting'] = state_1['jobWaiting']\n",
    "                pre_state_1['sojournTime'] = state_1['sojournTime']\n",
    "                state_1 = pre_state_1\n",
    "\n",
    "                state_2 = json.loads(state_2)\n",
    "                pre_state_2['jobWaiting'] = state_2['jobWaiting']\n",
    "                pre_state_2['sojournTime'] = state_2['sojournTime']\n",
    "                state_2 = pre_state_2\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            node_waiting_state_1 = np.array(eval(str(state_1['nodeState'])))\n",
    "            node_processing_state_1 = np.array(eval(state_1['nodeProcessing']))\n",
    "            link_state_1 = np.array(eval(state_1['linkWaiting']))\n",
    "            job_waiting_state_1 = np.array(eval(state_1['jobWaiting']))\n",
    "            activated_job_list_1 = eval(state_1['activatedJobList'])\n",
    "            isAction_1 = int(state_1['isAction'])\n",
    "            reward_1 = float(state_1['reward'])\n",
    "            averageLatency_1 = float(state_1['averageLatency'])\n",
    "            completeJobNum_1 = int(state_1['completeJobNum'])\n",
    "            sojournTime_1 = float(state_1['sojournTime'])\n",
    "\n",
    "            node_waiting_state_2 = np.array(eval(str(state_2['nodeState'])))\n",
    "            node_processing_state_2 = np.array(eval(state_2['nodeProcessing']))\n",
    "            link_state_2 = np.array(eval(state_2['linkWaiting']))\n",
    "            job_waiting_state_2 = np.array(eval(state_2['jobWaiting']))\n",
    "            activated_job_list_2 = eval(state_2['activatedJobList'])\n",
    "            isAction_2 = int(state_2['isAction'])\n",
    "            reward_2 = float(state_2['reward'])\n",
    "            averageLatency_2 = float(state_2['averageLatency'])\n",
    "            completeJobNum_2 = int(state_2['completeJobNum'])\n",
    "            sojournTime_2 = float(state_2['sojournTime'])\n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum_2 ,step)\n",
    "            \n",
    "\n",
    "            # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            if averageLatency_2 == -1:\n",
    "                reward_2 = 0\n",
    "            else:\n",
    "                reward_2 = completeJobNum_2\n",
    "            \n",
    "            writer.add_scalar(\"Reward/train\", reward_2, step)\n",
    "                \n",
    "            first_sample = True\n",
    "            \n",
    "\n",
    "            model.history = model.history[-replay_buffer_size:]\n",
    "\n",
    "            ################ s' 받아와서 저장 #################\n",
    "            if is_train and len(pre_state_1) == 0:\n",
    "                for index, historys in enumerate(temp_history):\n",
    "                    for history in historys:\n",
    "                        history[3] = reward_2\n",
    "                        history[4] = network_state\n",
    "                        history[5] = job_waiting_state\n",
    "                        model.history[index].append(history)\n",
    "                        model.put_data(history, index)\n",
    "            ###################################################\n",
    "\n",
    "            temp_history = [[] for i in range(modelNum + 1)]\n",
    "\n",
    "            job_index = int(state_2['jobIndex'])\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "\n",
    "            node_state_1 = np.concatenate((node_waiting_state_1,node_processing_state_1) ,axis = 1)\n",
    "            node_state_1 = torch.tensor(node_state_1, dtype=torch.float)\n",
    "\n",
    "            node_state_2 = np.concatenate((node_waiting_state_2,node_processing_state_2) ,axis = 1)\n",
    "            node_state_2 = torch.tensor(node_state_2, dtype=torch.float)\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state_1 = torch.tensor(link_state_1, dtype=torch.float)\n",
    "            link_state_2 = torch.tensor(link_state_2, dtype=torch.float)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = collections.deque()\n",
    "            for job in job_waiting_state_2:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state_1 = torch.tensor(job_waiting_state_1, dtype=torch.float).view(1, -1)\n",
    "            job_waiting_state_2 = torch.tensor(job_waiting_state_2, dtype=torch.float).view(1, -1)\n",
    "            # print(job_waiting_state)\n",
    "\n",
    "            network_state_1 = Data(x=node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "            network_state_2 = Data(x=node_state_2, edge_attr=link_state_2, edge_index=adjacency)\n",
    "\n",
    "            network_state = [network_state_1, network_state_2]\n",
    "            job_waiting_state = [job_waiting_state_1, job_waiting_state_2]\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            pre_state_1 = state_1\n",
    "            pre_state_2 = state_2\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward_2\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward_2 - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(nodeNum):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(nodeNum)] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward_2 != 0:\n",
    "                    with torch.no_grad():\n",
    "                        writer.add_scalar(\"Value/train\", torch.mean(model.v([network_state, job_waiting_state], 0)[0]), step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction_2 = False\n",
    "                \n",
    "\n",
    "            if isAction_2:\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = 1\n",
    "                for i in range(nodeNum):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[nodeNum:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dist, entropy, void = model.pi([network_state, job_waiting_state], 0)\n",
    "\n",
    "                writer.add_scalar(\"Entropy/train\", entropy, step)\n",
    "                #print(f'prob : {prob}')\n",
    "\n",
    "                # isVoid = F.sigmoid(dists[modelNum].sample())\n",
    "\n",
    "                m = Categorical(void) \n",
    "                isVoid = m.sample().item()\n",
    "                #print(f'node : {node}')\n",
    "                \n",
    "                # void action 실험용\n",
    "                # node = nodeNum \n",
    "                \n",
    "                \n",
    "                # void action 뽑으면 void만 업데이트\n",
    "                if isVoid and not scheduling_start: \n",
    "                    temp_history[modelNum].append([\n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]], \n",
    "                        isVoid, 0, \n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]],\n",
    "                        torch.log(void[0][isVoid]), \n",
    "                        0, isVoid, -1]\n",
    "                    )\n",
    "\n",
    "                    sendOmnetMessage(\"void\")\n",
    "\n",
    "                    #print(\"action finish.\")\n",
    "                    \n",
    "                    if getOmnetMessage() == \"ok\":\n",
    "                        void_selected_num += 1\n",
    "\n",
    "                else:\n",
    "                    scheduling_start = True\n",
    "\n",
    "                if scheduling_start:\n",
    "\n",
    "                    for index in range(modelNum):\n",
    "\n",
    "                        \n",
    "                        dist, entropy, _ = model.pi([network_state, job_waiting_state], index)\n",
    "\n",
    "                        if random.random() > eps:\n",
    "                            node = torch.floor(torch.sigmoid(dist.sample()) * nodeNum)\n",
    "                        else:\n",
    "                            node = torch.tensor(random.randint(0, nodeNum - 1))\n",
    "\n",
    "                        temp_history[index].append([\n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]], \n",
    "                        node, -1, \n",
    "                        [-1, -1], \n",
    "                        [-1, -1],\n",
    "                        dist.log_prob(node), \n",
    "                        0, False, index]\n",
    "                        )\n",
    "\n",
    "                        offloading_vector.append(int(node.item()))\n",
    "                        node_selected_num[int(node.item())] += 1\n",
    "                    \n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    \n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            pre_state_1 = {}\n",
    "            pre_state_2 = {}\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "            average_latency = float(episodic_reward['averageLatency'])\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            writer.add_scalar(\"CompleteNum/train\", complete_num, episode)\n",
    "            writer.add_scalar(\"averageLatency/train\", average_latency ,episode)\n",
    "\n",
    "            episode += 1\n",
    "            eps = eps * eps_gamma\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            # learning_rate *= 0.995\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = pathName + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            average_reward = 0\n",
    "            average_reward_num = 0\n",
    "\n",
    "            # entropy_weight *= 0.99\n",
    "\n",
    "\n",
    "            # if len(model.data) > 0 and step % train_cycle == train_quitient:\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training....\")\n",
    "            #     model.train_net()\n",
    "            #     modelPathName = pathName + \"/model.pth\"\n",
    "            #     torch.save(model.state_dict(), modelPathName)\n",
    "            #     writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            #     average_reward = 0\n",
    "            #     average_reward_num = 0\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training complete\")\n",
    "            if is_train:\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training replay buffer....\")\n",
    "                model.train_net_history()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    modelPathName = pathName + f\"/model_{episode}.pth\"\n",
    "                \n",
    "                modelPathName = pathName + \"/model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Normal(torch.tensor(2), torch.tensor(0.1))\n",
    "entropy = dist.entropy()\n",
    "print(entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('omnetTest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12c9d4573c12dd45eabff63c44badb6fcd2b70b85de11a1a1b2c23254cbf5db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
