{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from library.ipynb\n",
      "importing Jupyter notebook from job.ipynb\n",
      "importing Jupyter notebook from jobqueue.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from dataplane.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history122\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "\n",
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet, TopKPooling, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sys import getsizeof\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True) # nan 오류 잡는 함수\n",
    "\n",
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent\")\n",
    "\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(pathName)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "modelNum = 0\n",
    "availableJobNum = 0\n",
    "nodeNum = 0\n",
    "jobWaitingLength = 0\n",
    "adjacency = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda:0\n",
      "<class 'torch.device'>\n",
      "<built-in method cuda of Tensor object at 0x000001AF05B92F20>\n",
      "<class 'torch.device'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.get_device_name(device = 0))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "\n",
    "print(type(device))\n",
    "\n",
    "kkkkk = torch.Tensor(1)\n",
    "\n",
    "kkkkk = kkkkk.cuda()\n",
    "\n",
    "print(kkkkk.cuda)\n",
    "print(type(kkkkk.device))\n",
    "print(kkkkk.device == device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.0004\n",
    "gamma           = 0.995\n",
    "entropy_weight  = 0.003\n",
    "val_loss_coef = 1.0\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.8\n",
    "eps_clip      = 0.2\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100\n",
    "hidden_feature_num = 0\n",
    "\n",
    "job_generate_rate = 0.003\n",
    "pooling_rate = 0.8\n",
    "\n",
    "train_cycle = 300\n",
    "is_train = False\n",
    "train_quitient = 0\n",
    "\n",
    "replay_buffer_size = 6000\n",
    "history_learning_time = 1\n",
    "\n",
    "if not is_train:\n",
    "    train_quitient = train_cycle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os.path\n",
    "# import os\n",
    "# FIFO_FILENAME = './fifo-test'\n",
    "\n",
    "# if not os.path.exists(FIFO_FILENAME):\n",
    "#     os.mkfifo(FIFO_FILENAME)\n",
    "#     if os.path.exists(FIFO_FILENAME):\n",
    "#         fp_fifo = open(FIFO_FILENAME, \"rw\")\n",
    "\n",
    "# for i in range(128):\n",
    "#     fp_fifo.write(\"Hello,MakeCode\\n\")\n",
    "#     fp_fifo.write(\"\")\n",
    "\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\omnetPipe\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 초기화 완료\n",
      "노드 개수 : 7\n",
      "네트워크 최대 job 개수 : 20\n",
      "job 대기 가능 개수 : 15\n",
      "최대 subtask 개수 : 5\n",
      "인접 리스트 : [[0, 3, 1, 3, 1, 4, 1, 5, 2, 3, 2, 4, 2, 5, 3, 4, 3, 6, 4, 5, 4, 6, 5, 6], [3, 0, 3, 1, 4, 1, 5, 1, 3, 2, 4, 2, 5, 2, 4, 3, 6, 3, 5, 4, 6, 4, 6, 5]]\n",
      "node_feature_num : 200\n",
      "queue_feature_num : 180\n",
      "episode_length : 100\n"
     ]
    }
   ],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "\n",
    "sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "print(f\"노드 개수 : {nodeNum}\")\n",
    "print(f\"네트워크 최대 job 개수 : {availableJobNum}\")\n",
    "print(f\"job 대기 가능 개수 : {jobWaitingLength}\")\n",
    "print(f\"최대 subtask 개수 : {modelNum}\")\n",
    "print(f\"인접 리스트 : {adjacency}\")\n",
    "print(f\"node_feature_num : {node_feature_num}\")\n",
    "print(f\"queue_feature_num : {queue_feature_num}\")\n",
    "print(f\"episode_length : {episode_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = []\n",
    "        self.history = []\n",
    "\n",
    "        self.x_mean = 0\n",
    "        self.x2_mean = 0\n",
    "        self.sd = 0\n",
    "        self.reward_sample_num = 0\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        \n",
    "        # print(f\"self.adjacency : \", self.adjacency.shape)\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * int(node_feature_num)), nn.ReLU())\n",
    "        self.pi_s_ecc1 = NNConv(node_feature_num, node_feature_num, self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_mlp2 = nn.Sequential(nn.Linear(1, node_feature_num  * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_ecc2 = NNConv(node_feature_num, node_feature_num, self.pi_mlp2, aggr='mean')\n",
    "        \n",
    "        self.pi_conv1 = GCNConv(node_feature_num,  int(node_feature_num//2))\n",
    "        self.pi_conv2 = GCNConv(int(node_feature_num//2),  int(node_feature_num//4))\n",
    "\n",
    "        self.pi_pooling1 = TopKPooling(int(node_feature_num//2), pooling_rate)\n",
    "        self.pi_pooling2 = TopKPooling(int(node_feature_num//4), pooling_rate)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear(int(node_feature_num//4) + queue_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(hidden_feature_num, nodeNum + 1) # nodeNum + voidAction\n",
    "\n",
    "        self.v_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.v_s_ecc1 = NNConv(node_feature_num, node_feature_num, self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.v_mlp2 = nn.Sequential(nn.Linear(1, node_feature_num  * node_feature_num), nn.ReLU())\n",
    "        self.v_s_ecc2 = NNConv(node_feature_num, node_feature_num, self.pi_mlp2, aggr='mean')\n",
    "        \n",
    "        self.v_conv1 = GCNConv(node_feature_num,  int(node_feature_num//2))\n",
    "        self.v_conv2 = GCNConv( int(node_feature_num//2),  int(node_feature_num//4))\n",
    "\n",
    "        self.v_pooling1 = TopKPooling(int(node_feature_num//2), pooling_rate)\n",
    "        self.v_pooling2 = TopKPooling(int(node_feature_num//4), pooling_rate)\n",
    "\n",
    "        self.v_backbone = nn.Sequential(\n",
    "            nn.Linear(int(node_feature_num//4) + queue_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(hidden_feature_num, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "\n",
    "        output = torch.tensor([[1] * (nodeNum + 1)], dtype=torch.float)\n",
    "\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # 아래는 엔트로피 구하는 과정\n",
    "        log_prob = 1\n",
    "        entropy = - 1\n",
    "\n",
    "        \n",
    "        \n",
    "        return prob, entropy, output\n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state):\n",
    "        value = torch.tensor([1], dtype=torch.float)\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "        \n",
    "    def return_link_dict(sel, ad):\n",
    "        result = {}\n",
    "        for i in range(len(ad[0])//2):\n",
    "            result[f'{ad[0][2*i]}{ad[0][2 *(i)+1]}'] = i\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def return_new_mean(self, mean, num, new_data):\n",
    "        result = (mean * num + new_data) / (num + 1)\n",
    "        return result\n",
    "        \n",
    "    def return_new_sd(self, square_mean, mean):\n",
    "        result = (square_mean - mean**2)**0.5\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def return_normalize_reward(self, reward):\n",
    "        self.x_mean = self.return_new_mean(self.x_mean, self.reward_sample_num, reward)\n",
    "        self.x2_mean = self.return_new_mean(self.x2_mean, self.reward_sample_num, reward**2)\n",
    "        self.sd = self.return_new_sd(self.x2_mean, self.x_mean)\n",
    "        \n",
    "        if self.sd == 0:\n",
    "            z_normalized = 0\n",
    "        else:  \n",
    "            z_normalized = (reward - self.x_mean) / self.sd\n",
    "\n",
    "        self.reward_sample_num += 1\n",
    "        \n",
    "        return z_normalized\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "        \n",
    "    def make_batch_history(self):\n",
    "        sample_index = torch.randperm(len(self.history))[:batch_size]\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        sampled_data = []\n",
    "        for sample_idx in sample_index:\n",
    "            sampled_data.append(self.history[sample_idx])\n",
    "            \n",
    "\n",
    "        for idx, transition in enumerate(sampled_data):\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "            \n",
    "\n",
    "            r_lst.append([r/100.0])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask_lst\n",
    "\n",
    "\n",
    "        \n",
    "    def make_batch(self):\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "                \n",
    "            r_lst.append([r/100.0])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask_lst\n",
    "\n",
    "    def make_first_batch(self):\n",
    "        self.data = self.data[::-1]\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "\n",
    "            r_lst.append([r/100.0])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_net(self):\n",
    "        self = self.cuda()\n",
    "        first = True\n",
    "        pre_advantage = 0.0\n",
    "        while len(self.data) > 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            if first:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_first_batch()\n",
    "                first = False\n",
    "            else:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_batch()\n",
    "            \n",
    "            network_batch = network_batch.clone().cuda()\n",
    "            job_waiting = job_waiting.clone().cuda()\n",
    "            a = a.clone().cuda()\n",
    "            r = r.clone().cuda()\n",
    "            next_network_batch = next_network_batch.clone().cuda()\n",
    "            next_job_waiting = next_job_waiting.clone().cuda()\n",
    "            prob_a = prob_a.clone().cuda()\n",
    "            entropy = entropy.clone().cuda()\n",
    "            sojourn_time = sojourn_time.clone().cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).clone().cuda()\n",
    "            action_mask = action_mask.clone().cuda()\n",
    "\n",
    "            for i in range(3):\n",
    "                \n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                \n",
    "                delta = td_target - self.v([network_batch, job_waiting])\n",
    "                delta = delta.detach().to('cpu').numpy()\n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = (gamma_gpu**sojourn_time[i][0]) * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                # advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "\n",
    "                pre_advantage = advantage\n",
    "\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(self.v([network_batch, job_waiting]) , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                pi, current_entropy, outputs = self.pi([network_batch, job_waiting])\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                #outputs = outputs.clone().detach()\n",
    "\n",
    "\n",
    "                outputs = outputs * action_mask\n",
    "\n",
    "                exp_sum = torch.sum(torch.exp(outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "                exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "                outputs_a = outputs.gather(1,a)\n",
    "                pi_a = torch.exp(outputs_a) / exp_sum\n",
    "\n",
    "                # pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * advantage_lst\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage_lst\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train_net_history(self):\n",
    "        self = self.cuda()\n",
    "        # pre_advantage = 0.0\n",
    "        for i in range(history_learning_time):\n",
    "            torch.cuda.empty_cache()\n",
    "            network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_batch_history()\n",
    "\n",
    "            \n",
    "            network_batch = network_batch.clone().detach().cuda()\n",
    "            job_waiting = job_waiting.clone().cuda()\n",
    "            a = a.clone().cuda()\n",
    "            r = r.clone().cuda()\n",
    "            next_network_batch = next_network_batch.clone().detach().cuda()\n",
    "            next_job_waiting = next_job_waiting.clone().cuda()\n",
    "            prob_a = prob_a.clone().cuda()\n",
    "            entropy = entropy.clone().cuda()\n",
    "            sojourn_time = sojourn_time.clone().cuda()\n",
    "            \n",
    "            action_mask = action_mask.clone().cuda()\n",
    "            \n",
    "\n",
    "            gamma_gpu = torch.Tensor([gamma]).cuda()\n",
    "\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                \n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                    \n",
    "                # delta = td_target - self.v([network_batch, job_waiting])\n",
    "                \"\"\"\n",
    "                \n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = (gamma_gpu**sojourn_time[i][0]) * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "\n",
    "                pre_advantage = advantage\n",
    "                \"\"\"\n",
    "\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(self.v([network_batch, job_waiting]) , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                delta = td_target - self.v([network_batch, job_waiting])\n",
    "                # print(delta)\n",
    "\n",
    "                pi, current_entropy, outputs = self.pi([network_batch, job_waiting])\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                outputs = outputs * action_mask\n",
    "\n",
    "                exp_sum = torch.sum(torch.exp(outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "                exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "                outputs_a = outputs.gather(1,a)\n",
    "                pi_a = torch.exp(pi) / exp_sum\n",
    "\n",
    "                pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * delta\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * delta\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global adjacency, entropy_weight, learning_rate\n",
    "    model = actor_network()\n",
    "    # model.load_state_dict(torch.load(\"./history30/model.pth\")) # -1 , +1\n",
    "    # model.load_state_dict(torch.load(\"./history109/max_model.pth\")) # original\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "\n",
    "    adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    # node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    # node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    # link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    # job_waiting_state = env.get_job_waiting_vector()\n",
    "\n",
    "    # network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "\n",
    "    # for each time step\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = []\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(nodeNum)]\n",
    "    void_selected_num = 0\n",
    "\n",
    "    pre_state = {}\n",
    "    \n",
    "    while True:\n",
    "        model = model.to('cpu')\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state = getOmnetMessage()\n",
    "\n",
    "            if len(pre_state) == 0: # action 시작\n",
    "                state = json.loads(state) # state 받았으므로 action 하면됨.\n",
    "                pre_state = state\n",
    "            else:\n",
    "                state = json.loads(state)\n",
    "                pre_state['jobWaiting'] = state['jobWaiting']\n",
    "                pre_state['sojournTime'] = state['sojournTime']\n",
    "                state = pre_state\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            node_waiting_state = np.array(eval(str(state['nodeState'])))\n",
    "            node_processing_state = np.array(eval(state['nodeProcessing']))\n",
    "            link_state = np.array(eval(state['linkWaiting']))\n",
    "            job_waiting_state = np.array(eval(state['jobWaiting']))\n",
    "            activated_job_list = eval(state['activatedJobList'])\n",
    "            isAction = int(state['isAction'])\n",
    "            reward = float(state['reward'])\n",
    "            averageLatency = float(state['averageLatency'])\n",
    "            completeJobNum = int(state['completeJobNum'])\n",
    "            sojournTime = float(state['sojournTime'])\n",
    "\n",
    "        \n",
    "            if averageLatency != -1:\n",
    "                writer.add_scalar(\"averageLatency/train\", averageLatency ,step)\n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum ,step)\n",
    "            writer.add_scalar(\"Reward/train\", reward, step)\n",
    "\n",
    "            # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            first_sample = True\n",
    "            if is_train:\n",
    "                if len(temp_history) > 0:\n",
    "                    temp_history[-1][8] = sojournTime\n",
    "                for history in temp_history:\n",
    "                    history[3] = reward/100.0\n",
    "                    model.history.append(history)\n",
    "                    model.put_data(history)\n",
    "\n",
    "            model.history = model.history[-replay_buffer_size:]\n",
    "\n",
    "            temp_history = []\n",
    "\n",
    "            job_index = int(state['jobIndex'])\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "\n",
    "            node_state = np.concatenate((node_waiting_state,node_processing_state) ,axis = 1)\n",
    "            node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = collections.deque()\n",
    "            for job in job_waiting_state:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state = torch.tensor(job_waiting_state, dtype=torch.float).view(1, -1)\n",
    "            # print(job_waiting_state)\n",
    "\n",
    "            network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(nodeNum):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(nodeNum)] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward != 0:\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                    writer.add_scalar(\"Value/train\", model.v([network_state, job_waiting_state])[0], step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction = False\n",
    "                \n",
    "\n",
    "            if isAction:\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = 1\n",
    "                for i in range(nodeNum):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[nodeNum:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "                #print(\"action start.\")\n",
    "                for order in range(len(subtasks)):\n",
    "                    if subtasks[order] == 0:\n",
    "                        break\n",
    "\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                    # print(node_state.shape)\n",
    "                    # print(link_state.shape)\n",
    "                    # print(adjacency.shape)\n",
    "                    # print(job_waiting_state.shape)\n",
    "                    prob, entropy, output = model.pi([network_state, job_waiting_state])\n",
    "                    \n",
    "                    #print(f'prob : {prob}')\n",
    "                    \n",
    "                    m = Categorical(prob) \n",
    "                    node = m.sample().item()\n",
    "                    #print(f'node : {node}')\n",
    "                    \n",
    "                    # void action 실험용\n",
    "                    # node = nodeNum \n",
    "                    \n",
    "                    \n",
    "                    # void action 뽑으면\n",
    "                    if node == nodeNum and not scheduling_start: \n",
    "                        # print(\"void\")\n",
    "                        prob[0] = torch.Tensor([0] * nodeNum + [1.0])\n",
    "                        action_mask = [int(not scheduling_start) if i == node else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, network_state, job_waiting_state, prob[0][node].item(), entropy, 0, action_mask])\n",
    "                        sendOmnetMessage(\"void\")\n",
    "\n",
    "                        #print(\"action finish.\")\n",
    "                        \n",
    "                        if getOmnetMessage() == \"ok\":\n",
    "                            void_selected_num += 1\n",
    "                            \n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        scheduling_start = True\n",
    "\n",
    "                    if scheduling_start:\n",
    "\n",
    "                        prob = torch.Tensor([F.softmax(output[0][:nodeNum], dim=0).tolist()])\n",
    "                        prob = torch.cat([prob[0], torch.tensor([0])]) # void action masking\n",
    "                        prob = torch.Tensor([prob.tolist()]).cuda()\n",
    "\n",
    "\n",
    "\n",
    "                        m = Categorical(prob[0])\n",
    "                        node = m.sample().item()\n",
    "                        \n",
    "\n",
    "                        offloading_vector.append(node)\n",
    "\n",
    "                        node_selected_num[node] += 1\n",
    "                        \n",
    "                        # state transition(환경으로부터 매번 state를 업데이트하는게 아닌, 현재 state를 기반으로 action에 해당하는 waiting만 더해줌).\n",
    "                        # 아래의 구체적인 코드는 이해하시기 보단 OMNeT++에서 받아온 데이터로 새로 짜시는게 빠를 것 같습니다.\n",
    "                        next_node_state = node_state.clone().detach()\n",
    "                        next_job_waiting_state = job_waiting_state.clone().detach()\n",
    "                        # print(next_job_waiting_state)\n",
    "                        \n",
    "                        next_node_state[node][modelNum * job_idx + order] += (subtasks[order]/100) # 100으로 나누는 이유 : 이렇게 해야 액션이 한쪽 노드로 쏠리게끔 학습이 되는 것을 어느정도 방지할 수 있는 것을 확인.\n",
    "                        node_waiting_state[node][modelNum * job_idx + order] += (subtasks[order]/100)\n",
    "\n",
    "                        # 100으로 안나눠주면 너무 큰 값이 state에 추가되어서 inference시 가중치랑 곱해지면서 액션이 한쪽으로 확 쏠리는 걸로 예상됨.\n",
    "                        next_network_state = Data(x=next_node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                        next_job_waiting_state[0][nodeNum + order] = 0\n",
    "\n",
    "                        action_mask = [int(not scheduling_start) if i == nodeNum else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy, 0, action_mask])\n",
    "\n",
    "                        # model.put_data([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy])\n",
    "                        node_state = next_node_state\n",
    "                        job_waiting_state = next_job_waiting_state\n",
    "\n",
    "                        pre_state['nodeState'] = str(node_waiting_state.tolist())\n",
    "\n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    \n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            pre_state = {}\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            \n",
    "\n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "            elapsed_time = float(episodic_reward['currentTime'])\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            writer.add_scalar(\"CompleteNum/train\", complete_num, episode)\n",
    "            writer.add_scalar(\"Elapsed_time/train\", elapsed_time, episode)\n",
    "            episode += 1\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            # learning_rate *= 0.995\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = pathName + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            average_reward = 0\n",
    "            average_reward_num = 0\n",
    "\n",
    "            # entropy_weight *= 0.99\n",
    "\n",
    "\n",
    "            # if len(model.data) > 0 and step % train_cycle == train_quitient:\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training....\")\n",
    "            #     model.train_net()\n",
    "            #     modelPathName = pathName + \"/model.pth\"\n",
    "            #     torch.save(model.state_dict(), modelPathName)\n",
    "            #     writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            #     average_reward = 0\n",
    "            #     average_reward_num = 0\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training complete\")\n",
    "            if is_train:\n",
    "\n",
    "               \n",
    "                \n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training replay buffer....\")\n",
    "                model.train_net_history()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                modelPathName = pathName + \"/model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "omnettest"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
