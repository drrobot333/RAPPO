{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from library.ipynb\n",
      "importing Jupyter notebook from job.ipynb\n",
      "importing Jupyter notebook from jobqueue.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from dataplane.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history101\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "\n",
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet, TopKPooling, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sys import getsizeof\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True) # nan 오류 잡는 함수\n",
    "\n",
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent\")\n",
    "\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(pathName)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "modelNum = 0\n",
    "availableJobNum = 0\n",
    "nodeNum = 0\n",
    "jobWaitingLength = 0\n",
    "adjacency = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda:0\n",
      "<class 'torch.device'>\n",
      "<built-in method cuda of Tensor object at 0x0000013C8D452160>\n",
      "<class 'torch.device'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.get_device_name(device = 0))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "\n",
    "print(type(device))\n",
    "\n",
    "kkkkk = torch.Tensor(1)\n",
    "\n",
    "kkkkk = kkkkk.cuda()\n",
    "\n",
    "print(kkkkk.cuda)\n",
    "print(type(kkkkk.device))\n",
    "print(kkkkk.device == device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.0001\n",
    "gamma           = 0.99\n",
    "entropy_weight  = 0.003\n",
    "val_loss_coef = 1.0\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.8\n",
    "eps_clip      = 0.2\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100\n",
    "hidden_feature_num = 0\n",
    "\n",
    "pooling_rate = 0.8\n",
    "\n",
    "job_generate_rate = 0.003\n",
    "\n",
    "train_cycle = 300\n",
    "is_train = True\n",
    "train_quitient = 0\n",
    "\n",
    "if not is_train:\n",
    "    train_quitient = train_cycle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os.path\n",
    "# import os\n",
    "# FIFO_FILENAME = './fifo-test'\n",
    "\n",
    "# if not os.path.exists(FIFO_FILENAME):\n",
    "#     os.mkfifo(FIFO_FILENAME)\n",
    "#     if os.path.exists(FIFO_FILENAME):\n",
    "#         fp_fifo = open(FIFO_FILENAME, \"rw\")\n",
    "\n",
    "# for i in range(128):\n",
    "#     fp_fifo.write(\"Hello,MakeCode\\n\")\n",
    "#     fp_fifo.write(\"\")\n",
    "\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\omnetPipe\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 초기화 완료\n",
      "노드 개수 : 7\n",
      "네트워크 최대 job 개수 : 20\n",
      "job 대기 가능 개수 : 15\n",
      "최대 subtask 개수 : 5\n",
      "인접 리스트 : [[0, 3, 1, 3, 1, 4, 1, 5, 2, 3, 2, 4, 2, 5, 3, 4, 3, 6, 4, 5, 4, 6, 5, 6], [3, 0, 3, 1, 4, 1, 5, 1, 3, 2, 4, 2, 5, 2, 4, 3, 6, 3, 5, 4, 6, 4, 6, 5]]\n",
      "node_feature_num : 200\n",
      "queue_feature_num : 180\n",
      "episode_length : 20\n"
     ]
    }
   ],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "\n",
    "sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "print(f\"노드 개수 : {nodeNum}\")\n",
    "print(f\"네트워크 최대 job 개수 : {availableJobNum}\")\n",
    "print(f\"job 대기 가능 개수 : {jobWaitingLength}\")\n",
    "print(f\"최대 subtask 개수 : {modelNum}\")\n",
    "print(f\"인접 리스트 : {adjacency}\")\n",
    "print(f\"node_feature_num : {node_feature_num}\")\n",
    "print(f\"queue_feature_num : {queue_feature_num}\")\n",
    "print(f\"episode_length : {episode_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.x_mean = 0\n",
    "        self.x2_mean = 0\n",
    "        self.sd = 0\n",
    "        self.reward_sample_num = 0\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        \n",
    "        # print(f\"self.adjacency : \", self.adjacency.shape)\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_ecc1 = NNConv(node_feature_num, node_feature_num, self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_mlp2 = nn.Sequential(nn.Linear(1, node_feature_num  * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_ecc2 = NNConv(node_feature_num, node_feature_num, self.pi_mlp2, aggr='mean')\n",
    "        \n",
    "        self.pi_conv1 = GCNConv(node_feature_num, node_feature_num)\n",
    "        self.pi_conv2 = GCNConv(node_feature_num, node_feature_num)\n",
    "\n",
    "        self.pi_pooling1 = TopKPooling(node_feature_num, pooling_rate)\n",
    "        self.pi_pooling2 = TopKPooling(node_feature_num, pooling_rate)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear(node_feature_num * 5 + queue_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(hidden_feature_num, nodeNum + 1) # nodeNum + voidAction\n",
    "\n",
    "        self.v_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.v_s_ecc1 = NNConv(node_feature_num, node_feature_num, self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.v_mlp2 = nn.Sequential(nn.Linear(1, node_feature_num  * node_feature_num), nn.ReLU())\n",
    "        self.v_s_ecc2 = NNConv(node_feature_num, node_feature_num, self.pi_mlp2, aggr='mean')\n",
    "        \n",
    "        self.v_conv1 = GCNConv(node_feature_num, node_feature_num)\n",
    "        self.v_conv2 = GCNConv(node_feature_num, node_feature_num)\n",
    "\n",
    "        self.v_pooling1 = TopKPooling(node_feature_num, pooling_rate)\n",
    "        self.v_pooling2 = TopKPooling(node_feature_num, pooling_rate)\n",
    "\n",
    "        self.v_backbone = nn.Sequential(\n",
    "            nn.Linear(node_feature_num * 5 + queue_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(hidden_feature_num, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        if job_waiting_feature.device == device:\n",
    "            link_feature = link_feature.cuda()\n",
    "            adjacency = adjacency.cuda()\n",
    "\n",
    "\n",
    "        readout_lst = []\n",
    "\n",
    "\n",
    "        new_data_batch = None\n",
    "\n",
    "        node_feature = F.relu(self.pi_s_ecc1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.pi_s_ecc2(node_feature, adjacency, link_feature))\n",
    "            \n",
    "        node_feature = F.relu(self.pi_conv1(node_feature, adjacency))\n",
    "\n",
    "        batch_num = (len(node_feature) // nodeNum)\n",
    "\n",
    "        if data.batch != None:\n",
    "            new_data_batch = torch.Tensor([i//nodeNum for i in range(nodeNum * batch_num)]).cuda()\n",
    "            new_data_batch = new_data_batch.type(torch.long)\n",
    "        node_feature, adjacency, _, _, perm, _ = self.pi_pooling1(node_feature, adjacency, None, new_data_batch)\n",
    "        \n",
    "        node_num_after_pooling = int(nodeNum * pooling_rate) + 1 # 올림\n",
    "\n",
    "        node_feature = F.relu(self.pi_conv2(node_feature, adjacency))\n",
    "        if data.batch != None:\n",
    "            new_data_batch = torch.Tensor([i//node_num_after_pooling for i in range(node_num_after_pooling *  batch_num)]).cuda()\n",
    "            new_data_batch = new_data_batch.type(torch.long)\n",
    "\n",
    "        node_feature, adjacency, _, _, perm, _ = self.pi_pooling2(node_feature, adjacency, None, new_data_batch)\n",
    "        \n",
    "        node_feature = node_feature.view(batch_num, -1)\n",
    "\n",
    "        concat = torch.cat([node_feature, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "\n",
    "        concat = F.normalize(concat) # normalize\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        output = self.pi_prob_fc(feature_extract)\n",
    "\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # 아래는 엔트로피 구하는 과정\n",
    "        log_prob = F.log_softmax(output, dim=1)\n",
    "        entropy = - (log_prob * prob).sum(1, keepdim=True)\n",
    "        \n",
    "        return prob, entropy, output\n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        if job_waiting_feature.device == device:\n",
    "            link_feature = link_feature.cuda()\n",
    "            adjacency = adjacency.cuda()\n",
    "\n",
    "        readout_lst = []\n",
    "\n",
    "        new_data_batch = None\n",
    "\n",
    "        node_feature = F.relu(self.v_s_ecc1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.v_s_ecc2(node_feature, adjacency, link_feature))\n",
    "            \n",
    "        node_feature = F.relu(self.v_conv1(node_feature, adjacency))\n",
    "\n",
    "        batch_num = (len(node_feature) // nodeNum)\n",
    "\n",
    "        if data.batch != None:\n",
    "            new_data_batch = torch.Tensor([i//nodeNum for i in range(nodeNum * batch_num)]).cuda()\n",
    "            new_data_batch = new_data_batch.type(torch.long)\n",
    "        node_feature, adjacency, _, _, perm, _ = self.v_pooling1(node_feature, adjacency, None, new_data_batch)\n",
    "        \n",
    "        node_num_after_pooling = int(nodeNum * pooling_rate) + 1 # 올림\n",
    "        \n",
    "        node_feature = F.relu(self.v_conv2(node_feature, adjacency))\n",
    "        if data.batch != None:\n",
    "            new_data_batch = torch.Tensor([i//node_num_after_pooling for i in range(node_num_after_pooling *  batch_num)]).cuda()\n",
    "            new_data_batch = new_data_batch.type(torch.long)\n",
    "\n",
    "        node_feature, adjacency, _, _, perm, _ = self.v_pooling2(node_feature, adjacency, None, new_data_batch)\n",
    "\n",
    "        # print(f\"node_feature_calc : {node_feature.shape}\"\n",
    "\n",
    "        node_feature = node_feature.view(batch_num, -1)\n",
    "\n",
    "        # print(f\"readout_lst : {readout_lst}\")\n",
    "        # print(f\"job_waiting_feature : {job_waiting_feature}\")\n",
    "            \n",
    "        concat = torch.cat([node_feature, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # print(concat.shape)\n",
    "\n",
    "        concat = F.normalize(concat) # normalize\n",
    "\n",
    "        feature_extract = self.v_backbone(concat)\n",
    "\n",
    "        \n",
    "        value = self.v_value_fc(feature_extract) # 앞부분은 pi랑 공유해야 하고, concat -> value_fc를 거치는 것만 다름.\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "        \n",
    "    def return_link_dict(sel, ad):\n",
    "        result = {}\n",
    "        for i in range(len(ad[0])//2):\n",
    "            result[f'{ad[0][2*i]}{ad[0][2 *(i)+1]}'] = i\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def return_new_mean(self, mean, num, new_data):\n",
    "        result = (mean * num + new_data) / (num + 1)\n",
    "        return result\n",
    "        \n",
    "    def return_new_sd(self, square_mean, mean):\n",
    "        result = (square_mean - mean**2)**0.5\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def return_normalize_reward(self, reward):\n",
    "        self.x_mean = self.return_new_mean(self.x_mean, self.reward_sample_num, reward)\n",
    "        self.x2_mean = self.return_new_mean(self.x2_mean, self.reward_sample_num, reward**2)\n",
    "        self.sd = self.return_new_sd(self.x2_mean, self.x_mean)\n",
    "        \n",
    "        if self.sd == 0:\n",
    "            z_normalized = 0\n",
    "        else:  \n",
    "            z_normalized = (reward - self.x_mean) / self.sd\n",
    "\n",
    "        self.reward_sample_num += 1\n",
    "        \n",
    "        return z_normalized\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "        \n",
    "    def make_batch(self):\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "                \n",
    "            r_lst.append([r/100.0])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask_lst\n",
    "\n",
    "    def make_first_batch(self):\n",
    "        self.data = self.data[::-1]\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], [], [], [], [], []\n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "                \n",
    "            r_lst.append([r/100.0])\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting.tolist())\n",
    "            a_lst.append([a])\n",
    "            next_network_lst.append(nxt_network)\n",
    "            next_job_waiting_lst.append(nxt_job_waiting.tolist())\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting = torch.squeeze(torch.tensor(np.array(job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "        next_job_waiting = torch.squeeze(torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float), dim=1)\n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_net(self):\n",
    "        self = self.cuda()\n",
    "        first = True\n",
    "        pre_advantage = 0.0\n",
    "        while len(self.data) > 0:\n",
    "            if first:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_first_batch()\n",
    "                first = False\n",
    "            else:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_batch()\n",
    "            \n",
    "            network_batch = network_batch.cuda()\n",
    "            job_waiting = job_waiting.cuda()\n",
    "            a = a.cuda()\n",
    "            r = r.cuda()\n",
    "            next_network_batch = next_network_batch.cuda()\n",
    "            next_job_waiting = next_job_waiting.cuda()\n",
    "            prob_a = prob_a.cuda()\n",
    "            entropy = entropy.cuda()\n",
    "            sojourn_time = sojourn_time.cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).cuda()\n",
    "            action_mask = action_mask.cuda()\n",
    "\n",
    "            for i in range(3):\n",
    "                \n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                \n",
    "                delta = td_target - self.v([network_batch, job_waiting])\n",
    "                delta = delta.detach().to('cpu').numpy()\n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = (gamma_gpu**sojourn_time[i][0]) * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "                pre_advantage = advantage\n",
    "\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(self.v([network_batch, job_waiting]) , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                pi, _, outputs = self.pi([network_batch, job_waiting])\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                outputs = outputs * action_mask\n",
    "\n",
    "                exp_sum = torch.sum(torch.exp(outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "                exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "                outputs_a = outputs.gather(1,a)\n",
    "                pi_a = torch.exp(outputs_a) / exp_sum\n",
    "\n",
    "                # pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * advantage_lst\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage_lst\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-08-18 09:17:15 AM] training....\n",
      "[2022-08-18 09:17:21 AM] training complete\n",
      "[2022-08-18 09:17:32 AM] training....\n",
      "[2022-08-18 09:17:35 AM] training complete\n",
      "[2022-08-18 09:17:46 AM] training....\n",
      "[2022-08-18 09:17:50 AM] training complete\n",
      "[2022-08-18 09:17:58 AM] training....\n",
      "[2022-08-18 09:18:01 AM] training complete\n",
      "[2022-08-18 09:18:11 AM] training....\n",
      "[2022-08-18 09:18:15 AM] training complete\n",
      "[2022-08-18 09:18:26 AM] training....\n",
      "[2022-08-18 09:18:30 AM] training complete\n",
      "[2022-08-18 09:18:41 AM] training....\n",
      "[2022-08-18 09:18:45 AM] training complete\n",
      "[2022-08-18 09:18:58 AM] training....\n",
      "[2022-08-18 09:19:04 AM] training complete\n",
      "[2022-08-18 09:19:16 AM] training....\n",
      "[2022-08-18 09:19:19 AM] training complete\n",
      "[2022-08-18 09:19:31 AM] training....\n",
      "[2022-08-18 09:19:35 AM] training complete\n",
      "[2022-08-18 09:19:46 AM] training....\n",
      "[2022-08-18 09:19:49 AM] training complete\n",
      "[2022-08-18 09:20:01 AM] training....\n",
      "[2022-08-18 09:20:05 AM] training complete\n",
      "[2022-08-18 09:20:15 AM] training....\n",
      "[2022-08-18 09:20:18 AM] training complete\n",
      "[2022-08-18 09:20:32 AM] training....\n",
      "[2022-08-18 09:20:37 AM] training complete\n",
      "[2022-08-18 09:20:49 AM] training....\n",
      "[2022-08-18 09:20:53 AM] training complete\n",
      "[2022-08-18 09:21:04 AM] training....\n",
      "[2022-08-18 09:21:07 AM] training complete\n",
      "[2022-08-18 09:21:17 AM] training....\n",
      "[2022-08-18 09:21:21 AM] training complete\n",
      "[2022-08-18 09:21:34 AM] training....\n",
      "[2022-08-18 09:21:39 AM] training complete\n",
      "[2022-08-18 09:21:48 AM] training....\n",
      "[2022-08-18 09:21:51 AM] training complete\n",
      "[2022-08-18 09:22:04 AM] training....\n",
      "[2022-08-18 09:22:09 AM] training complete\n",
      "[2022-08-18 09:22:21 AM] training....\n",
      "[2022-08-18 09:22:26 AM] training complete\n",
      "[2022-08-18 09:22:36 AM] training....\n",
      "[2022-08-18 09:22:39 AM] training complete\n",
      "[2022-08-18 09:22:51 AM] training....\n",
      "[2022-08-18 09:22:57 AM] training complete\n",
      "[2022-08-18 09:23:09 AM] training....\n",
      "[2022-08-18 09:23:13 AM] training complete\n",
      "[2022-08-18 09:23:25 AM] training....\n",
      "[2022-08-18 09:23:31 AM] training complete\n",
      "[2022-08-18 09:23:44 AM] training....\n",
      "[2022-08-18 09:23:48 AM] training complete\n",
      "[2022-08-18 09:23:58 AM] training....\n",
      "[2022-08-18 09:24:02 AM] training complete\n",
      "[2022-08-18 09:24:13 AM] training....\n",
      "[2022-08-18 09:24:16 AM] training complete\n",
      "[2022-08-18 09:24:26 AM] training....\n",
      "[2022-08-18 09:24:30 AM] training complete\n",
      "[2022-08-18 09:24:40 AM] training....\n",
      "[2022-08-18 09:24:43 AM] training complete\n",
      "[2022-08-18 09:24:57 AM] training....\n",
      "[2022-08-18 09:25:02 AM] training complete\n",
      "[2022-08-18 09:25:12 AM] training....\n",
      "[2022-08-18 09:25:16 AM] training complete\n",
      "[2022-08-18 09:25:28 AM] training....\n",
      "[2022-08-18 09:25:32 AM] training complete\n",
      "[2022-08-18 09:25:44 AM] training....\n",
      "[2022-08-18 09:25:49 AM] training complete\n",
      "[2022-08-18 09:25:59 AM] training....\n",
      "[2022-08-18 09:26:03 AM] training complete\n",
      "[2022-08-18 09:26:16 AM] training....\n",
      "[2022-08-18 09:26:22 AM] training complete\n",
      "[2022-08-18 09:26:32 AM] training....\n",
      "[2022-08-18 09:26:36 AM] training complete\n",
      "[2022-08-18 09:26:50 AM] training....\n",
      "[2022-08-18 09:26:56 AM] training complete\n",
      "[2022-08-18 09:27:07 AM] training....\n",
      "[2022-08-18 09:27:11 AM] training complete\n",
      "[2022-08-18 09:27:23 AM] training....\n",
      "[2022-08-18 09:27:27 AM] training complete\n",
      "[2022-08-18 09:27:37 AM] training....\n",
      "[2022-08-18 09:27:41 AM] training complete\n",
      "[2022-08-18 09:27:51 AM] training....\n",
      "[2022-08-18 09:27:54 AM] training complete\n",
      "[2022-08-18 09:28:06 AM] training....\n",
      "[2022-08-18 09:28:10 AM] training complete\n",
      "[2022-08-18 09:28:20 AM] training....\n",
      "[2022-08-18 09:28:24 AM] training complete\n",
      "[2022-08-18 09:28:34 AM] training....\n",
      "[2022-08-18 09:28:37 AM] training complete\n",
      "[2022-08-18 09:28:50 AM] training....\n",
      "[2022-08-18 09:28:55 AM] training complete\n",
      "[2022-08-18 09:29:06 AM] training....\n",
      "[2022-08-18 09:29:10 AM] training complete\n",
      "[2022-08-18 09:29:23 AM] training....\n",
      "[2022-08-18 09:29:28 AM] training complete\n",
      "[2022-08-18 09:29:39 AM] training....\n",
      "[2022-08-18 09:29:43 AM] training complete\n",
      "[2022-08-18 09:29:55 AM] training....\n",
      "[2022-08-18 09:30:00 AM] training complete\n",
      "[2022-08-18 09:30:10 AM] training....\n",
      "[2022-08-18 09:30:13 AM] training complete\n",
      "[2022-08-18 09:30:25 AM] training....\n",
      "[2022-08-18 09:30:31 AM] training complete\n",
      "[2022-08-18 09:30:41 AM] training....\n",
      "[2022-08-18 09:30:44 AM] training complete\n",
      "[2022-08-18 09:30:57 AM] training....\n",
      "[2022-08-18 09:31:02 AM] training complete\n",
      "[2022-08-18 09:31:14 AM] training....\n",
      "[2022-08-18 09:31:19 AM] training complete\n",
      "[2022-08-18 09:31:32 AM] training....\n",
      "[2022-08-18 09:31:37 AM] training complete\n",
      "[2022-08-18 09:31:49 AM] training....\n",
      "[2022-08-18 09:31:52 AM] training complete\n",
      "[2022-08-18 09:32:02 AM] training....\n",
      "[2022-08-18 09:32:06 AM] training complete\n",
      "[2022-08-18 09:32:15 AM] training....\n",
      "[2022-08-18 09:32:19 AM] training complete\n",
      "[2022-08-18 09:32:27 AM] training....\n",
      "[2022-08-18 09:32:31 AM] training complete\n",
      "[2022-08-18 09:32:42 AM] training....\n",
      "[2022-08-18 09:32:46 AM] training complete\n",
      "[2022-08-18 09:32:55 AM] training....\n",
      "[2022-08-18 09:32:59 AM] training complete\n",
      "[2022-08-18 09:33:11 AM] training....\n",
      "[2022-08-18 09:33:15 AM] training complete\n",
      "[2022-08-18 09:33:24 AM] training....\n",
      "[2022-08-18 09:33:27 AM] training complete\n",
      "[2022-08-18 09:33:37 AM] training....\n",
      "[2022-08-18 09:33:40 AM] training complete\n",
      "[2022-08-18 09:33:51 AM] training....\n",
      "[2022-08-18 09:33:55 AM] training complete\n",
      "[2022-08-18 09:34:01 AM] training....\n",
      "[2022-08-18 09:34:04 AM] training complete\n",
      "[2022-08-18 09:34:11 AM] training....\n",
      "[2022-08-18 09:34:14 AM] training complete\n",
      "[2022-08-18 09:34:25 AM] training....\n",
      "[2022-08-18 09:34:31 AM] training complete\n",
      "[2022-08-18 09:34:37 AM] training....\n",
      "[2022-08-18 09:34:39 AM] training complete\n",
      "[2022-08-18 09:34:47 AM] training....\n",
      "[2022-08-18 09:34:50 AM] training complete\n",
      "[2022-08-18 09:35:01 AM] training....\n",
      "[2022-08-18 09:35:05 AM] training complete\n",
      "[2022-08-18 09:35:17 AM] training....\n",
      "[2022-08-18 09:35:21 AM] training complete\n",
      "[2022-08-18 09:35:30 AM] training....\n",
      "[2022-08-18 09:35:33 AM] training complete\n",
      "[2022-08-18 09:35:43 AM] training....\n",
      "[2022-08-18 09:35:47 AM] training complete\n",
      "[2022-08-18 09:35:56 AM] training....\n",
      "[2022-08-18 09:36:00 AM] training complete\n",
      "[2022-08-18 09:36:10 AM] training....\n",
      "[2022-08-18 09:36:13 AM] training complete\n",
      "[2022-08-18 09:36:23 AM] training....\n",
      "[2022-08-18 09:36:27 AM] training complete\n",
      "[2022-08-18 09:36:36 AM] training....\n",
      "[2022-08-18 09:36:40 AM] training complete\n",
      "[2022-08-18 09:36:51 AM] training....\n",
      "[2022-08-18 09:36:54 AM] training complete\n",
      "[2022-08-18 09:37:06 AM] training....\n",
      "[2022-08-18 09:37:10 AM] training complete\n",
      "[2022-08-18 09:37:20 AM] training....\n",
      "[2022-08-18 09:37:24 AM] training complete\n",
      "[2022-08-18 09:37:36 AM] training....\n",
      "[2022-08-18 09:37:39 AM] training complete\n",
      "[2022-08-18 09:37:47 AM] training....\n",
      "[2022-08-18 09:37:50 AM] training complete\n",
      "[2022-08-18 09:38:00 AM] training....\n",
      "[2022-08-18 09:38:03 AM] training complete\n",
      "[2022-08-18 09:38:13 AM] training....\n",
      "[2022-08-18 09:38:16 AM] training complete\n",
      "[2022-08-18 09:38:24 AM] training....\n",
      "[2022-08-18 09:38:27 AM] training complete\n",
      "[2022-08-18 09:38:37 AM] training....\n",
      "[2022-08-18 09:38:41 AM] training complete\n",
      "[2022-08-18 09:38:50 AM] training....\n",
      "[2022-08-18 09:38:53 AM] training complete\n",
      "[2022-08-18 09:39:04 AM] training....\n",
      "[2022-08-18 09:39:08 AM] training complete\n",
      "[2022-08-18 09:39:21 AM] training....\n",
      "[2022-08-18 09:39:27 AM] training complete\n",
      "[2022-08-18 09:39:36 AM] training....\n",
      "[2022-08-18 09:39:39 AM] training complete\n",
      "[2022-08-18 09:39:46 AM] training....\n",
      "[2022-08-18 09:39:49 AM] training complete\n",
      "[2022-08-18 09:39:55 AM] training....\n",
      "[2022-08-18 09:39:58 AM] training complete\n",
      "[2022-08-18 09:40:07 AM] training....\n",
      "[2022-08-18 09:40:11 AM] training complete\n",
      "[2022-08-18 09:40:20 AM] training....\n",
      "[2022-08-18 09:40:24 AM] training complete\n",
      "[2022-08-18 09:40:32 AM] training....\n",
      "[2022-08-18 09:40:35 AM] training complete\n",
      "[2022-08-18 09:40:46 AM] training....\n",
      "[2022-08-18 09:40:49 AM] training complete\n",
      "[2022-08-18 09:40:57 AM] training....\n",
      "[2022-08-18 09:41:01 AM] training complete\n",
      "[2022-08-18 09:41:10 AM] training....\n",
      "[2022-08-18 09:41:13 AM] training complete\n",
      "[2022-08-18 09:41:20 AM] training....\n",
      "[2022-08-18 09:41:23 AM] training complete\n",
      "[2022-08-18 09:41:31 AM] training....\n",
      "[2022-08-18 09:41:34 AM] training complete\n",
      "[2022-08-18 09:41:42 AM] training....\n",
      "[2022-08-18 09:41:46 AM] training complete\n",
      "[2022-08-18 09:41:59 AM] training....\n",
      "[2022-08-18 09:42:05 AM] training complete\n",
      "[2022-08-18 09:42:15 AM] training....\n",
      "[2022-08-18 09:42:18 AM] training complete\n",
      "[2022-08-18 09:42:27 AM] training....\n",
      "[2022-08-18 09:42:31 AM] training complete\n",
      "[2022-08-18 09:42:41 AM] training....\n",
      "[2022-08-18 09:42:44 AM] training complete\n",
      "[2022-08-18 09:42:55 AM] training....\n",
      "[2022-08-18 09:42:59 AM] training complete\n",
      "[2022-08-18 09:43:08 AM] training....\n",
      "[2022-08-18 09:43:12 AM] training complete\n",
      "[2022-08-18 09:43:20 AM] training....\n",
      "[2022-08-18 09:43:23 AM] training complete\n",
      "[2022-08-18 09:43:35 AM] training....\n",
      "[2022-08-18 09:43:39 AM] training complete\n",
      "[2022-08-18 09:43:47 AM] training....\n",
      "[2022-08-18 09:43:50 AM] training complete\n",
      "[2022-08-18 09:44:01 AM] training....\n",
      "[2022-08-18 09:44:04 AM] training complete\n",
      "[2022-08-18 09:44:15 AM] training....\n",
      "[2022-08-18 09:44:19 AM] training complete\n",
      "[2022-08-18 09:44:29 AM] training....\n",
      "[2022-08-18 09:44:32 AM] training complete\n",
      "[2022-08-18 09:44:39 AM] training....\n",
      "[2022-08-18 09:44:42 AM] training complete\n",
      "[2022-08-18 09:44:51 AM] training....\n",
      "[2022-08-18 09:44:54 AM] training complete\n",
      "[2022-08-18 09:45:04 AM] training....\n",
      "[2022-08-18 09:45:08 AM] training complete\n",
      "[2022-08-18 09:45:17 AM] training....\n",
      "[2022-08-18 09:45:21 AM] training complete\n",
      "[2022-08-18 09:45:29 AM] training....\n",
      "[2022-08-18 09:45:32 AM] training complete\n",
      "[2022-08-18 09:45:40 AM] training....\n",
      "[2022-08-18 09:45:44 AM] training complete\n",
      "[2022-08-18 09:45:54 AM] training....\n",
      "[2022-08-18 09:45:57 AM] training complete\n",
      "[2022-08-18 09:46:07 AM] training....\n",
      "[2022-08-18 09:46:11 AM] training complete\n",
      "[2022-08-18 09:46:16 AM] training....\n",
      "[2022-08-18 09:46:19 AM] training complete\n",
      "[2022-08-18 09:46:31 AM] training....\n",
      "[2022-08-18 09:46:34 AM] training complete\n",
      "[2022-08-18 09:46:43 AM] training....\n",
      "[2022-08-18 09:46:46 AM] training complete\n",
      "[2022-08-18 09:46:56 AM] training....\n",
      "[2022-08-18 09:46:59 AM] training complete\n",
      "[2022-08-18 09:47:11 AM] training....\n",
      "[2022-08-18 09:47:15 AM] training complete\n",
      "[2022-08-18 09:47:23 AM] training....\n",
      "[2022-08-18 09:47:26 AM] training complete\n",
      "[2022-08-18 09:47:36 AM] training....\n",
      "[2022-08-18 09:47:39 AM] training complete\n",
      "[2022-08-18 09:47:51 AM] training....\n",
      "[2022-08-18 09:47:57 AM] training complete\n",
      "[2022-08-18 09:48:06 AM] training....\n",
      "[2022-08-18 09:48:09 AM] training complete\n",
      "[2022-08-18 09:48:17 AM] training....\n",
      "[2022-08-18 09:48:20 AM] training complete\n",
      "[2022-08-18 09:48:27 AM] training....\n",
      "[2022-08-18 09:48:30 AM] training complete\n",
      "[2022-08-18 09:48:38 AM] training....\n",
      "[2022-08-18 09:48:42 AM] training complete\n",
      "[2022-08-18 09:48:49 AM] training....\n",
      "[2022-08-18 09:48:52 AM] training complete\n",
      "[2022-08-18 09:48:59 AM] training....\n",
      "[2022-08-18 09:49:02 AM] training complete\n",
      "[2022-08-18 09:49:12 AM] training....\n",
      "[2022-08-18 09:49:15 AM] training complete\n",
      "[2022-08-18 09:49:23 AM] training....\n",
      "[2022-08-18 09:49:26 AM] training complete\n",
      "[2022-08-18 09:49:32 AM] training....\n",
      "[2022-08-18 09:49:35 AM] training complete\n",
      "[2022-08-18 09:49:44 AM] training....\n",
      "[2022-08-18 09:49:48 AM] training complete\n",
      "[2022-08-18 09:49:55 AM] training....\n",
      "[2022-08-18 09:49:59 AM] training complete\n",
      "[2022-08-18 09:50:08 AM] training....\n",
      "[2022-08-18 09:50:11 AM] training complete\n",
      "[2022-08-18 09:50:21 AM] training....\n",
      "[2022-08-18 09:50:24 AM] training complete\n",
      "[2022-08-18 09:50:34 AM] training....\n",
      "[2022-08-18 09:50:37 AM] training complete\n",
      "[2022-08-18 09:50:45 AM] training....\n",
      "[2022-08-18 09:50:48 AM] training complete\n",
      "[2022-08-18 09:50:53 AM] training....\n",
      "[2022-08-18 09:50:56 AM] training complete\n",
      "[2022-08-18 09:51:02 AM] training....\n",
      "[2022-08-18 09:51:06 AM] training complete\n",
      "[2022-08-18 09:51:13 AM] training....\n",
      "[2022-08-18 09:51:16 AM] training complete\n",
      "[2022-08-18 09:51:22 AM] training....\n",
      "[2022-08-18 09:51:25 AM] training complete\n",
      "[2022-08-18 09:51:34 AM] training....\n",
      "[2022-08-18 09:51:38 AM] training complete\n",
      "[2022-08-18 09:51:47 AM] training....\n",
      "[2022-08-18 09:51:50 AM] training complete\n",
      "[2022-08-18 09:52:00 AM] training....\n",
      "[2022-08-18 09:52:03 AM] training complete\n",
      "[2022-08-18 09:52:12 AM] training....\n",
      "[2022-08-18 09:52:16 AM] training complete\n",
      "[2022-08-18 09:52:23 AM] training....\n",
      "[2022-08-18 09:52:26 AM] training complete\n",
      "[2022-08-18 09:52:35 AM] training....\n",
      "[2022-08-18 09:52:38 AM] training complete\n",
      "[2022-08-18 09:52:49 AM] training....\n",
      "[2022-08-18 09:52:52 AM] training complete\n",
      "[2022-08-18 09:53:02 AM] training....\n",
      "[2022-08-18 09:53:05 AM] training complete\n",
      "[2022-08-18 09:53:14 AM] training....\n",
      "[2022-08-18 09:53:17 AM] training complete\n",
      "[2022-08-18 09:53:27 AM] training....\n",
      "[2022-08-18 09:53:31 AM] training complete\n",
      "[2022-08-18 09:53:39 AM] training....\n",
      "[2022-08-18 09:53:42 AM] training complete\n",
      "[2022-08-18 09:53:52 AM] training....\n",
      "[2022-08-18 09:53:55 AM] training complete\n",
      "[2022-08-18 09:54:05 AM] training....\n",
      "[2022-08-18 09:54:08 AM] training complete\n",
      "[2022-08-18 09:54:16 AM] training....\n",
      "[2022-08-18 09:54:19 AM] training complete\n",
      "[2022-08-18 09:54:25 AM] training....\n",
      "[2022-08-18 09:54:28 AM] training complete\n",
      "[2022-08-18 09:54:37 AM] training....\n",
      "[2022-08-18 09:54:40 AM] training complete\n",
      "[2022-08-18 09:54:48 AM] training....\n",
      "[2022-08-18 09:54:51 AM] training complete\n",
      "[2022-08-18 09:55:00 AM] training....\n",
      "[2022-08-18 09:55:03 AM] training complete\n",
      "[2022-08-18 09:55:13 AM] training....\n",
      "[2022-08-18 09:55:16 AM] training complete\n",
      "[2022-08-18 09:55:27 AM] training....\n",
      "[2022-08-18 09:55:31 AM] training complete\n",
      "[2022-08-18 09:55:42 AM] training....\n",
      "[2022-08-18 09:55:46 AM] training complete\n",
      "[2022-08-18 09:55:54 AM] training....\n",
      "[2022-08-18 09:55:57 AM] training complete\n",
      "[2022-08-18 09:56:06 AM] training....\n",
      "[2022-08-18 09:56:09 AM] training complete\n",
      "[2022-08-18 09:56:18 AM] training....\n",
      "[2022-08-18 09:56:21 AM] training complete\n",
      "[2022-08-18 09:56:33 AM] training....\n",
      "[2022-08-18 09:56:36 AM] training complete\n",
      "[2022-08-18 09:56:45 AM] training....\n",
      "[2022-08-18 09:56:49 AM] training complete\n",
      "[2022-08-18 09:56:59 AM] training....\n",
      "[2022-08-18 09:57:02 AM] training complete\n",
      "[2022-08-18 09:57:13 AM] training....\n",
      "[2022-08-18 09:57:17 AM] training complete\n",
      "[2022-08-18 09:57:24 AM] training....\n",
      "[2022-08-18 09:57:28 AM] training complete\n",
      "[2022-08-18 09:57:38 AM] training....\n",
      "[2022-08-18 09:57:42 AM] training complete\n",
      "[2022-08-18 09:57:53 AM] training....\n",
      "[2022-08-18 09:57:56 AM] training complete\n",
      "[2022-08-18 09:58:04 AM] training....\n",
      "[2022-08-18 09:58:07 AM] training complete\n",
      "[2022-08-18 09:58:17 AM] training....\n",
      "[2022-08-18 09:58:21 AM] training complete\n",
      "[2022-08-18 09:58:31 AM] training....\n",
      "[2022-08-18 09:58:34 AM] training complete\n",
      "[2022-08-18 09:58:44 AM] training....\n",
      "[2022-08-18 09:58:47 AM] training complete\n",
      "[2022-08-18 09:58:55 AM] training....\n",
      "[2022-08-18 09:58:59 AM] training complete\n",
      "[2022-08-18 09:59:09 AM] training....\n",
      "[2022-08-18 09:59:12 AM] training complete\n",
      "[2022-08-18 09:59:22 AM] training....\n",
      "[2022-08-18 09:59:25 AM] training complete\n",
      "[2022-08-18 09:59:36 AM] training....\n",
      "[2022-08-18 09:59:39 AM] training complete\n",
      "[2022-08-18 09:59:45 AM] training....\n",
      "[2022-08-18 09:59:48 AM] training complete\n",
      "[2022-08-18 10:00:00 AM] training....\n",
      "[2022-08-18 10:00:05 AM] training complete\n",
      "[2022-08-18 10:00:12 AM] training....\n",
      "[2022-08-18 10:00:16 AM] training complete\n",
      "[2022-08-18 10:00:24 AM] training....\n",
      "[2022-08-18 10:00:27 AM] training complete\n",
      "[2022-08-18 10:00:37 AM] training....\n",
      "[2022-08-18 10:00:41 AM] training complete\n",
      "[2022-08-18 10:00:47 AM] training....\n",
      "[2022-08-18 10:00:49 AM] training complete\n",
      "[2022-08-18 10:00:56 AM] training....\n",
      "[2022-08-18 10:00:59 AM] training complete\n",
      "[2022-08-18 10:01:06 AM] training....\n",
      "[2022-08-18 10:01:09 AM] training complete\n",
      "[2022-08-18 10:01:17 AM] training....\n",
      "[2022-08-18 10:01:21 AM] training complete\n",
      "[2022-08-18 10:01:29 AM] training....\n",
      "[2022-08-18 10:01:32 AM] training complete\n",
      "[2022-08-18 10:01:41 AM] training....\n",
      "[2022-08-18 10:01:44 AM] training complete\n",
      "[2022-08-18 10:01:52 AM] training....\n",
      "[2022-08-18 10:01:55 AM] training complete\n",
      "[2022-08-18 10:02:03 AM] training....\n",
      "[2022-08-18 10:02:06 AM] training complete\n",
      "[2022-08-18 10:02:16 AM] training....\n",
      "[2022-08-18 10:02:19 AM] training complete\n",
      "[2022-08-18 10:02:26 AM] training....\n",
      "[2022-08-18 10:02:29 AM] training complete\n",
      "[2022-08-18 10:02:36 AM] training....\n",
      "[2022-08-18 10:02:40 AM] training complete\n",
      "[2022-08-18 10:02:45 AM] training....\n",
      "[2022-08-18 10:02:47 AM] training complete\n",
      "[2022-08-18 10:02:58 AM] training....\n",
      "[2022-08-18 10:03:02 AM] training complete\n",
      "[2022-08-18 10:03:10 AM] training....\n",
      "[2022-08-18 10:03:13 AM] training complete\n",
      "[2022-08-18 10:03:19 AM] training....\n",
      "[2022-08-18 10:03:22 AM] training complete\n",
      "[2022-08-18 10:03:30 AM] training....\n",
      "[2022-08-18 10:03:33 AM] training complete\n",
      "[2022-08-18 10:03:42 AM] training....\n",
      "[2022-08-18 10:03:45 AM] training complete\n",
      "[2022-08-18 10:03:55 AM] training....\n",
      "[2022-08-18 10:03:58 AM] training complete\n",
      "[2022-08-18 10:04:03 AM] training....\n",
      "[2022-08-18 10:04:06 AM] training complete\n",
      "[2022-08-18 10:04:14 AM] training....\n",
      "[2022-08-18 10:04:18 AM] training complete\n",
      "[2022-08-18 10:04:27 AM] training....\n",
      "[2022-08-18 10:04:31 AM] training complete\n",
      "[2022-08-18 10:04:40 AM] training....\n",
      "[2022-08-18 10:04:43 AM] training complete\n",
      "[2022-08-18 10:04:49 AM] training....\n",
      "[2022-08-18 10:04:52 AM] training complete\n",
      "[2022-08-18 10:05:01 AM] training....\n",
      "[2022-08-18 10:05:04 AM] training complete\n",
      "[2022-08-18 10:05:13 AM] training....\n",
      "[2022-08-18 10:05:16 AM] training complete\n",
      "[2022-08-18 10:05:23 AM] training....\n",
      "[2022-08-18 10:05:26 AM] training complete\n",
      "[2022-08-18 10:05:37 AM] training....\n",
      "[2022-08-18 10:05:41 AM] training complete\n",
      "[2022-08-18 10:05:48 AM] training....\n",
      "[2022-08-18 10:05:51 AM] training complete\n",
      "[2022-08-18 10:06:02 AM] training....\n",
      "[2022-08-18 10:06:05 AM] training complete\n",
      "[2022-08-18 10:06:13 AM] training....\n",
      "[2022-08-18 10:06:16 AM] training complete\n",
      "[2022-08-18 10:06:26 AM] training....\n",
      "[2022-08-18 10:06:28 AM] training complete\n",
      "[2022-08-18 10:06:37 AM] training....\n",
      "[2022-08-18 10:06:40 AM] training complete\n",
      "[2022-08-18 10:06:52 AM] training....\n",
      "[2022-08-18 10:06:56 AM] training complete\n",
      "[2022-08-18 10:07:03 AM] training....\n",
      "[2022-08-18 10:07:05 AM] training complete\n",
      "[2022-08-18 10:07:13 AM] training....\n",
      "[2022-08-18 10:07:16 AM] training complete\n",
      "[2022-08-18 10:07:24 AM] training....\n",
      "[2022-08-18 10:07:26 AM] training complete\n",
      "[2022-08-18 10:07:37 AM] training....\n",
      "[2022-08-18 10:07:40 AM] training complete\n",
      "[2022-08-18 10:07:48 AM] training....\n",
      "[2022-08-18 10:07:51 AM] training complete\n",
      "[2022-08-18 10:08:01 AM] training....\n",
      "[2022-08-18 10:08:04 AM] training complete\n",
      "[2022-08-18 10:08:14 AM] training....\n",
      "[2022-08-18 10:08:18 AM] training complete\n",
      "[2022-08-18 10:08:26 AM] training....\n",
      "[2022-08-18 10:08:30 AM] training complete\n",
      "[2022-08-18 10:08:41 AM] training....\n",
      "[2022-08-18 10:08:45 AM] training complete\n",
      "[2022-08-18 10:08:53 AM] training....\n",
      "[2022-08-18 10:08:57 AM] training complete\n",
      "[2022-08-18 10:09:05 AM] training....\n",
      "[2022-08-18 10:09:09 AM] training complete\n",
      "[2022-08-18 10:09:15 AM] training....\n",
      "[2022-08-18 10:09:18 AM] training complete\n",
      "[2022-08-18 10:09:27 AM] training....\n",
      "[2022-08-18 10:09:31 AM] training complete\n",
      "[2022-08-18 10:09:40 AM] training....\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global adjacency, entropy_weight\n",
    "    model = actor_network()\n",
    "    # model.load_state_dict(torch.load(\"./history30/model.pth\")) # -1 , +1\n",
    "    # model.load_state_dict(torch.load(\"./history80/model.pth\")) # original\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "\n",
    "    adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    # node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    # node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    # link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    # job_waiting_state = env.get_job_waiting_vector()\n",
    "\n",
    "    # network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "\n",
    "    # for each time step\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = []\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(nodeNum)]\n",
    "    void_selected_num = 0\n",
    "    \n",
    "    while True:\n",
    "        model = model.to('cpu')\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state = getOmnetMessage()\n",
    "\n",
    "            state = json.loads(state) # state 받았으므로 action 하면됨.\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            node_waiting_state = np.array(eval(state['nodeState']))\n",
    "            node_processing_state = np.array(eval(state['nodeProcessing']))\n",
    "            link_state = np.array(eval(state['linkWaiting']))\n",
    "            job_waiting_state = np.array(eval(state['jobWaiting']))\n",
    "            activated_job_list = eval(state['activatedJobList'])\n",
    "            isAction = int(state['isAction'])\n",
    "            reward = float(state['reward'])\n",
    "            averageLatency = float(state['averageLatency'])\n",
    "            completeJobNum = int(state['completeJobNum'])\n",
    "            sojournTime = float(state['sojournTime'])\n",
    "\n",
    "            if averageLatency != -1:\n",
    "                writer.add_scalar(\"averageLatency/train\", averageLatency ,step)\n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum ,step)\n",
    "            writer.add_scalar(\"Reward/train\", reward, step)\n",
    "\n",
    "            # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            first_sample = True\n",
    "            if is_train:\n",
    "                if len(temp_history) > 0:\n",
    "                    temp_history[-1][8] = sojournTime\n",
    "                for history in temp_history:\n",
    "                    history[3] = reward/100.0\n",
    "                    model.put_data(history)\n",
    "\n",
    "            temp_history = []\n",
    "\n",
    "            job_index = int(state['jobIndex'])\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "\n",
    "            node_state = np.concatenate((node_waiting_state,node_processing_state) ,axis = 1)\n",
    "            node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = collections.deque()\n",
    "            for job in job_waiting_state:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state = torch.tensor(job_waiting_state, dtype=torch.float).view(1, -1)\n",
    "            # print(job_waiting_state)\n",
    "\n",
    "            network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(nodeNum):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(nodeNum)] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward != 0:\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                    writer.add_scalar(\"Value/train\", model.v([network_state, job_waiting_state])[0], step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction = False\n",
    "                \n",
    "\n",
    "            if isAction:\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = 1\n",
    "                for i in range(nodeNum):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[nodeNum:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "                #print(\"action start.\")\n",
    "                for order in range(len(subtasks)):\n",
    "                    if subtasks[order] == 0:\n",
    "                        break\n",
    "\n",
    "                    network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                    # print(node_state.shape)\n",
    "                    # print(link_state.shape)\n",
    "                    # print(adjacency.shape)\n",
    "                    # print(job_waiting_state.shape)\n",
    "                    prob, entropy, output = model.pi([network_state, job_waiting_state])\n",
    "                    \n",
    "                    #print(f'prob : {prob}')\n",
    "                    \n",
    "                    m = Categorical(prob) \n",
    "                    node = m.sample().item()\n",
    "                    #print(f'node : {node}')\n",
    "                    \n",
    "                    # void action 실험용\n",
    "                    # node = nodeNum \n",
    "                    \n",
    "                    \n",
    "                    # void action 뽑으면\n",
    "                    if node == nodeNum and not scheduling_start: \n",
    "                        # print(\"void\")\n",
    "                        prob[0] = torch.Tensor([0] * nodeNum + [1.0])\n",
    "                        action_mask = [int(not scheduling_start) if i == node else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, network_state, job_waiting_state, prob[0][node].item(), entropy, 0, action_mask])\n",
    "                        sendOmnetMessage(\"void\")\n",
    "\n",
    "                        #print(\"action finish.\")\n",
    "                        \n",
    "                        if getOmnetMessage() == \"ok\":\n",
    "                            void_selected_num += 1\n",
    "                            \n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        scheduling_start = True\n",
    "\n",
    "                    if scheduling_start:\n",
    "\n",
    "                        prob = torch.Tensor([F.softmax(output[0][:nodeNum], dim=0).tolist()])\n",
    "                        prob = torch.cat([prob[0], torch.tensor([0])]) # void action masking\n",
    "                        prob = torch.Tensor([prob.tolist()]).cuda()\n",
    "\n",
    "\n",
    "\n",
    "                        m = Categorical(prob[0])\n",
    "                        node = m.sample().item()\n",
    "                        \n",
    "\n",
    "                        offloading_vector.append(node)\n",
    "\n",
    "                        node_selected_num[node] += 1\n",
    "                        \n",
    "                        # state transition(환경으로부터 매번 state를 업데이트하는게 아닌, 현재 state를 기반으로 action에 해당하는 waiting만 더해줌).\n",
    "                        # 아래의 구체적인 코드는 이해하시기 보단 OMNeT++에서 받아온 데이터로 새로 짜시는게 빠를 것 같습니다.\n",
    "                        next_node_state = node_state.clone().detach()\n",
    "                        next_job_waiting_state = job_waiting_state.clone().detach()\n",
    "                        # print(next_job_waiting_state)\n",
    "                        \n",
    "                        next_node_state[node][modelNum * job_idx + order] += (subtasks[order]/100) # 100으로 나누는 이유 : 이렇게 해야 액션이 한쪽 노드로 쏠리게끔 학습이 되는 것을 어느정도 방지할 수 있는 것을 확인.\n",
    "                        # 100으로 안나눠주면 너무 큰 값이 state에 추가되어서 inference시 가중치랑 곱해지면서 액션이 한쪽으로 확 쏠리는 걸로 예상됨.\n",
    "                        next_network_state = Data(x=next_node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                        next_job_waiting_state[0][nodeNum + order] = 0\n",
    "\n",
    "                        action_mask = [int(not scheduling_start) if i == nodeNum else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "                        temp_history.append([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy, 0, action_mask])\n",
    "\n",
    "                        # model.put_data([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy])\n",
    "                        node_state = next_node_state\n",
    "                        job_waiting_state = next_job_waiting_state\n",
    "\n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            \n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            writer.add_scalar(\"CompleteNum/train\", complete_num, episode)\n",
    "            episode += 1\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = pathName + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            average_reward = 0\n",
    "            average_reward_num = 0\n",
    "\n",
    "            # entropy_weight *= 0.99\n",
    "\n",
    "\n",
    "            # if len(model.data) > 0 and step % train_cycle == train_quitient:\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training....\")\n",
    "            #     model.train_net()\n",
    "            #     modelPathName = pathName + \"/model.pth\"\n",
    "            #     torch.save(model.state_dict(), modelPathName)\n",
    "            #     writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            #     average_reward = 0\n",
    "            #     average_reward_num = 0\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training complete\")\n",
    "            if is_train:\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                modelPathName = pathName + \"/model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                \n",
    "                \n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "omnettest"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
