{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from library.ipynb\n",
      "importing Jupyter notebook from job.ipynb\n",
      "importing Jupyter notebook from jobqueue.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n",
      "importing Jupyter notebook from dataplane.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history16\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "\n",
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet, TopKPooling, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sys import getsizeof\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/stacking\")\n",
    "\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(pathName)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "modelNum = 0\n",
    "availableJobNum = 0\n",
    "nodeNum = 0\n",
    "jobWaitingLength = 0\n",
    "adjacency = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n",
      "cuda:0\n",
      "<class 'torch.device'>\n",
      "<built-in method cuda of Tensor object at 0x0000026D59475760>\n",
      "<class 'torch.device'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.get_device_name(device = 0))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(device)\n",
    "\n",
    "print(type(device))\n",
    "\n",
    "kkkkk = torch.Tensor(1)\n",
    "\n",
    "kkkkk = kkkkk.cuda()\n",
    "\n",
    "print(kkkkk.cuda)\n",
    "print(type(kkkkk.device))\n",
    "print(kkkkk.device == device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.0003 # 0.0003\n",
    "gamma           = 0.95\n",
    "entropy_weight  = 0.0008 # 0.001\n",
    "val_loss_coef = 0.8\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.8\n",
    "eps_clip      = 0.2\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100\n",
    "hidden_feature_num = 0\n",
    "\n",
    "job_generate_rate = 0.003\n",
    "\n",
    "train_cycle = 300\n",
    "is_train = False\n",
    "train_quitient = 0\n",
    "\n",
    "replay_buffer_size = 10000\n",
    "history_learning_time = 6\n",
    "\n",
    "if not is_train:\n",
    "    train_quitient = train_cycle\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os.path\n",
    "# import os\n",
    "# FIFO_FILENAME = './fifo-test'\n",
    "\n",
    "# if not os.path.exists(FIFO_FILENAME):\n",
    "#     os.mkfifo(FIFO_FILENAME)\n",
    "#     if os.path.exists(FIFO_FILENAME):\n",
    "#         fp_fifo = open(FIFO_FILENAME, \"rw\")\n",
    "\n",
    "# for i in range(128):\n",
    "#     fp_fifo.write(\"Hello,MakeCode\\n\")\n",
    "#     fp_fifo.write(\"\")\n",
    "\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "# PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\omnetPipe2\"\n",
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\omnetPipe3\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 초기화 완료\n",
      "\n",
      "노드 개수 : 10\n",
      "네트워크 최대 job 개수 : 5\n",
      "job 대기 가능 개수 : 15\n",
      "최대 subtask 개수 : 4\n",
      "인접 리스트 : [[0, 1, 0, 2, 0, 3, 0, 6, 0, 8, 1, 3, 1, 4, 1, 5, 1, 7, 1, 8, 3, 4, 4, 5, 4, 6, 4, 7, 5, 9, 7, 9], [1, 0, 2, 0, 3, 0, 6, 0, 8, 0, 3, 1, 4, 1, 5, 1, 7, 1, 8, 1, 4, 3, 5, 4, 6, 4, 7, 4, 9, 5, 9, 7]]\n",
      "node_feature_num : 40\n",
      "queue_feature_num : 210\n",
      "episode_length : 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "\n",
    "sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "info = f\"\"\"\n",
    "노드 개수 : {nodeNum}\n",
    "네트워크 최대 job 개수 : {availableJobNum}\n",
    "job 대기 가능 개수 : {jobWaitingLength}\n",
    "최대 subtask 개수 : {modelNum}\n",
    "인접 리스트 : {adjacency}\n",
    "node_feature_num : {node_feature_num}\n",
    "queue_feature_num : {queue_feature_num}\n",
    "episode_length : {episode_length}\n",
    "\"\"\"\n",
    "print(info)\n",
    "\n",
    "with open(f'{pathName}/info.txt', 'w') as f:\n",
    "    f.write(f'{info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = []\n",
    "        self.history = []\n",
    "\n",
    "        self.x_mean = 0\n",
    "        self.x2_mean = 0\n",
    "        self.sd = 0\n",
    "        self.reward_sample_num = 0\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        \n",
    "        # print(f\"self.adjacency : \", self.adjacency.shape)\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * int(node_feature_num//2)), nn.ReLU())\n",
    "        self.pi_s_ecc1 = NNConv(node_feature_num, int(node_feature_num//2), self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_mlp2 = nn.Sequential(nn.Linear(1, int(node_feature_num//2) * int(node_feature_num//2)), nn.ReLU())\n",
    "        self.pi_s_ecc2 = NNConv(int(node_feature_num//2), int(node_feature_num//2), self.pi_mlp2, aggr='mean')\n",
    "\n",
    "        self.pi_graph_u_net1 = GraphUNet(int(node_feature_num//2), 50, int(node_feature_num//2), 3, 0.8)\n",
    "        self.pi_graph_u_net2 = GraphUNet(int(node_feature_num//2), 50, int(node_feature_num//2), 3, 0.8)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear((int(node_feature_num//2) + queue_feature_num) * 2, hidden_feature_num),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(hidden_feature_num, hidden_feature_num),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(hidden_feature_num, nodeNum + 1) # nodeNum + voidAction\n",
    "\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(hidden_feature_num, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature_1, link_feature_1, adjacency_1 = data[0].x, data[0].edge_attr, data[0].edge_index\n",
    "        node_feature_2, link_feature_2, adjacency_2 = data[1].x, data[1].edge_attr, data[1].edge_index\n",
    "\n",
    "        job_waiting_feature_1 = job_waiting_feature[0]\n",
    "        job_waiting_feature_2 = job_waiting_feature[1]\n",
    "\n",
    "        if job_waiting_feature_2.device == 'cuda':\n",
    "            link_feature_1 = link_feature_1.cuda()\n",
    "            adjacency_1 = adjacency_1.cuda()\n",
    "\n",
    "            link_feature_2 = link_feature_2.cuda()\n",
    "            adjacency_2 = adjacency_2.cuda()\n",
    "\n",
    "        \n",
    "        # =========================================================================\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc1(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net1(node_feature_1, adjacency_1))\n",
    "\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc2(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net2(node_feature_1, adjacency_1))\n",
    "\n",
    "        data_num_1 = len(node_feature_1) // nodeNum\n",
    "        \n",
    "        readout_1 = global_mean_pool(node_feature_1, data[0].batch)\n",
    "\n",
    "        concat_1 = torch.cat([readout_1, job_waiting_feature_1], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc1(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net1(node_feature_2, adjacency_2))\n",
    "\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc2(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net2(node_feature_2, adjacency_2))\n",
    "\n",
    "        data_num_2 = len(node_feature_2) // nodeNum\n",
    "        \n",
    "        readout_2 = global_mean_pool(node_feature_2, data[0].batch)\n",
    "\n",
    "        concat_2 = torch.cat([readout_2, job_waiting_feature_2], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        concat = torch.cat([concat_1, concat_2], dim=1)\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        # print(feature_extract)\n",
    "\n",
    "        output = self.pi_prob_fc(feature_extract) \n",
    "\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # 아래는 엔트로피 구하는 과정\n",
    "        log_prob = F.log_softmax(output, dim=1)\n",
    "        entropy = - (log_prob * prob).sum(1, keepdim=True)\n",
    "        \n",
    "        return prob, entropy, output\n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature_1, link_feature_1, adjacency_1 = data[0].x, data[0].edge_attr, data[0].edge_index\n",
    "        node_feature_2, link_feature_2, adjacency_2 = data[1].x, data[1].edge_attr, data[1].edge_index\n",
    "\n",
    "        job_waiting_feature_1 = job_waiting_feature[0]\n",
    "        job_waiting_feature_2 = job_waiting_feature[1]\n",
    "\n",
    "        if job_waiting_feature_2.device == 'cuda':\n",
    "            link_feature_1 = link_feature_1.cuda()\n",
    "            adjacency_1 = adjacency_1.cuda()\n",
    "\n",
    "            link_feature_2 = link_feature_2.cuda()\n",
    "            adjacency_2 = adjacency_2.cuda()\n",
    "\n",
    "        \n",
    "        # =========================================================================\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc1(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net1(node_feature_1, adjacency_1))\n",
    "\n",
    "        node_feature_1 = F.relu(self.pi_s_ecc2(node_feature_1, adjacency_1, link_feature_1))\n",
    "        node_feature_1 = F.relu(self.pi_graph_u_net2(node_feature_1, adjacency_1))\n",
    "\n",
    "        data_num_1 = len(node_feature_1) // nodeNum\n",
    "        \n",
    "        readout_1 = global_mean_pool(node_feature_1, data[0].batch)\n",
    "\n",
    "        concat_1 = torch.cat([readout_1, job_waiting_feature_1], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc1(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net1(node_feature_2, adjacency_2))\n",
    "\n",
    "        node_feature_2 = F.relu(self.pi_s_ecc2(node_feature_2, adjacency_2, link_feature_2))\n",
    "        node_feature_2 = F.relu(self.pi_graph_u_net2(node_feature_2, adjacency_2))\n",
    "\n",
    "        data_num_2 = len(node_feature_2) // nodeNum\n",
    "        \n",
    "        readout_2 = global_mean_pool(node_feature_2, data[0].batch)\n",
    "\n",
    "        concat_2 = torch.cat([readout_2, job_waiting_feature_2], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        concat = torch.cat([concat_1, concat_2], dim=1)\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        \n",
    "        value = self.v_value_fc(feature_extract) # 앞부분은 pi랑 공유해야 하고, concat -> value_fc를 거치는 것만 다름.\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "\n",
    "    def set_reward(self, reward):\n",
    "        self.data[-1][3] = reward\n",
    "        \n",
    "    def return_link_dict(sel, ad):\n",
    "        result = {}\n",
    "        for i in range(len(ad[0])//2):\n",
    "            result[f'{ad[0][2*i]}{ad[0][2 *(i)+1]}'] = i\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def return_new_mean(self, mean, num, new_data):\n",
    "        result = (mean * num + new_data) / (num + 1)\n",
    "        return result\n",
    "        \n",
    "    def return_new_sd(self, square_mean, mean):\n",
    "        result = (square_mean - mean**2)**0.5\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def return_normalize_reward(self, reward):\n",
    "        self.x_mean = self.return_new_mean(self.x_mean, self.reward_sample_num, reward)\n",
    "        self.x2_mean = self.return_new_mean(self.x2_mean, self.reward_sample_num, reward**2)\n",
    "        self.sd = self.return_new_sd(self.x2_mean, self.x_mean)\n",
    "        \n",
    "        if self.sd == 0:\n",
    "                z_normalized = 0\n",
    "        else:  \n",
    "            z_normalized = (reward - self.x_mean) / self.sd\n",
    "\n",
    "        self.reward_sample_num += 1\n",
    "        \n",
    "        return z_normalized\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "\n",
    "    def make_batch_history(self):\n",
    "        sample_index = torch.randperm(len(self.history))[:batch_size]\n",
    "\n",
    "        sampled_data = []\n",
    "        for sample_idx in sample_index:\n",
    "            sampled_data.append(self.history[sample_idx])\n",
    "\n",
    "        network_1_lst, network_2_lst = [], []\n",
    "        next_network_1_lst, next_network_2_lst = [], []\n",
    "\n",
    "        job_waiting_1_lst, job_waiting_2_lst = [], []\n",
    "        next_job_waiting_1_lst, next_job_waiting_2_lst = [], []\n",
    "\n",
    "        a_lst, r_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(sampled_data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "\n",
    "            r_lst.append([r/10.0])\n",
    "\n",
    "            network_1_lst.append(network[0])\n",
    "            network_2_lst.append(network[1])\n",
    "\n",
    "            job_waiting_1_lst.append(job_waiting[0].tolist())\n",
    "            job_waiting_2_lst.append(job_waiting[1].tolist())\n",
    "\n",
    "            a_lst.append([a])\n",
    "\n",
    "            next_network_1_lst.append(nxt_network[0])\n",
    "            next_network_2_lst.append(nxt_network[1])\n",
    "            \n",
    "            next_job_waiting_1_lst.append(nxt_job_waiting[0].tolist())\n",
    "            next_job_waiting_2_lst.append(nxt_job_waiting[1].tolist())\n",
    "\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_1_loader = DataLoader(network_1_lst, batch_size=len(network_1_lst))\n",
    "        next_network_1_loader = DataLoader(next_network_1_lst, batch_size=len(network_1_lst))\n",
    "        network_1_batch = next(iter(network_1_loader))\n",
    "        next_network_1_batch = next(iter(next_network_1_loader))\n",
    "\n",
    "        network_2_loader = DataLoader(network_2_lst, batch_size=len(network_2_lst))\n",
    "        next_network_2_loader = DataLoader(next_network_2_lst, batch_size=len(network_2_lst))\n",
    "        network_2_batch = next(iter(network_2_loader))\n",
    "        next_network_2_batch = next(iter(next_network_2_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting_1 = torch.squeeze(torch.tensor(np.array(job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        job_waiting_2 = torch.squeeze(torch.tensor(np.array(job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "\n",
    "        next_job_waiting_1 = torch.squeeze(torch.tensor(np.array(next_job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        next_job_waiting_2 = torch.squeeze(torch.tensor(np.array(next_job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "        \n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return [network_1_batch, network_2_batch], [job_waiting_1, job_waiting_2], a, r, [next_network_1_batch, next_network_2_batch], [next_job_waiting_1, next_job_waiting_2], prob_a, entropy, sojourn_time, action_mask_lst\n",
    "\n",
    "        \n",
    "    def make_batch(self):\n",
    "        network_1_lst, network_2_lst = [], []\n",
    "        next_network_1_lst, next_network_2_lst = [], []\n",
    "\n",
    "        job_waiting_1_lst, job_waiting_2_lst = [], []\n",
    "        next_job_waiting_1_lst, next_job_waiting_2_lst = [], []\n",
    "\n",
    "        a_lst, r_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "\n",
    "            r_lst.append([r/10.0])\n",
    "\n",
    "            network_1_lst.append(network[0])\n",
    "            network_2_lst.append(network[1])\n",
    "\n",
    "            job_waiting_1_lst.append(job_waiting[0].tolist())\n",
    "            job_waiting_2_lst.append(job_waiting[1].tolist())\n",
    "\n",
    "            a_lst.append([a])\n",
    "\n",
    "            next_network_1_lst.append(nxt_network[0])\n",
    "            next_network_2_lst.append(nxt_network[1])\n",
    "            \n",
    "            next_job_waiting_1_lst.append(nxt_job_waiting[0].tolist())\n",
    "            next_job_waiting_2_lst.append(nxt_job_waiting[1].tolist())\n",
    "\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_1_loader = DataLoader(network_1_lst, batch_size=len(network_1_lst))\n",
    "        next_network_1_loader = DataLoader(next_network_1_lst, batch_size=len(network_1_lst))\n",
    "        network_1_batch = next(iter(network_1_loader))\n",
    "        next_network_1_batch = next(iter(next_network_1_loader))\n",
    "\n",
    "        network_2_loader = DataLoader(network_2_lst, batch_size=len(network_2_lst))\n",
    "        next_network_2_loader = DataLoader(next_network_2_lst, batch_size=len(network_2_lst))\n",
    "        network_2_batch = next(iter(network_2_loader))\n",
    "        next_network_2_batch = next(iter(next_network_2_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting_1 = torch.squeeze(torch.tensor(np.array(job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        job_waiting_2 = torch.squeeze(torch.tensor(np.array(job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "\n",
    "        next_job_waiting_1 = torch.squeeze(torch.tensor(np.array(next_job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        next_job_waiting_2 = torch.squeeze(torch.tensor(np.array(next_job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "        \n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return [network_1_batch, network_2_batch], [job_waiting_1, job_waiting_2], a, r, [next_network_1_batch, next_network_2_batch], [next_job_waiting_1, next_job_waiting_2], prob_a, entropy, sojourn_time, action_mask_lst\n",
    "    \n",
    "    \n",
    "    def make_first_batch(self):\n",
    "        self.data = self.data[::-1]\n",
    "        \n",
    "        network_1_lst, network_2_lst = [], []\n",
    "        next_network_1_lst, next_network_2_lst = [], []\n",
    "\n",
    "        job_waiting_1_lst, job_waiting_2_lst = [], []\n",
    "        next_job_waiting_1_lst, next_job_waiting_2_lst = [], []\n",
    "\n",
    "        a_lst, r_lst, prob_a_lst, sojourn_time_lst, action_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        entropy_lst = []\n",
    "\n",
    "        for idx, transition in enumerate(self.data):\n",
    "            #self.history.append(transition)\n",
    "            network, job_waiting, a, r, nxt_network, nxt_job_waiting, prob_a, entropy, sojourn_time, action_mask = transition\n",
    "\n",
    "            r_lst.append([r/10.0])\n",
    "\n",
    "            network_1_lst.append(network[0])\n",
    "            network_2_lst.append(network[1])\n",
    "\n",
    "            job_waiting_1_lst.append(job_waiting[0].tolist())\n",
    "            job_waiting_2_lst.append(job_waiting[1].tolist())\n",
    "\n",
    "            a_lst.append([a])\n",
    "\n",
    "            next_network_1_lst.append(nxt_network[0])\n",
    "            next_network_2_lst.append(nxt_network[1])\n",
    "            \n",
    "            next_job_waiting_1_lst.append(nxt_job_waiting[0].tolist())\n",
    "            next_job_waiting_2_lst.append(nxt_job_waiting[1].tolist())\n",
    "\n",
    "            prob_a_lst.append([prob_a])\n",
    "            entropy_lst.append([entropy])\n",
    "            sojourn_time_lst.append([sojourn_time * 5])\n",
    "            action_mask_lst.append(action_mask)\n",
    "\n",
    "            if idx == batch_size - 1:\n",
    "                self.data = self.data[batch_size:]\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            self.data = []\n",
    "        \n",
    "        # r_lst = np.array(r_lst)\n",
    "        # r_lst = (r_lst - np.mean(r_lst)) / np.std(r_lst)\n",
    "        # r_lst = r_lst.tolist()\n",
    "        \n",
    "\n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_1_loader = DataLoader(network_1_lst, batch_size=len(network_1_lst))\n",
    "        next_network_1_loader = DataLoader(next_network_1_lst, batch_size=len(network_1_lst))\n",
    "        network_1_batch = next(iter(network_1_loader))\n",
    "        next_network_1_batch = next(iter(next_network_1_loader))\n",
    "\n",
    "        network_2_loader = DataLoader(network_2_lst, batch_size=len(network_2_lst))\n",
    "        next_network_2_loader = DataLoader(next_network_2_lst, batch_size=len(network_2_lst))\n",
    "        network_2_batch = next(iter(network_2_loader))\n",
    "        next_network_2_batch = next(iter(next_network_2_loader))\n",
    "\n",
    "        # print(job_waiting_lst)\n",
    "        \n",
    "        # job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        job_waiting_1 = torch.squeeze(torch.tensor(np.array(job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        job_waiting_2 = torch.squeeze(torch.tensor(np.array(job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        sojourn_time = torch.Tensor(sojourn_time_lst)\n",
    "\n",
    "        next_job_waiting_1 = torch.squeeze(torch.tensor(np.array(next_job_waiting_1_lst), dtype=torch.float), dim=1)\n",
    "        next_job_waiting_2 = torch.squeeze(torch.tensor(np.array(next_job_waiting_2_lst), dtype=torch.float), dim=1)\n",
    "        \n",
    "        prob_a = torch.tensor(prob_a_lst, dtype=torch.float)\n",
    "        entropy = torch.tensor(entropy_lst, dtype=torch.float)\n",
    "        action_mask_lst = torch.tensor(action_mask_lst)\n",
    "        \n",
    "        # self.data = []\n",
    "        return [network_1_batch, network_2_batch], [job_waiting_1, job_waiting_2], a, r, [next_network_1_batch, next_network_2_batch], [next_job_waiting_1, next_job_waiting_2], prob_a, entropy, sojourn_time, action_mask_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_net(self):\n",
    "        self = self.cuda()\n",
    "        first = True\n",
    "        pre_advantage = 0.0\n",
    "        while len(self.data) > 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            if first:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_first_batch()\n",
    "                first = False\n",
    "            else:\n",
    "                network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_batch()\n",
    "            \n",
    "            network_batch_1 = network_batch[0].clone().cuda()\n",
    "            network_batch_2 = network_batch[1].clone().cuda()\n",
    "\n",
    "            network_batch = [network_batch_1, network_batch_2]\n",
    "\n",
    "            job_waiting_1 = job_waiting[0].clone().cuda()\n",
    "            job_waiting_2 = job_waiting[1].clone().cuda()\n",
    "\n",
    "            job_waiting = [job_waiting_1, job_waiting_2]\n",
    "\n",
    "            a = a.clone().cuda()\n",
    "            r = r.clone().cuda()\n",
    "\n",
    "            next_network_batch_1 = next_network_batch[0].clone().cuda()\n",
    "            next_network_batch_2 = next_network_batch[1].clone().cuda()\n",
    "\n",
    "            next_network_batch = [next_network_batch_1, next_network_batch_2]\n",
    "\n",
    "            next_job_waiting_1 = next_job_waiting[0].clone().cuda()\n",
    "            next_job_waiting_2 = next_job_waiting[1].clone().cuda()\n",
    "\n",
    "            next_job_waiting = [next_job_waiting_1, next_job_waiting_2]\n",
    "\n",
    "            prob_a = prob_a.clone().cuda()\n",
    "            entropy = entropy.clone().cuda()\n",
    "            sojourn_time = sojourn_time.clone().cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).clone().cuda()\n",
    "            action_mask = action_mask.clone().cuda()\n",
    "\n",
    "            for i in range(2):\n",
    "                \n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                \n",
    "                delta = td_target - self.v([network_batch, job_waiting])\n",
    "                delta = delta.detach().to('cpu').numpy()\n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = (gamma_gpu**sojourn_time[i][0]) * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                # advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "\n",
    "                pre_advantage = advantage\n",
    "\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(self.v([network_batch, job_waiting]) , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                pi, current_entropy, outputs = self.pi([network_batch, job_waiting])\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                #outputs = outputs.clone().detach()\n",
    "\n",
    "\n",
    "                outputs = outputs * action_mask\n",
    "\n",
    "                exp_sum = torch.sum(torch.exp(outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "                exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "                outputs_a = outputs.gather(1,a)\n",
    "                pi_a = torch.exp(outputs_a) / exp_sum\n",
    "\n",
    "                # pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * advantage_lst\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage_lst\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train_net_history(self):\n",
    "        self = self.cuda()\n",
    "        # pre_advantage = 0.0\n",
    "        for i in range(history_learning_time):\n",
    "            torch.cuda.empty_cache()\n",
    "            network_batch, job_waiting, a, r, next_network_batch, next_job_waiting, prob_a, entropy, sojourn_time, action_mask = self.make_batch_history()\n",
    "\n",
    "            \n",
    "            network_batch_1 = network_batch[0].clone().cuda()\n",
    "            network_batch_2 = network_batch[1].clone().cuda()\n",
    "\n",
    "            network_batch = [network_batch_1, network_batch_2]\n",
    "\n",
    "            job_waiting_1 = job_waiting[0].clone().cuda()\n",
    "            job_waiting_2 = job_waiting[1].clone().cuda()\n",
    "\n",
    "            job_waiting = [job_waiting_1, job_waiting_2]\n",
    "\n",
    "            a = a.clone().cuda()\n",
    "            r = r.clone().cuda()\n",
    "\n",
    "            next_network_batch_1 = next_network_batch[0].clone().cuda()\n",
    "            next_network_batch_2 = next_network_batch[1].clone().cuda()\n",
    "\n",
    "            next_network_batch = [next_network_batch_1, next_network_batch_2]\n",
    "\n",
    "            next_job_waiting_1 = next_job_waiting[0].clone().cuda()\n",
    "            next_job_waiting_2 = next_job_waiting[1].clone().cuda()\n",
    "\n",
    "            next_job_waiting = [next_job_waiting_1, next_job_waiting_2]\n",
    "\n",
    "            prob_a = prob_a.clone().cuda()\n",
    "            entropy = entropy.clone().cuda()\n",
    "            sojourn_time = sojourn_time.clone().cuda()\n",
    "            gamma_gpu = torch.Tensor([gamma]).clone().cuda()\n",
    "            action_mask = action_mask.clone().cuda()\n",
    "\n",
    "\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                \n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                # print(\"r : \", r)\n",
    "                # print(\"td_target : \", td_target)\n",
    "                # print(\"v : \", self.v([network_batch, job_waiting]))\n",
    "                # delta = td_target - self.v([network_batch, job_waiting])\n",
    "                \"\"\"\n",
    "                \n",
    "                advantage_lst = []\n",
    "                advantage = pre_advantage\n",
    "                for i, delta_t in enumerate(delta):\n",
    "                    advantage = (gamma_gpu**sojourn_time[i][0]) * lmbda * advantage + delta_t[0]\n",
    "                    advantage_lst.append([advantage])\n",
    "                advantage_lst.reverse()\n",
    "                advantage_lst = torch.tensor(advantage_lst, dtype=torch.float).cuda()\n",
    "\n",
    "                pre_advantage = advantage\n",
    "                \"\"\"\n",
    "\n",
    "                v_loss = val_loss_coef * F.smooth_l1_loss(self.v([network_batch, job_waiting]) , td_target.detach())\n",
    "                #print(\"v_loss\", v_loss)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                v_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                td_target = r + (gamma_gpu**sojourn_time) * self.v([next_network_batch, next_job_waiting])\n",
    "                delta = td_target - self.v([network_batch, job_waiting])\n",
    "                # print(delta)\n",
    "\n",
    "                pi, current_entropy, outputs = self.pi([network_batch, job_waiting])\n",
    "\n",
    "                # true가 valid action\n",
    "\n",
    "                outputs = outputs * action_mask\n",
    "\n",
    "                exp_sum = torch.sum(torch.exp(outputs), dim=1) - torch.sum(action_mask.logical_not(), dim=1) # 0인 애들 빼줌\n",
    "                exp_sum = exp_sum.view(-1, 1) # 전치행렬\n",
    "\n",
    "                outputs_a = outputs.gather(1,a)\n",
    "                pi_a = torch.exp(pi) / exp_sum\n",
    "\n",
    "                pi_a = pi.gather(1,a)\n",
    "                ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                surr1 = ratio * delta.detach()\n",
    "                surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * delta.detach()\n",
    "                pi_loss = - torch.min(surr1, surr2) - entropy_weight * current_entropy\n",
    "\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                pi_loss.mean().backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global adjacency, entropy_weight, learning_rate\n",
    "    model = actor_network()\n",
    "    # model.load_state_dict(torch.load(\"./history30/model.pth\")) # -1 , +1\n",
    "    model.load_state_dict(torch.load(\"./history6/model.pth\")) # original\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "\n",
    "    adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    # node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    # node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    # link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    # job_waiting_state = env.get_job_waiting_vector()\n",
    "\n",
    "    # network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "\n",
    "    # for each time step\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = []\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(nodeNum)]\n",
    "    void_selected_num = 0\n",
    "\n",
    "    pre_state_1 = {}\n",
    "    pre_state_2 = {}\n",
    "    \n",
    "    while True:\n",
    "        model = model.to('cpu')\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state_1 = getOmnetMessage()\n",
    "            state_2 = getOmnetMessage()\n",
    "\n",
    "            if len(state_1) == 0:\n",
    "                state_1 = state_2\n",
    "            \n",
    "\n",
    "            if len(pre_state_1) == 0: # action 시작\n",
    "                state_1 = json.loads(state_1) # state 받았으므로 action 하면됨.\n",
    "                state_2 = json.loads(state_2) # state 받았으므로 action 하면됨.\n",
    "                \n",
    "            else:\n",
    "                state_1 = json.loads(state_1)\n",
    "                pre_state_1['jobWaiting'] = state_1['jobWaiting']\n",
    "                pre_state_1['sojournTime'] = state_1['sojournTime']\n",
    "                state_1 = pre_state_1\n",
    "\n",
    "                state_2 = json.loads(state_2)\n",
    "                pre_state_2['jobWaiting'] = state_2['jobWaiting']\n",
    "                pre_state_2['sojournTime'] = state_2['sojournTime']\n",
    "                state_2 = pre_state_2\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            node_waiting_state_1 = np.array(eval(str(state_1['nodeState'])))\n",
    "            node_processing_state_1 = np.array(eval(state_1['nodeProcessing']))\n",
    "            link_state_1 = np.array(eval(state_1['linkWaiting']))\n",
    "            job_waiting_state_1 = np.array(eval(state_1['jobWaiting']))\n",
    "            activated_job_list_1 = eval(state_1['activatedJobList'])\n",
    "            isAction_1 = int(state_1['isAction'])\n",
    "            reward_1 = float(state_1['reward'])\n",
    "            averageLatency_1 = float(state_1['averageLatency'])\n",
    "            completeJobNum_1 = int(state_1['completeJobNum'])\n",
    "            sojournTime_1 = float(state_1['sojournTime'])\n",
    "\n",
    "            node_waiting_state_2 = np.array(eval(str(state_2['nodeState'])))\n",
    "            node_processing_state_2 = np.array(eval(state_2['nodeProcessing']))\n",
    "            link_state_2 = np.array(eval(state_2['linkWaiting']))\n",
    "            job_waiting_state_2 = np.array(eval(state_2['jobWaiting']))\n",
    "            activated_job_list_2 = eval(state_2['activatedJobList'])\n",
    "            isAction_2 = int(state_2['isAction'])\n",
    "            reward_2 = float(state_2['reward'])\n",
    "            averageLatency_2 = float(state_2['averageLatency'])\n",
    "            completeJobNum_2 = int(state_2['completeJobNum'])\n",
    "            sojournTime_2 = float(state_2['sojournTime'])\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum_2 ,step)\n",
    "            writer.add_scalar(\"Reward/train\", reward_2, step)\n",
    "\n",
    "            # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            if averageLatency_2 == -1:\n",
    "                reward_2 = 0\n",
    "            else:\n",
    "                reward_2 = averageLatency_2\n",
    "                \n",
    "            first_sample = True\n",
    "            if is_train and len(pre_state_1) == 0:\n",
    "                if len(temp_history) > 0:\n",
    "                    temp_history[-1][8] = sojournTime_2\n",
    "                for history in temp_history:\n",
    "                    history[3] = reward_2\n",
    "                    model.history.append(history)\n",
    "                    model.put_data(history)\n",
    "\n",
    "            model.history = model.history[-replay_buffer_size:]\n",
    "            pre_state_1 = state_1\n",
    "            pre_state_2 = state_2\n",
    "\n",
    "            temp_history = []\n",
    "\n",
    "            job_index = int(state_2['jobIndex'])\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "\n",
    "            node_state_1 = np.concatenate((node_waiting_state_1,node_processing_state_1) ,axis = 1)\n",
    "            node_state_1 = torch.tensor(node_state_1, dtype=torch.float)\n",
    "\n",
    "            node_state_2 = np.concatenate((node_waiting_state_2,node_processing_state_2) ,axis = 1)\n",
    "            node_state_2 = torch.tensor(node_state_2, dtype=torch.float)\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state_1 = torch.tensor(link_state_1, dtype=torch.float)\n",
    "            link_state_2 = torch.tensor(link_state_2, dtype=torch.float)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = collections.deque()\n",
    "            for job in job_waiting_state_2:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state_1 = torch.tensor(job_waiting_state_1, dtype=torch.float).view(1, -1)\n",
    "            job_waiting_state_2 = torch.tensor(job_waiting_state_2, dtype=torch.float).view(1, -1)\n",
    "            # print(job_waiting_state)\n",
    "\n",
    "            network_state_1 = Data(x=node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "            network_state_2 = Data(x=node_state_2, edge_attr=link_state_2, edge_index=adjacency)\n",
    "\n",
    "            network_state = [network_state_1, network_state_2]\n",
    "            job_waiting_state = [job_waiting_state_1, job_waiting_state_2]\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward_2\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward_2 - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(nodeNum):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(nodeNum)] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward_2 != 0:\n",
    "                    with torch.no_grad():\n",
    "                        writer.add_scalar(\"Value/train\", model.v([network_state, job_waiting_state])[0], step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction_2 = False\n",
    "                \n",
    "\n",
    "            if isAction_2:\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = 1\n",
    "                for i in range(nodeNum):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[nodeNum:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "                #print(\"action start.\")\n",
    "                for order in range(len(subtasks)):\n",
    "                    if subtasks[order] == 0:\n",
    "                        break\n",
    "\n",
    "                    network_state_1 = Data(x=node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "                    network_state_2 = Data(x=node_state_2, edge_attr=link_state_2, edge_index=adjacency)\n",
    "                    network_state = [network_state_1, network_state_2]\n",
    "                    job_waiting_state = [job_waiting_state_1, job_waiting_state_2]\n",
    "                    # print(node_state.shape)\n",
    "                    # print(link_state.shape)\n",
    "                    # print(adjacency.shape)\n",
    "                    # print(job_waiting_state.shape)\n",
    "                    with torch.no_grad():\n",
    "                        prob, entropy, output = model.pi([network_state, job_waiting_state])\n",
    "\n",
    "                    writer.add_scalar(\"Entropy/train\", entropy, step)\n",
    "                    #print(f'prob : {prob}')\n",
    "                    \n",
    "                    m = Categorical(prob) \n",
    "                    node = m.sample().item()\n",
    "                    #print(f'node : {node}')\n",
    "                    \n",
    "                    # void action 실험용\n",
    "                    # node = nodeNum \n",
    "                    \n",
    "                    \n",
    "                    # void action 뽑으면\n",
    "                    if node == nodeNum and not scheduling_start: \n",
    "                        # print(\"void\")\n",
    "                        prob[0] = torch.Tensor([0] * nodeNum + [1.0])\n",
    "                        action_mask = [int(not scheduling_start) if i == node else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "\n",
    "                        temp_history.append([\n",
    "                            [network_state[0], network_state[1]], \n",
    "                            [job_waiting_state[0], job_waiting_state[1]], \n",
    "                            node, 0, \n",
    "                            [network_state[0], network_state[1]], \n",
    "                            [job_waiting_state[0], job_waiting_state[1]],\n",
    "                            prob[0][node].item(), \n",
    "                            entropy, 0, action_mask]\n",
    "                        )\n",
    "\n",
    "                        sendOmnetMessage(\"void\")\n",
    "\n",
    "                        #print(\"action finish.\")\n",
    "                        \n",
    "                        if getOmnetMessage() == \"ok\":\n",
    "                            void_selected_num += 1\n",
    "                            \n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        scheduling_start = True\n",
    "\n",
    "                    if scheduling_start:\n",
    "\n",
    "                        prob = torch.Tensor([F.softmax(output[0][:nodeNum], dim=0).tolist()])\n",
    "                        prob = torch.cat([prob[0], torch.tensor([0])]) # void action masking\n",
    "                        prob = torch.Tensor([prob.tolist()]).cuda()\n",
    "\n",
    "\n",
    "\n",
    "                        m = Categorical(prob[0])\n",
    "                        node = m.sample().item()\n",
    "                        \n",
    "\n",
    "                        offloading_vector.append(node)\n",
    "\n",
    "                        node_selected_num[node] += 1\n",
    "                        \n",
    "                        # state transition(환경으로부터 매번 state를 업데이트하는게 아닌, 현재 state를 기반으로 action에 해당하는 waiting만 더해줌).\n",
    "                        # 아래의 구체적인 코드는 이해하시기 보단 OMNeT++에서 받아온 데이터로 새로 짜시는게 빠를 것 같습니다.\n",
    "                        next_node_state_1 = node_state_2.clone().detach()\n",
    "                        next_job_waiting_state_1 = job_waiting_state_2.clone().detach()\n",
    "                        node_waiting_state_1 = node_waiting_state_2.copy()\n",
    "                        \n",
    "                        next_node_state_2 = node_state_2.clone().detach()\n",
    "                        next_job_waiting_state_2 = job_waiting_state_2.clone().detach()\n",
    "                        node_waiting_state_2 = node_waiting_state_2.copy()\n",
    "                        # print(next_job_waiting_state)\n",
    "                        \n",
    "                        next_node_state_2[node][modelNum * job_idx + order] += (subtasks[order]/100) # 100으로 나누는 이유 : 이렇게 해야 액션이 한쪽 노드로 쏠리게끔 학습이 되는 것을 어느정도 방지할 수 있는 것을 확인.\n",
    "                        node_waiting_state_2[node][modelNum * job_idx + order] += (subtasks[order]/100)\n",
    "\n",
    "                        # 100으로 안나눠주면 너무 큰 값이 state에 추가되어서 inference시 가중치랑 곱해지면서 액션이 한쪽으로 확 쏠리는 걸로 예상됨.\n",
    "\n",
    "                        next_network_state_1 = Data(x=next_node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "                        next_network_state_2 = Data(x=next_node_state_2, edge_attr=link_state_1, edge_index=adjacency)\n",
    "                        # next_job_waiting_state_1[0][nodeNum + order] = 0\n",
    "                        next_job_waiting_state_2[0][nodeNum + order] = 0\n",
    "\n",
    "                        next_network_state = [next_network_state_1, next_network_state_2]\n",
    "                        next_job_waiting_state = [next_job_waiting_state_1, next_job_waiting_state_2]\n",
    "\n",
    "                        action_mask = [int(not scheduling_start) if i == nodeNum else int(scheduling_start) for i in range(nodeNum + 1)]\n",
    "\n",
    "                        temp_history.append([\n",
    "                            [network_state[0], network_state[1]], \n",
    "                            [job_waiting_state[0], job_waiting_state[1]], \n",
    "                            node, 0, \n",
    "                            [next_network_state[0], next_network_state[1]], \n",
    "                            [next_job_waiting_state[0], next_job_waiting_state[1]],\n",
    "                            prob[0][node].item(), \n",
    "                            entropy, 0, action_mask]\n",
    "                        )\n",
    "                        # model.put_data([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy])\n",
    "                        node_state_1 = next_node_state_1\n",
    "                        node_state_2 = next_node_state_2\n",
    "\n",
    "                        job_waiting_state_1 = next_job_waiting_state_1\n",
    "                        job_waiting_state_2 = next_job_waiting_state_2\n",
    "\n",
    "\n",
    "                        pre_state_1['nodeState'] = str(node_waiting_state_1.tolist())\n",
    "                        pre_state_2['nodeState'] = str(node_waiting_state_2.tolist())\n",
    "\n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    \n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            pre_state_1 = {}\n",
    "            pre_state_2 = {}\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "            average_latency = float(episodic_reward['averageLatency'])\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            writer.add_scalar(\"CompleteNum/train\", complete_num, episode)\n",
    "            writer.add_scalar(\"averageLatency/train\", average_latency ,episode)\n",
    "\n",
    "            episode += 1\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            # learning_rate *= 0.995\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = pathName + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            average_reward = 0\n",
    "            average_reward_num = 0\n",
    "\n",
    "            # entropy_weight *= 0.99\n",
    "\n",
    "\n",
    "            # if len(model.data) > 0 and step % train_cycle == train_quitient:\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training....\")\n",
    "            #     model.train_net()\n",
    "            #     modelPathName = pathName + \"/model.pth\"\n",
    "            #     torch.save(model.state_dict(), modelPathName)\n",
    "            #     writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            #     average_reward = 0\n",
    "            #     average_reward_num = 0\n",
    "            #     tm = localtime(time.time())\n",
    "            #     time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "            #     print(f\"[{time_string}] training complete\")\n",
    "            if is_train:\n",
    "\n",
    "               \n",
    "                \n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training replay buffer....\")\n",
    "                model.train_net_history()\n",
    "                tm = localtime(time.time())\n",
    "                time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                print(f\"[{time_string}] training complete\")\n",
    "                \n",
    "                modelPathName = pathName + \"/model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.Tensor([[-0.0256,  0.0438,  0.0015, -0.0308, -0.0386, -0.0370, -0.0170, -0.0413]])\n",
    "print(output[0][:nodeNum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [17.941176], [17.941176], [17.941176], [17.941176], [17.941176], [43.333333], [43.333333], [43.333333], [43.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [55.333333], [10.0], [10.0], [10.0], [10.0], [10.0], [26.25], [26.25], [26.25], [26.25], [24.054054], [24.054054], [24.054054], [24.054054], [24.054054], [14.137931], [14.137931], [41.5], [41.5], [21.538462], [21.538462], [37.058824], [21.315789], [21.315789], [21.315789], [21.315789], [24.571429], [24.571429], [24.571429], [24.571429], [32.307692], [32.307692], [32.307692], [32.307692], [7.592593], [7.592593], [7.592593], [7.592593], [7.592593], [14.705882], [112.5], [21.666667], [21.666667], [21.666667], [21.666667], [21.666667], [19.574468], [19.574468], [19.574468], [19.574468], [19.574468], [68.0], [68.0], [68.0], [17.692308], [17.692308], [17.692308], [43.636364], [43.636364], [43.636364], [43.636364], [1.52381], [1.52381], [1.52381], [1.52381], [1.52381], [2.966102], [35.333333], [35.333333], [3.608247], [3.608247], [3.608247], [3.608247], [3.608247], [17.037037], [17.037037], [17.037037], [10.0], [10.0], [10.0], [34.074074], [34.074074], [34.074074], [34.074074], [10.0], [87.142857], [77.272727], [10.09901], [10.09901], [70.833333], [70.833333], [70.833333], [70.833333], [59.375], [59.375], [59.375], [59.375], [33.214286], [33.214286], [33.214286], [46.0], [46.0], [46.0], [46.0], [20.357143], [20.357143], [20.357143], [20.357143], [28.4375], [22.857143], [22.857143], [22.857143], [22.857143], [56.923077], [56.923077], [56.923077], [56.923077], [56.923077], [16.842105], [18.666667], [18.666667], [18.666667], [18.666667], [18.666667], [5.806452], [5.806452], [5.806452], [35.0], [35.0], [35.0], [35.0], [32.352941], [32.352941], [10.0], [10.0], [16.666667], [16.666667], [16.666667], [16.666667], [34.375], [34.375], [34.375], [34.375], [34.375], [35.172414], [35.172414], [35.172414], [14.0], [14.0], [14.0], [8.695652], [18.4], [16.153846], [16.153846], [16.153846], [16.153846], [16.153846], [27.575758], [27.575758], [14.761905], [14.761905], [14.761905], [22.1875], [22.1875], [59.333333], [10.0], [10.0], [10.0], [27.666667], [20.0], [20.0], [20.0], [118.0], [118.0], [118.0], [118.0], [10.0], [10.0], [19.677419], [19.677419], [19.677419], [19.677419], [40.769231], [40.769231], [40.769231], [19.393939], [19.393939], [19.393939], [19.393939], [7.173913], [7.173913], [7.173913], [31.052632], [31.052632], [10.0], [25.714286], [25.714286], [25.714286], [25.714286], [28.888889], [40.526316], [31.363636], [31.363636], [31.363636], [31.363636], [21.5], [21.5], [21.5], [92.857143], [92.857143], [92.857143], [8.148148], [10.0], [16.666667], [16.666667], [16.666667], [16.666667], [16.666667], [83.994253], [83.994253], [83.994253], [83.994253], [83.994253], [83.994253], [3.873874], [3.873874], [3.873874], [3.873874], [5.416667], [5.416667], [5.416667], [9.904762], [9.904762], [9.904762], [9.904762], [9.904762], [34.0], [26.956522], [26.956522], [26.956522], [3.714286], [3.714286], [3.714286], [3.714286], [7.037037], [7.037037], [7.037037], [7.037037], [24.615385], [16.25], [16.25], [27.142857], [27.142857], [4.736842], [4.736842], [4.736842], [10.0], [23.0], [23.0], [34.583333], [34.583333], [10.0], [10.0], [34.705882], [19.02439], [19.02439], [19.02439], [19.02439], [19.02439], [40.0], [40.0], [40.0], [44.444444], [44.444444], [44.444444], [44.444444], [44.444444], [20.416667], [20.416667], [20.416667], [5.30303], [5.30303], [5.30303], [6.666667], [6.666667]]\n",
    "\n",
    "a = np.array(a)\n",
    "\n",
    "print(sum(a))\n",
    "print(np.std(a))\n",
    "print(np.mean(a))\n",
    "a = (a-np.mean(a)) / np.std(a)\n",
    "a = a.tolist()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 2, 3, 4, 5])\n",
    "b = torch.Tensor([[1], [2], [3], [4], [5]])\n",
    "\n",
    "a = torch.Tensor([3])\n",
    "\n",
    "print(a**b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('omnetTest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12c9d4573c12dd45eabff63c44badb6fcd2b70b85de11a1a1b2c23254cbf5db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
