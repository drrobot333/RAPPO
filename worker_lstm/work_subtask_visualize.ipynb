{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "from actor_no_readout import actor_network\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendOmnetMessage(msg):\n",
    "    win32file.WriteFile(pipe, msg.encode('utf-8'))\n",
    "    \n",
    "def getOmnetMessage():\n",
    "    response_byte = win32file.ReadFile(pipe, BUFFER_SIZE)\n",
    "    response_str = response_byte[1].decode('utf-8')\n",
    "    return response_str\n",
    "\n",
    "def closePipe():\n",
    "    win32file.CloseHandle(pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실험 정보 관련 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordExpInfo(config):\n",
    "\n",
    "    networkInfo = config[\"network_info\"]\n",
    "\n",
    "    modelNum = int(networkInfo['modelNum'])\n",
    "    availableJobNum = int(networkInfo['availableJobNum'])\n",
    "    nodeNum = int(networkInfo['nodeNum'])\n",
    "    jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "    adjacency = eval(networkInfo['adjacencyList'])\n",
    "    episode_length = int(networkInfo['episode_length'])\n",
    "    node_capacity = networkInfo['node_capacity']\n",
    "    job_generate_rate = networkInfo['job_generate_rate']\n",
    "\n",
    "    node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "    queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "    hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "    reward_weight = 1/modelNum\n",
    "    entropy_weight = config[\"entropy_weight\"]\n",
    "    \n",
    "    info = f\"\"\"\n",
    "    노드 개수 : {nodeNum}\n",
    "    네트워크 최대 job 개수 : {availableJobNum}\n",
    "    job 대기 가능 개수 : {jobWaitingLength}\n",
    "    최대 subtask 개수 : {modelNum}\n",
    "    인접 리스트 : {adjacency}\n",
    "    node_feature_num : {node_feature_num}\n",
    "    queue_feature_num : {queue_feature_num}\n",
    "    episode_length : {episode_length}\n",
    "    node_capacity : {node_capacity}\n",
    "    entropy_weight : {entropy_weight}\n",
    "    reward_weight : {reward_weight}\n",
    "    job_generate_rate : {job_generate_rate}\n",
    "    \"\"\"\n",
    "    print(info)\n",
    "\n",
    "    with open(f'{config[\"path_name\"]}/info.txt', 'w') as f:\n",
    "        f.write(f'{info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def main(config):\n",
    "    global adjacency, writer\n",
    "\n",
    "    recordExpInfo(config)\n",
    "    \n",
    "    model = actor_network(config)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(total_params)\n",
    "    #model.load_state_dict(torch.load(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/experiment/subtask_reward/history13/model_100.pth\"))\n",
    "    #good_model = actor_network(config)\n",
    "    #good_model.load_state_dict(torch.load(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/experiment/subtask_reward/history13/model.pth\"))\n",
    "    # model.load_state_dict(torch.load(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/experiment/subtask_num_test_same_env/history17/model_1100.pth\"))\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "\n",
    "    adjacency = torch.tensor(adjacency, dtype=torch.long)\n",
    "\n",
    "    step = 1\n",
    "    episode = 1\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "    average_reward_num = 0\n",
    "\n",
    "    temp_history = deque([])\n",
    "    episode_history = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    isStop = False\n",
    "    node_selected_num = [0 for i in range(config[\"node_num\"])]\n",
    "    void_selected_num = 0\n",
    "\n",
    "    pre_state_1 = {}\n",
    "    pre_state_2 = {}\n",
    "\n",
    "    void_mask = [0] * config[\"node_num\"] + [1]\n",
    "    unvoid_mask = [1] * config[\"node_num\"] + [0]\n",
    "\n",
    "    episode_total_reward = 0\n",
    "    hidden = (torch.zeros(1, 1, config[\"lstm_hidden_num\"]), torch.zeros(1, 1, config[\"lstm_hidden_num\"]))\n",
    "\n",
    "    states = None\n",
    "    aaction = []\n",
    "    values = []\n",
    "    \n",
    "    while True:\n",
    "        # time.sleep(config[\"cpu_load_balance_time\"])\n",
    "        \n",
    "        model = model.to('cpu')\n",
    "        model.eval()\n",
    "        msg = getOmnetMessage()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            sendOmnetMessage(\"ok\")\n",
    "            state_1 = getOmnetMessage()\n",
    "            state_2 = getOmnetMessage()\n",
    "\n",
    "            if len(state_1) == 0:\n",
    "                state_1 = state_2\n",
    "            \n",
    "\n",
    "            if len(pre_state_1) == 0: # action 시작\n",
    "                state_1 = json.loads(state_1) # state 받았으므로 action 하면됨.\n",
    "                state_2 = json.loads(state_2) # state 받았으므로 action 하면됨.\n",
    "                \n",
    "            else:\n",
    "                state_1 = json.loads(state_1)\n",
    "                pre_state_1['jobWaiting'] = state_1['jobWaiting']\n",
    "                pre_state_1['sojournTime'] = state_1['sojournTime']\n",
    "                state_1 = pre_state_1\n",
    "\n",
    "                state_2 = json.loads(state_2)\n",
    "                pre_state_2['jobWaiting'] = state_2['jobWaiting']\n",
    "                pre_state_2['sojournTime'] = state_2['sojournTime']\n",
    "                state_2 = pre_state_2\n",
    "            \n",
    "            sendOmnetMessage(\"ok\") # 답장\n",
    "\n",
    "            \n",
    "            node_waiting_state_1 = torch.tensor(eval(str(state_1['nodeState'])), dtype=torch.float)\n",
    "            node_processing_state_1 = torch.tensor(eval(state_1['nodeProcessing']), dtype=torch.float)\n",
    "            link_state_1 = torch.tensor(eval(state_1['linkWaiting']), dtype=torch.float)\n",
    "            job_waiting_state_1 = torch.tensor(eval(state_1['jobWaiting']), dtype=torch.float)\n",
    "            activated_job_list_1 = eval(state_1['activatedJobList'])\n",
    "            isAction_1 = int(state_1['isAction'])\n",
    "            reward_1 = float(state_1['reward'])\n",
    "            averageLatency_1 = float(state_1['averageLatency'])\n",
    "            completeJobNum_1 = int(state_1['completeJobNum'])\n",
    "            sojournTime_1 = float(state_1['sojournTime'])\n",
    "            #startLatency_1 = float(state_1[\"startLatency\"])\n",
    "\n",
    "            node_waiting_state_2 = torch.tensor(eval(str(state_2['nodeState'])), dtype=torch.float)\n",
    "            node_processing_state_2 = torch.tensor(eval(state_2['nodeProcessing']), dtype=torch.float)\n",
    "            link_state_2 = torch.tensor(eval(state_2['linkWaiting']), dtype=torch.float)\n",
    "            job_waiting_state_2 = torch.tensor(eval(state_2['jobWaiting']), dtype=torch.float)\n",
    "            activated_job_list_2 = eval(state_2['activatedJobList'])\n",
    "            isAction_2 = int(state_2['isAction'])\n",
    "            reward_2 = float(state_2['reward'])\n",
    "            averageLatency_2 = float(state_2['averageLatency'])\n",
    "            completeJobNum_2 = int(state_2['completeJobNum'])\n",
    "            sojournTime_2 = float(state_2['sojournTime'])\n",
    "            #startLatency_2 = float(state_2[\"startLatency\"])\n",
    "\n",
    "            # print(reward_2)\n",
    "\n",
    "\n",
    "\n",
    "            # node_waiting_state_2 = torch.tensor(eval(str(state_1['nodeState'])), dtype=torch.float)\n",
    "            # node_processing_state_2 = torch.tensor(eval(state_1['nodeProcessing']), dtype=torch.float)\n",
    "            # link_state_2 = torch.tensor(eval(state_1['linkWaiting']), dtype=torch.float)\n",
    "            # job_waiting_state_2 = torch.tensor(eval(state_1['jobWaiting']), dtype=torch.float)\n",
    "            # activated_job_list_2 = eval(state_1['activatedJobList'])\n",
    "            # isAction_2 = int(state_1['isAction'])\n",
    "            # reward_2 = float(state_1['reward'])\n",
    "            # averageLatency_2 = float(state_1['averageLatency'])\n",
    "            # completeJobNum_2 = int(state_1['completeJobNum'])\n",
    "            # sojournTime_2 = float(state_1['sojournTime'])\n",
    "\n",
    "            writer.add_scalar(\"completeJobNum/train\", completeJobNum_2 ,step)\n",
    "\n",
    "            #print(node_waiting_state_2)\n",
    "            #print(node_processing_state_2)\n",
    "            \n",
    "\n",
    "            # # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "            # if averageLatency_2 == -1:\n",
    "            #     reward_2 = 0\n",
    "            # else:\n",
    "            #     reward_2 = completeJobNum_2\n",
    "\n",
    "            # reward_2 = startLatency_2 * 100\n",
    "\n",
    "            #print(reward_2)\n",
    "            \n",
    "            writer.add_scalar(\"Reward/train\", reward_2, step)\n",
    "\n",
    "            episode_total_reward += reward_2\n",
    "\n",
    "            if reward_2 != 0:\n",
    "                rewards.append(reward_2)\n",
    "                \n",
    "            first_sample = True\n",
    "            if config[\"is_train\"] and len(pre_state_1) == 0:\n",
    "                if temp_history:\n",
    "                    # temp_history[-1][3] = reward_2\n",
    "                    temp_history[-1][4] = network_state\n",
    "                    temp_history[-1][5] = job_waiting_state\n",
    "\n",
    "                while temp_history:\n",
    "                    history = temp_history.popleft()\n",
    "                    \n",
    "                    # model.history.append(history)\n",
    "                    episode_history.append(history)\n",
    "                    model.put_data(history)\n",
    "\n",
    "            \n",
    "\n",
    "            temp_history = deque([])\n",
    "\n",
    "            job_index = int(state_2['jobIndex'])\n",
    "            #print(\"job_index: \", job_index)\n",
    "\n",
    "            #print('sojourn time :', sojournTime)\n",
    "\n",
    "            #print(node_waiting_state_1)\n",
    "            #print(node_processing_state_1)\n",
    "            node_state_1 = np.concatenate((node_waiting_state_1,node_processing_state_1) ,axis = 1)\n",
    "            node_state_1 = torch.concat([node_waiting_state_1, node_processing_state_1], dim=1)\n",
    "            #print(node_state_1)\n",
    "            #node_state_1 = node_waiting_state_1\n",
    "            node_state_2 = np.concatenate((node_waiting_state_2,node_processing_state_2) ,axis = 1)\n",
    "            node_state_2 = torch.concat([node_waiting_state_2, node_processing_state_2], dim=1)\n",
    "            #print(node_state_2)\n",
    "            #node_state_2 = node_waiting_state_2\n",
    "\n",
    "            #print(reward)\n",
    "\n",
    "            link_state_1 = torch.tensor(link_state_1, dtype=torch.float)\n",
    "            link_state_2 = torch.tensor(link_state_2, dtype=torch.float)\n",
    "            #print(link_state_1)\n",
    "            #print(link_state_2)\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = deque()\n",
    "            for job in job_waiting_state_2:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "            \n",
    "            job_waiting_state_1 = job_waiting_state_1.view(1, -1)\n",
    "            job_waiting_state_2 = job_waiting_state_2.view(1, -1)\n",
    "            #print(job_waiting_state_1)\n",
    "            #print(job_waiting_state_2)\n",
    "\n",
    "            network_state_1 = Data(x=node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "            network_state_2 = Data(x=node_state_2, edge_attr=link_state_2, edge_index=adjacency)\n",
    "\n",
    "            network_state = [network_state_1, network_state_2]\n",
    "            job_waiting_state = [job_waiting_state_1, job_waiting_state_2]\n",
    "\n",
    "            pre_state_1 = state_1\n",
    "            pre_state_2 = state_2\n",
    "            \n",
    "            if average_reward_num == 0:\n",
    "                average_reward = reward_2\n",
    "                average_reward_num = 1\n",
    "            else:\n",
    "                average_reward = average_reward + (reward_2 - average_reward)/(average_reward_num + 1)\n",
    "                average_reward_num += 1\n",
    "                \n",
    "            if step > 1:\n",
    "                for i in range(config[\"node_num\"]):\n",
    "                    node_tag = \"node/\" + str(i) + \"/train\"\n",
    "                    writer.add_scalar(node_tag, node_selected_num[i], step)\n",
    "\n",
    "                writer.add_scalar(\"node/void/train\", void_selected_num, step)\n",
    "                    \n",
    "                node_selected_num = [0 for i in range(config[\"node_num\"])] # node selected num 초기화\n",
    "                void_selected_num = 0\n",
    "\n",
    "                if reward_2 != 0:\n",
    "                    with torch.no_grad():\n",
    "                        state = model.gnn([network_state, job_waiting_state])\n",
    "                        writer.add_scalar(\"Value/train\", torch.mean(model.v(state)), step)\n",
    "                \n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            \n",
    "            \n",
    "            # print(job_waiting_queue)\n",
    "            if job_waiting_num == 0:\n",
    "                isAction_2 = False\n",
    "                \n",
    "\n",
    "            if isAction_2:\n",
    "                \"\"\"if step % config[\"T_horizon\"] == 0:\n",
    "\n",
    "                    print(\"hello\")\n",
    "\n",
    "                    if config[\"is_train\"]:\n",
    "                        if episode % 100 == 0:\n",
    "                            tm = localtime(time.time())\n",
    "                            time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                            print(f\"[{time_string}] training....\")\n",
    "                        model.train_net()\n",
    "                        if episode % 100 == 0:\n",
    "                            tm = localtime(time.time())\n",
    "                            time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                            print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                        model = model.cpu()\"\"\"\n",
    "\n",
    "\n",
    "                job_idx = job_index\n",
    "                job = job_waiting_queue.popleft()\n",
    "                src = -1\n",
    "                dst = 1\n",
    "                for i in range(config[\"node_num\"]):\n",
    "                    if job[i] == -1:\n",
    "                        src = i\n",
    "                    if job[i] == 1:\n",
    "                        dst = i\n",
    "\n",
    "                if src == -1:\n",
    "                    src = dst\n",
    "                    \n",
    "                #print(f\"src : {src}, dst : {dst}\")\n",
    "                #print(job)\n",
    "                subtasks = job[config[\"node_num\"]:]\n",
    "                offloading_vector = []\n",
    "                temp_data = []\n",
    "                scheduling_start = False\n",
    "                # print(subtasks)\n",
    "                step += 1\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    feature = model.gnn([network_state, job_waiting_state])\n",
    "                    feature = F.normalize(feature, dim=1)\n",
    "                    new_feature = feature.unsqueeze(0)\n",
    "                    first_prob, entropy, output, hidden = model.pi([new_feature, hidden])\n",
    "\n",
    "                    if states == None:\n",
    "                        states = new_feature.squeeze(0)\n",
    "                    else:\n",
    "                        states = torch.cat([states, new_feature.squeeze(0)])\n",
    "\n",
    "                    #values.append(good_model.v(new_feature))\n",
    "\n",
    "                writer.add_scalar(\"Entropy/train\", torch.mean(entropy).item(), step)\n",
    "                #print(f'prob : {prob}')\n",
    "\n",
    "                # isVoid = F.sigmoid(dists[modelNum].sample())\n",
    "\n",
    "                m = Categorical(first_prob[0][0]) # 첫 번째 batch의 첫 번째 node+void개의 확률들\n",
    "                nodes = m.sample()\n",
    "\n",
    "                node = nodes.item()\n",
    "\n",
    "                aaction.append(node)\n",
    "\n",
    "                #print(f'node : {node}')\n",
    "                \n",
    "                # void action 실험용\n",
    "                # node = nodeNum \n",
    "                \n",
    "                \n",
    "                # void action 뽑으면 void만 업데이트\n",
    "                if node == config[\"node_num\"] and not scheduling_start: \n",
    "                    actions.append(node)\n",
    "                    action_mask = void_mask\n",
    "\n",
    "                    temp_history.append([\n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]], \n",
    "                        node, 0, \n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]],\n",
    "                        1, \n",
    "                        0, action_mask, 0]\n",
    "                    )\n",
    "\n",
    "                    sendOmnetMessage(\"void\")\n",
    "\n",
    "                    #print(\"action finish.\")\n",
    "                    \n",
    "                    if getOmnetMessage() == \"ok\":\n",
    "                        void_selected_num += 1\n",
    "\n",
    "                else:\n",
    "                    scheduling_start = True\n",
    "\n",
    "                if scheduling_start:\n",
    "                    \n",
    "                    if random.random() > config[\"imitation_probability\"]:\n",
    "                            config[\"our\"] = True\n",
    "                    else:\n",
    "                        config[\"our\"] = False\n",
    "                    \n",
    "\n",
    "                    for sub_index in range(config[\"model_num\"]):\n",
    "                        #start_time = time.time()\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            feature = model.gnn([network_state, job_waiting_state])\n",
    "                            feature = F.normalize(feature, dim=1)\n",
    "                            new_feature = feature.unsqueeze(0)\n",
    "                            prob, entropy, output, hidden = model.pi([new_feature, hidden])\n",
    "\n",
    "                            if states == None:\n",
    "                                states = new_feature.squeeze(0)\n",
    "                            else:\n",
    "                                states = torch.cat([states, new_feature.squeeze(0)])\n",
    "\n",
    "                            #values.append(good_model.v(new_feature))\n",
    "                            # print(prob)\n",
    "                        #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "                        # print(new_feature.shape)\n",
    "\n",
    "                        writer.add_scalar(\"Entropy/train\", torch.mean(entropy).item(), step)\n",
    "\n",
    "                        output = output[:, :, 0:-1].squeeze(0)\n",
    "\n",
    "                        prob = F.softmax(output, dim=1)\n",
    "                        prob = torch.concat([prob, torch.zeros(1, 1)], dim=1)\n",
    "\n",
    "                        m = Categorical(prob)\n",
    "                        nodes = m.sample()\n",
    "                        action_mask = unvoid_mask\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "                        if config[\"our\"]:\n",
    "                            node = nodes[0].item()\n",
    "                        else:\n",
    "                            node = torch.argmin(node_waiting_state_2[:,0]).item()\n",
    "\n",
    "                        aaction.append(node)\n",
    "\n",
    "                        next_node_state_1 = node_state_2.clone() # node_state_1을 node_state2로 복사 node_state_2가 최근인데..\n",
    "\n",
    "                        # print(node)\n",
    "\n",
    "                        #print(node_state_1)\n",
    "\n",
    "                        next_node_state_2 = node_state_2.clone()\n",
    "                        #print(\"node:\", node)\n",
    "                        #print(\"add_index:\",job_idx * config[\"model_num\"] + sub_index)\n",
    "                        next_node_state_2[node][job_idx * config[\"model_num\"] + sub_index] = subtasks[sub_index]\n",
    "\n",
    "                        #print(next_node_state_2)\n",
    "\n",
    "\n",
    "                        next_network_state_1 = Data(x=next_node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "                        next_network_state_2 = Data(x=next_node_state_2, edge_attr=link_state_2, edge_index=adjacency)\n",
    "\n",
    "                        next_network_state = [next_network_state_1, next_network_state_2]\n",
    "                        # allJobWait, allJobWaitTime, power, jobRemain\n",
    "                        next_job_waiting_state = [job_waiting_state_1, job_waiting_state_2]\n",
    "\n",
    "                        temp_history.append([\n",
    "                        [network_state[0], network_state[1]], \n",
    "                        [job_waiting_state[0], job_waiting_state[1]], \n",
    "                        node, 0, \n",
    "                        next_network_state, \n",
    "                        next_job_waiting_state,\n",
    "                        prob[0][node].item(),\n",
    "                        0, action_mask, 0]\n",
    "                        )\n",
    "\n",
    "                        offloading_vector.append(node)\n",
    "                        node_selected_num[node] += 1\n",
    "                        actions.append(node)\n",
    "\n",
    "                        node_state_1 = next_node_state_1.clone()\n",
    "                        node_state_2 = next_node_state_2.clone()\n",
    "\n",
    "                        network_state_1 = Data(x=node_state_1, edge_attr=link_state_1, edge_index=adjacency)\n",
    "                        network_state_2 = Data(x=node_state_2, edge_attr=link_state_2, edge_index=adjacency)\n",
    "\n",
    "                        network_state = [network_state_1, network_state_2]\n",
    "                        job_waiting_state = next_job_waiting_state\n",
    "\n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    msg = str(offloading_vector)\n",
    "                    sendOmnetMessage(msg)\n",
    "                    \n",
    "                    #print(\"action finish.\")\n",
    "                    if(getOmnetMessage() == \"ok\"):\n",
    "                        pass\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            \n",
    "            sendOmnetMessage(\"ok\")\n",
    "            pre_state_1 = {}\n",
    "            pre_state_2 = {}\n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "\n",
    "\n",
    "            ## to visualize\n",
    "            # print(states.shape)\n",
    "            # print(len(aaction))\n",
    "            # print(len(values))\n",
    "\n",
    "            # length = 10000\n",
    "\n",
    "            # if len(aaction) >= length:\n",
    "            #     torch.save(states, f\"state_{length}.pt\")\n",
    "            #     torch.save(torch.tensor(aaction), f\"action_{length}.pt\")\n",
    "            #     torch.save(torch.tensor(values), f\"value_{length}.pt\")\n",
    "\n",
    "            #     return\n",
    "            ## to visualize\n",
    "\n",
    "\n",
    "\n",
    "            # print(len(model.history))\n",
    "\n",
    "            mean = np.mean(rewards)\n",
    "            std = np.std(rewards)\n",
    "\n",
    "            # print(actions)\n",
    "\n",
    "            #print(mean)\n",
    "            #print(std)\n",
    "            #print(rewards)\n",
    "\n",
    "            #for i in range(len(episode_history)):\n",
    "            #    if episode_history[i][3] != 0:\n",
    "            #        episode_history[i][3] = (episode_history[i][3] - mean) / (std + 1e-10)\n",
    "            #        #print((episode_history[i][3] - mean) / (std + 1e-10))\n",
    "\n",
    "            rewards = []\n",
    "            actions = []\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            episodic_reward = getOmnetMessage()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "            average_latency = float(episodic_reward['averageLatency'])\n",
    "            jitter = episodic_reward['jitter']\n",
    "            jitterMake = episodic_reward['jitterMake']\n",
    "            action_id = episodic_reward['action_id']\n",
    "            action_reward = episodic_reward['action_reward']\n",
    "            #print(list(map(float, jitter.strip().split(\" \"))))\n",
    "            #print(list(map(float, jitterMake.strip().split(\" \"))))\n",
    "            #print(list(map(int, action_id.strip().split(\" \"))))\n",
    "            #print(list(map(float, action_reward.strip().split(\" \"))))\n",
    "\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            writer.add_scalar(\"EpisodicReward/train\", finish_num, episode)\n",
    "            writer.add_scalar(\"NormalizedEpisodicReward/train\", normalized_finish_num, episode)\n",
    "            writer.add_scalar(\"CompleteNum/train\", complete_num, episode)\n",
    "            writer.add_scalar(\"averageLatency/train\", average_latency ,episode)\n",
    "\n",
    "            # model.history.append(episode_history)\n",
    "            #aaaa = [episode_history[i][2] for i in range(len(episode_history))]\n",
    "            #print(len(aaaa))\n",
    "            #print(aaaa)\n",
    "            #rrrr = [episode_history[i][3] for i in range(len(episode_history))]\n",
    "            #print(len(rrrr))\n",
    "            #print(rrrr)\n",
    "            \n",
    "            model.data = episode_history[:]\n",
    "            data_length = len(model.data)\n",
    "            reward_list = [(data_length - i) * -0.01 for i in range(data_length)]\n",
    "\n",
    "            action_id_list = list(map(int, action_id.strip().split(\" \"))) if action_id != '' else []\n",
    "            action_reward_list = list(map(float, action_reward.strip().split(\" \"))) if action_reward != '' else []\n",
    "\n",
    "            action_id_reward_list = sorted(zip(action_id_list, action_reward_list))\n",
    "\n",
    "            #print(action_id_reward_list)\n",
    "\n",
    "            for i in range(len(action_id_reward_list)):\n",
    "                index = action_id_reward_list[i][0]\n",
    "                if index < data_length:\n",
    "                    # total_latency_bonus = 1/average_latency if average_latency > 0 else 0\n",
    "                    reward_list[index] = action_id_reward_list[i][1]\n",
    "\n",
    "            #print(reward_list)\n",
    "\n",
    "            #for i in range(len(reward_list)):\n",
    "            #    if i > 0:\n",
    "            #        reward_list[i] = reward_list[i-1] * 0.3 + reward_list[i]\n",
    "\n",
    "            #print(reward_list)\n",
    "\n",
    "            for i in range(len(model.data)):\n",
    "                model.data[i][3] = reward_list[i]\n",
    "\n",
    "            tt = []\n",
    "            for i in range(len(model.data)):\n",
    "                tt.append(model.data[i][3])\n",
    "            #print(tt)\n",
    "            #print(len(model.data))\n",
    "            #print(tt)\n",
    "\n",
    "            model.history.append(model.data[:])\n",
    "            \n",
    "            episode_history = []\n",
    "            temp_history = deque([])\n",
    "\n",
    "            \n",
    "\n",
    "            episode_total_reward += complete_num\n",
    "\n",
    "            writer.add_scalar(\"episode_total_reward/train\", episode_total_reward, episode)\n",
    "\n",
    "            episode_total_reward = 0\n",
    "            hidden = (torch.zeros(1, 1, config[\"lstm_hidden_num\"]), torch.zeros(1, 1, config[\"lstm_hidden_num\"]))\n",
    "\n",
    "            episode += 1\n",
    "            sendOmnetMessage(\"ok\")\n",
    "\n",
    "            config[\"entropy_weight\"] = max(0.0001, config[\"entropy_weight\"] * config[\"entropy_gamma\"])\n",
    "            config[\"imitation_probability\"] = max(config[\"imitation_gamma\"] * config[\"imitation_probability\"], 0.2)\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = config[\"path_name\"] + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            writer.add_scalar(\"AverageReward/train\", average_reward, step)\n",
    "            average_reward = 0\n",
    "            average_reward_num = 0\n",
    "\n",
    "            if config[\"is_train\"] and episode % config[\"data_collect_time\"] == 0:\n",
    "                \n",
    "                if episode % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training....\")\n",
    "                \n",
    "                #model.train_net()\n",
    "                if episode % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training replay buffer....\")\n",
    "\n",
    "                for i in range(config[\"data_collect_time\"]):\n",
    "                    model.train_net_history()\n",
    "                    \n",
    "                if episode % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                model.clear_data()\n",
    "                model.history = []\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    modelPathName = config[\"path_name\"] + \"/model.pth\"\n",
    "                    torch.save(model.state_dict(), modelPathName)\n",
    "                    modelPathName = config[\"path_name\"] + f\"/model_{episode}.pth\"\n",
    "                    torch.save(model.state_dict(), modelPathName)\n",
    "\n",
    "                    time.sleep(10)\n",
    "\n",
    "                model.eval()\n",
    "                \n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통신 관련 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\worker_right_latency_sub_trace\"\n",
    "BUFFER_SIZE = 200000\n",
    "\n",
    "try:\n",
    "    pipe = win32pipe.CreateNamedPipe(\n",
    "        PIPE_NAME,\n",
    "        win32pipe.PIPE_ACCESS_DUPLEX,\n",
    "        win32pipe.PIPE_TYPE_MESSAGE | win32pipe.PIPE_READMODE_MESSAGE | win32pipe.PIPE_WAIT,\n",
    "        1,\n",
    "        BUFFER_SIZE,\n",
    "        BUFFER_SIZE,\n",
    "        0,\n",
    "        None\n",
    "    )    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "win32pipe.ConnectNamedPipe(pipe, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_message = getOmnetMessage()\n",
    "networkInfo = json.loads(initial_message)\n",
    "\n",
    "modelNum = int(networkInfo['modelNum'])\n",
    "availableJobNum = int(networkInfo['availableJobNum'])\n",
    "nodeNum = int(networkInfo['nodeNum'])\n",
    "jobWaitingLength = int(networkInfo['jobWaitingQueueLength'])\n",
    "adjacency = eval(networkInfo['adjacencyList'])\n",
    "episode_length = int(networkInfo['episode_length'])\n",
    "node_capacity = networkInfo['node_capacity']\n",
    "job_generate_rate = networkInfo['job_generate_rate']\n",
    "\n",
    "node_feature_num = 2 * (modelNum * availableJobNum)\n",
    "queue_feature_num = (nodeNum + modelNum) * jobWaitingLength\n",
    "hidden_feature_num = 10*(node_feature_num + queue_feature_num)\n",
    "reward_weight = 1/modelNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history21\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/user/Desktop/suhwan/connection_test/python_agent/experiment/subtask_reward\")\n",
    "folderList = glob.glob(\"history*\")\n",
    "\n",
    "pathName = \"history\" + str(len(folderList))\n",
    "\n",
    "print(pathName)\n",
    "\n",
    "os.mkdir(pathName)\n",
    "\n",
    "writer = SummaryWriter(pathName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 초기화 완료\n",
      "\n",
      "    노드 개수 : 5\n",
      "    네트워크 최대 job 개수 : 10\n",
      "    job 대기 가능 개수 : 15\n",
      "    최대 subtask 개수 : 3\n",
      "    인접 리스트 : [[0, 1, 0, 2, 1, 2, 1, 3, 2, 3, 2, 4, 3, 4], [1, 0, 2, 0, 2, 1, 3, 1, 3, 2, 4, 2, 4, 3]]\n",
      "    node_feature_num : 60\n",
      "    queue_feature_num : 120\n",
      "    episode_length : 500\n",
      "    node_capacity : 0.100000, 0.300000\n",
      "    entropy_weight : 0.002\n",
      "    reward_weight : 0.3333333333333333\n",
      "    job_generate_rate : 5\n",
      "    \n",
      "3308663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20952\\3257271219.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  link_state_1 = torch.tensor(link_state_1, dtype=torch.float)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20952\\3257271219.py:183: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  link_state_2 = torch.tensor(link_state_2, dtype=torch.float)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'good_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\worker_lstm\\work_subtask.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m네트워크 초기화 완료\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m         : \u001b[39m0.0001\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m\"\u001b[39m                 : \u001b[39m0.9\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimitation_gamma\u001b[39m\u001b[39m\"\u001b[39m       : \u001b[39m1.0\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m main(config)\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\worker_lstm\\work_subtask.ipynb Cell 11\u001b[0m in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=288'>289</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=289'>290</a>\u001b[0m         states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([states, new_feature\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=291'>292</a>\u001b[0m     values\u001b[39m.\u001b[39mappend(good_model\u001b[39m.\u001b[39mv(new_feature))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=293'>294</a>\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mEntropy/train\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mmean(entropy)\u001b[39m.\u001b[39mitem(), step)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=294'>295</a>\u001b[0m \u001b[39m#print(f'prob : {prob}')\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=295'>296</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X13sZmlsZQ%3D%3D?line=296'>297</a>\u001b[0m \u001b[39m# isVoid = F.sigmoid(dists[modelNum].sample())\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'good_model' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sendOmnetMessage(\"init\") # 입력 끝나면 omnet에 전송\n",
    "    print(\"네트워크 초기화 완료\")\n",
    "\n",
    "    config = {\n",
    "        \"learning_rate\"         : 0.0001,\n",
    "        \"gamma\"                 : 0.9,\n",
    "        \"entropy_weight\"        : 0.002,\n",
    "        \"entropy_gamma\"         : 0.9995,\n",
    "        \"lambda\"                : 0.99,\n",
    "        \"eps_clip\"              : 0.1,\n",
    "        \"batch_size\"            : 256,\n",
    "        \"loss_coef\"             : 0.5,\n",
    "        \"job_generate_rate\"     : 0.003,\n",
    "        \"is_train\"              : True,\n",
    "        \"replay_buffer_size\"    : 100,\n",
    "        \"data_collect_time\"     : 10,\n",
    "        \"history_learning_time\" : 2,\n",
    "        \"current_learning_time\" : 2,\n",
    "        \"node_feature_num\"      : 2 * (modelNum * availableJobNum),\n",
    "        \"queue_feature_num\"     : (nodeNum + modelNum) * jobWaitingLength,\n",
    "        \"hidden_feature_num\"    : 128,\n",
    "        \"reward_weight\"         : 1.0/3,\n",
    "        \"node_num\"              : nodeNum,\n",
    "        \"model_num\"             : modelNum,\n",
    "        \"lstm_hidden_num\"       : 256,\n",
    "        \"cpu_load_balance_time\" : 0.1,\n",
    "        \"network_info\"          : networkInfo,\n",
    "        \"path_name\"             : pathName,\n",
    "        \"T_horizon\"             : 1000,\n",
    "        \"link_num\"              : 32,\n",
    "        \"state_weight\"          : 1.0,\n",
    "        \"our\"                   : True,\n",
    "        \"imitation_probability\" : 0.0,\n",
    "        \"imitation_gamma\"       : 1.0,\n",
    "    }\n",
    "\n",
    "\n",
    "    main(config)\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\connection_test\\python_agent\\worker_lstm\\work_subtask.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m88\u001b[39m, \u001b[39m88\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/connection_test/python_agent/worker_lstm/work_subtask.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(a\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "a = np.random.rand(1, 88, 88)\n",
    "\n",
    "print(a.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12c9d4573c12dd45eabff63c44badb6fcd2b70b85de11a1a1b2c23254cbf5db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
