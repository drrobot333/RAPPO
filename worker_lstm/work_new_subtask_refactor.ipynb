{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\omnetTest\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from collections import deque\n",
    "import random\n",
    "from DotDict import DotDict\n",
    "\n",
    "from Communicator import Communicator\n",
    "from Manager import Manager\n",
    "from Simulator import Simulator\n",
    "\n",
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import sys \n",
    "import win32pipe, win32file, pywintypes\n",
    "\n",
    "from actor_no_readout import actor_network\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = None\n",
    "manager = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \n",
    "    model = actor_network(config)\n",
    "    total_params = model.return_parameter_number()\n",
    "    print(total_params)\n",
    "\n",
    "    if config.load_model:\n",
    "        model.load_state_dict(torch.load(config.model_path))\n",
    "\n",
    "    max_reward = 0\n",
    "\n",
    "    average_reward = 0\n",
    "\n",
    "    temp_history = deque([])\n",
    "    episode_history = []\n",
    "    actions = []\n",
    "\n",
    "    void_mask = [0] * config[\"node_num\"] + [1]\n",
    "    unvoid_mask = [1] * config[\"node_num\"] + [0]\n",
    "\n",
    "    episode_total_reward = 0\n",
    "    hidden = (torch.zeros(1, 1, config[\"lstm_hidden_num\"]), torch.zeros(1, 1, config[\"lstm_hidden_num\"]))\n",
    "    \n",
    "    while True:\n",
    "        # time.sleep(config[\"cpu_load_balance_time\"])\n",
    "        \n",
    "        model = model.to('cpu')\n",
    "        msg = env.return_env_status()\n",
    "        \n",
    "        if msg == \"action\": # omnet의 메세지, state 받으면 됨\n",
    "            \n",
    "            cur_state = env.return_state()\n",
    "\n",
    "            manager.record_summary(\"completeJobNum/train\", cur_state[\"cur_network\"][\"complete_job_num\"], env.get_time_step())\n",
    "            manager.record_summary(\"reward/train\", cur_state[\"cur_network\"][\"reward\"], env.get_time_step())\n",
    "            manager.update_episode_total_reward(cur_state[\"cur_network\"][\"reward\"])\n",
    "\n",
    "            \n",
    "            if config.is_train: # 보류\n",
    "                if temp_history:\n",
    "                    # temp_history[-1][3] = reward_2\n",
    "                    temp_history[-1][4] = network_state\n",
    "                    temp_history[-1][5] = job_waiting_state\n",
    "\n",
    "                while temp_history:\n",
    "                    history = temp_history.popleft()\n",
    "                    \n",
    "                    # model.history.append(history)\n",
    "                    episode_history.append(history)\n",
    "                    model.put_data(history)\n",
    "\n",
    "            network_state = [cur_state[\"pre_network\"][\"network_state\"], cur_state[\"cur_network\"][\"network_state\"]]\n",
    "            job_waiting_state = [cur_state[\"pre_network\"][\"job_waiting_state\"], cur_state[\"cur_network\"][\"job_waiting_state\"]]\n",
    "\n",
    "            temp_history = deque([])\n",
    "\n",
    "\n",
    "            job_waiting_num = 0\n",
    "            job_waiting_queue = deque()\n",
    "            for job in cur_state[\"cur_network\"][\"job_waiting_state\"]:\n",
    "                if any(job): # 하나라도 0이 아닌 것 이 있으면 job이 있는것임.\n",
    "                    job_waiting_num += 1\n",
    "                    job_waiting_queue.append(job)\n",
    "                \n",
    "            if env.get_time_step() > 1:\n",
    "                if cur_state[\"cur_network\"][\"reward\"] != 0:\n",
    "                    with torch.no_grad():\n",
    "                        state = model.gnn([network_state, job_waiting_state])\n",
    "                        manager.record_summary(\"Value/train\", torch.mean(model.v(state)), env.get_time_step())\n",
    "\n",
    "            job = job_waiting_queue.popleft()\n",
    "            src = -1\n",
    "            dst = 1\n",
    "            for i in range(config.node_num):\n",
    "                if job[i] == -1:\n",
    "                    src = i\n",
    "                if job[i] == 1:\n",
    "                    dst = i\n",
    "\n",
    "            if src == -1:\n",
    "                src = dst\n",
    "                \n",
    "            #print(f\"src : {src}, dst : {dst}\")\n",
    "            #print(job)\n",
    "            subtasks = job[config.node_num:]\n",
    "            offloading_vector = []\n",
    "\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                feature = model.gnn([network_state, job_waiting_state])\n",
    "                feature = F.normalize(feature, dim=1)\n",
    "                new_feature = feature.unsqueeze(0)\n",
    "                first_prob, entropy, output, hidden = model.pi([new_feature, hidden])\n",
    "\n",
    "            manager.record_summary(\"Entropy/train\", torch.mean(entropy).item(), env.get_time_step())\n",
    "\n",
    "\n",
    "            m = Categorical(first_prob[0][0]) # 첫 번째 batch의 첫 번째 node+void개의 확률들\n",
    "            nodes = m.sample()\n",
    "\n",
    "            node = nodes.item()\n",
    "\n",
    "            #print(f'node : {node}')\n",
    "            \n",
    "            # void action 실험용\n",
    "            # node = nodeNum \n",
    "            \n",
    "            \n",
    "            # void action 뽑으면 void만 업데이트\n",
    "            if node == config[\"node_num\"]: \n",
    "                actions.append(node)\n",
    "                action_mask = void_mask\n",
    "\n",
    "                temp_history.append([\n",
    "                    [cur_state[\"pre_network\"][\"network_state\"], cur_state[\"cur_network\"][\"network_state\"]], \n",
    "                    [cur_state[\"pre_network\"][\"job_waiting_state\"], cur_state[\"cur_network\"][\"job_waiting_state\"]], \n",
    "                    node, 0, \n",
    "                    [cur_state[\"pre_network\"][\"network_state\"], cur_state[\"cur_network\"][\"network_state\"]], \n",
    "                    [cur_state[\"pre_network\"][\"job_waiting_state\"], cur_state[\"cur_network\"][\"job_waiting_state\"]], \n",
    "                    1, \n",
    "                    0, action_mask, 0]\n",
    "                )\n",
    "\n",
    "                env.step(\"void\")\n",
    "\n",
    "                #print(\"action finish.\")\n",
    "                \n",
    "                if env.get_response() == \"ok\":\n",
    "                    manager.update_void_selected_num()\n",
    "\n",
    "            else: # void action 아니면 다른 action 시작\n",
    "                \n",
    "                if random.random() > config[\"imitation_probability\"]:\n",
    "                        config[\"our\"] = True\n",
    "                else:\n",
    "                    config[\"our\"] = False\n",
    "                \n",
    "\n",
    "                for sub_index in range(config.model_num):\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        feature = model.gnn([network_state, job_waiting_state])\n",
    "                        feature = F.normalize(feature, dim=1)\n",
    "                        new_feature = feature.unsqueeze(0)\n",
    "                        prob, entropy, output, hidden = model.pi([new_feature, hidden])\n",
    "                        # print(prob)\n",
    "\n",
    "                    manager.record_summary(\"Entropy/train\", torch.mean(entropy).item(), env.get_time_step())\n",
    "\n",
    "                    output = output[:, :, 0:-1].squeeze(0) # action중 마지막 action은 masking\n",
    "\n",
    "                    prob = F.softmax(output, dim=1)\n",
    "                    prob = torch.concat([prob, torch.zeros(1, 1)], dim=1)\n",
    "\n",
    "                    m = Categorical(prob)\n",
    "                    nodes = m.sample()\n",
    "                    action_mask = unvoid_mask\n",
    "\n",
    "\n",
    "                    if config[\"our\"]:\n",
    "                        node = nodes[0].item() # 확률에서 sampling\n",
    "                    else:\n",
    "                        node = torch.argmin(cur_state[\"cur_network\"][\"node_waiting_state\"][:,0]).item() # argmin으로 가장 waiting이 작은 node 뽑음\n",
    "\n",
    "                    next_state = env.return_estimated_state(cur_state, node, subtasks, sub_index) # 예측된 state를 반환\n",
    "\n",
    "                    temp_history.append([\n",
    "                    [cur_state[\"pre_network\"][\"network_state\"], cur_state[\"cur_network\"][\"network_state\"]], \n",
    "                    [cur_state[\"pre_network\"][\"job_waiting_state\"], cur_state[\"cur_network\"][\"job_waiting_state\"]], \n",
    "                    node, 0, \n",
    "                    [next_state[\"pre_network\"][\"network_state\"], next_state[\"cur_network\"][\"network_state\"]], \n",
    "                    [next_state[\"pre_network\"][\"job_waiting_state\"], next_state[\"cur_network\"][\"job_waiting_state\"]], \n",
    "                    prob[0][node].item(),\n",
    "                    0, action_mask, 0]\n",
    "                    )\n",
    "\n",
    "                    offloading_vector.append(node)\n",
    "                    manager.update_node_selected_num(node)\n",
    "\n",
    "                    actions.append(node)\n",
    "\n",
    "                    cur_state = next_state # 현재 state를 next로 변경\n",
    "\n",
    "                env.set_time_step(env.get_time_step() + 1) # step + 1\n",
    "\n",
    "                if len(offloading_vector) != 0: # for문을 다 돌면 -> void action 안뽑으면\n",
    "                    # print(offloading_vector)\n",
    "                    action = str(offloading_vector)\n",
    "\n",
    "                    env.step(action)\n",
    "\n",
    "        elif msg == \"stop\":\n",
    "            env.send_response()\n",
    "            \n",
    "            \n",
    "        elif msg == \"episode_finish\":\n",
    "            env.send_response()\n",
    "\n",
    "            env.set_episode(env.get_episode() + 1) # episode + 1\n",
    "\n",
    "\n",
    "            actions = []\n",
    "\n",
    "            manager.record_node_selected_num(env.get_episode())\n",
    "            manager.record_void_selected_num(env.get_episode())\n",
    "\n",
    "            episodic_reward = env.get_episode_result()\n",
    "            episodic_reward = json.loads(episodic_reward)\n",
    "\n",
    "            env.send_response()\n",
    "            \n",
    "            finish_num = float(episodic_reward['reward'])\n",
    "            complete_num = int(episodic_reward['completNum'])\n",
    "            average_latency = float(episodic_reward['averageLatency'])\n",
    "            jitter = episodic_reward['jitter']\n",
    "            jitterMake = episodic_reward['jitterMake']\n",
    "            action_id = episodic_reward['action_id']\n",
    "            action_reward = episodic_reward['action_reward']\n",
    "            #print(list(map(float, jitter.strip().split(\" \"))))\n",
    "            #print(list(map(float, jitterMake.strip().split(\" \"))))\n",
    "            #print(list(map(int, action_id.strip().split(\" \"))))\n",
    "            #print(list(map(float, action_reward.strip().split(\" \"))))\n",
    "\n",
    "\n",
    "            normalized_finish_num = model.return_normalize_reward(finish_num)\n",
    "            \n",
    "            manager.record_summary(\"EpisodicReward/train\", finish_num, env.get_episode())\n",
    "            manager.record_summary(\"NormalizedEpisodicReward/train\", normalized_finish_num, env.get_episode())\n",
    "            manager.record_summary(\"CompleteNum/train\", complete_num, env.get_episode())\n",
    "            manager.record_summary(\"averageLatency/train\", average_latency ,env.get_episode())\n",
    "            manager.record_episode_total_reward(env.get_episode())\n",
    "            \n",
    "            model.data = episode_history[:]\n",
    "            data_length = len(model.data)\n",
    "            reward_list = [(data_length - i) * -0.01 for i in range(data_length)]\n",
    "\n",
    "            action_id_list = list(map(int, action_id.strip().split(\" \"))) if action_id != '' else []\n",
    "            action_reward_list = list(map(float, action_reward.strip().split(\" \"))) if action_reward != '' else []\n",
    "\n",
    "            action_id_reward_list = sorted(zip(action_id_list, action_reward_list))\n",
    "\n",
    "            #print(action_id_reward_list)\n",
    "\n",
    "            for i in range(len(action_id_reward_list)):\n",
    "                index = action_id_reward_list[i][0]\n",
    "                if index < data_length:\n",
    "                    reward_list[index] = action_id_reward_list[i][1]\n",
    "\n",
    "            #print(reward_list)\n",
    "\n",
    "            for i in range(len(model.data)):\n",
    "                model.data[i][3] = reward_list[i]\n",
    "\n",
    "            tt = []\n",
    "            for i in range(len(model.data)):\n",
    "                tt.append(model.data[i][3])\n",
    "            #print(tt)\n",
    "            #print(len(model.data))\n",
    "            #print(tt)\n",
    "            episode_history = []\n",
    "            temp_history = deque([])\n",
    "\n",
    "            \n",
    "\n",
    "            hidden = (torch.zeros(1, 1, config[\"lstm_hidden_num\"]), torch.zeros(1, 1, config[\"lstm_hidden_num\"]))\n",
    "\n",
    "            \n",
    "\n",
    "            config[\"entropy_weight\"] = max(0.0001, config[\"entropy_weight\"] * config[\"entropy_gamma\"])\n",
    "            config[\"imitation_probability\"] = max(config[\"imitation_gamma\"] * config[\"imitation_probability\"], 0.2)\n",
    "\n",
    "            if finish_num > max_reward:\n",
    "                modelPathName = config[\"path_name\"] + \"/max_model.pth\"\n",
    "                torch.save(model.state_dict(), modelPathName)\n",
    "                max_reward = finish_num\n",
    "\n",
    "            manager.record_summary(\"AverageReward/train\", average_reward, env.get_episode())\n",
    "            average_reward = 0\n",
    "\n",
    "            if config[\"is_train\"]:\n",
    "                \n",
    "                if env.get_episode() % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training....\")\n",
    "                model.train_net()\n",
    "                if env.get_episode() % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                if env.get_episode() % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training replay buffer....\")\n",
    "                model.train_net_history()\n",
    "                if env.get_episode() % 100 == 0:\n",
    "                    tm = localtime(time.time())\n",
    "                    time_string = strftime('%Y-%m-%d %I:%M:%S %p', tm)\n",
    "                    print(f\"[{time_string}] training complete\")\n",
    "\n",
    "                model.clear_data()\n",
    "\n",
    "                if env.get_episode() % 100 == 0:\n",
    "                    modelPathName = config[\"path_name\"] + \"/model.pth\"\n",
    "                    torch.save(model.state_dict(), modelPathName)\n",
    "                    modelPathName = config[\"path_name\"] + f\"/model_{env.get_episode()}.pth\"\n",
    "                    torch.save(model.state_dict(), modelPathName)\n",
    "\n",
    "                    time.sleep(10)\n",
    "\n",
    "                model.eval()\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 초기화 완료\n",
      "2014615\n"
     ]
    }
   ],
   "source": [
    "PIPE_NAME = \"\\\\\\\\.\\\\pipe\\\\worker_right_latency_sub_trace2\"\n",
    "BUFFER_SIZE = 200000\n",
    "PARENT_PATH = \"C:/Users/user/Desktop/suhwan/connection_test/python_agent/experiment/subtask_reward\"\n",
    "\n",
    "env = Simulator(PIPE_NAME, BUFFER_SIZE)\n",
    "\n",
    "network_info = env.get_initial_info()\n",
    "env.start_simulator() # 입력 끝나면 omnet에 전송\n",
    "\n",
    "print(\"네트워크 초기화 완료\")\n",
    "\n",
    "config = DotDict({\n",
    "    \"learning_rate\"         : 0.00005,\n",
    "    \"gamma\"                 : 0.9,\n",
    "    \"entropy_weight\"        : 0.0001,\n",
    "    \"entropy_gamma\"         : 0.9998,\n",
    "    \"lambda\"                : 0.99,\n",
    "    \"eps_clip\"              : 0.08,\n",
    "    \"batch_size\"            : 256,\n",
    "    \"loss_coef\"             : 0.5,\n",
    "    \"job_generate_rate\"     : 0.003,\n",
    "    \"is_train\"              : False,\n",
    "    \"replay_buffer_size\"    : 100,\n",
    "    \"history_learning_time\" : 0,\n",
    "    \"current_learning_time\" : 2,\n",
    "    \"node_feature_num\"      : 4,\n",
    "    \"queue_feature_num\"     : (network_info.node_num + network_info.model_num) * network_info.job_waiting_length,\n",
    "    \"hidden_feature_num\"    : 128,\n",
    "    \"reward_weight\"         : 1.0/1,\n",
    "    \"node_num\"              : network_info.node_num,\n",
    "    \"model_num\"             : network_info.model_num,\n",
    "    \"adjacency\"             : network_info.adjacency,\n",
    "    \"lstm_hidden_num\"       : 128,\n",
    "    \"cpu_load_balance_time\" : 0.1,\n",
    "    \"network_info\"          : network_info,\n",
    "    \"path_name\"             : PARENT_PATH,\n",
    "    \"T_horizon\"             : 1000,\n",
    "    \"link_num\"              : 32,\n",
    "    \"state_weight\"          : 1.0,\n",
    "    \"our\"                   : True,\n",
    "    \"imitation_probability\" : 0.0,\n",
    "    \"imitation_gamma\"       : 1.0,\n",
    "    \"pipe_name\"             : PIPE_NAME,\n",
    "    \"buffer_size\"           : BUFFER_SIZE,\n",
    "    \"load_model\"            : False, \n",
    "    \"model_path\"            : \"C:/Users/user/Desktop/suhwan/connection_test/python_agent/experiment/subtask_reward/history2/model_2500.pth\",\n",
    "})\n",
    "\n",
    "manager = Manager(config, PARENT_PATH)\n",
    "\n",
    "main(config)\n",
    "            \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12c9d4573c12dd45eabff63c44badb6fcd2b70b85de11a1a1b2c23254cbf5db5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
